{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import collections as col\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pdb\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from pytorch_datasets import DiagnosticInpainted\n",
    "import models\n",
    "import layers\n",
    "import utilities.reading_images as reading_images\n",
    "from utilities.loading import get_single_image\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        \n",
    "        # Conv2d: (input_channels, output_channels, kernel_size, padding)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(latent_dim, latent_dim, (1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConstantPad2d((1, 1, 0, 0), 0),\n",
    "            nn.Conv2d(latent_dim, latent_dim, (1, 3)),\n",
    "            nn.ConstantPad2d((0, 0, 0, 1), 0),\n",
    "            nn.Conv2d(latent_dim, latent_dim, (2, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(latent_dim, latent_dim, (1, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, latents):\n",
    "        \n",
    "        # latents: [B, C, H, W]\n",
    "        cres = latents\n",
    "        \n",
    "        for _ in range(5):\n",
    "            c = self.model(cres)\n",
    "            cres = cres + c\n",
    "        cres = self.relu(cres)\n",
    "        return cres      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boom = torch.tensor(np.arange(20).reshape(2, 10))\n",
    "boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9]],\n",
       "\n",
       "        [[10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boom.reshape(2, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC_loss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CPC_loss, self).__init__()\n",
    "        self.pixel_cnn = PixelCNN(2048)\n",
    "        self.target_to_32 = nn.Conv2d(2048, 16, kernel_size = (1, 1))\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(2048, 16, kernel_size = (1, 1))\n",
    "        self.conv_2 = nn.Conv2d(2048, 16, kernel_size = (1, 1))\n",
    "        self.conv_3 = nn.Conv2d(2048, 16, kernel_size = (1, 1))\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, latents, device, target_dim = 16, steps_to_ignore = 2, steps_to_predict = 3, emb_scale = 0.1):\n",
    "        # latents: [B, D, H, W]\n",
    "        # aka:     [B, 512, 6, 6]\n",
    "        loss = 0.0\n",
    "        latents = latents.to(device)\n",
    "        context = self.pixel_cnn(latents) # These are the c's (apply pixelCNN to Z's)\n",
    "        targets = self.target_to_32(latents)\n",
    "#         targets = latents\n",
    "        \n",
    "        batch_dim, emb_dim, col_dim, row_dim = targets.shape\n",
    "        targets = targets.reshape(-1, target_dim)\n",
    "        \n",
    "        # Trying to do the arbitrary context vector\n",
    "        index = np.random.choice(a = [0, 1, 2])\n",
    "        context = context[:, :, index, :].unsqueeze(3) # [2, 512, 6, 1]\n",
    "        \n",
    "        \n",
    "        preds_1 = self.conv_1(context).reshape(-1, target_dim) * emb_scale\n",
    "        preds_2 = self.conv_2(context).reshape(-1, target_dim) * emb_scale\n",
    "        preds_3 = self.conv_3(context).reshape(-1, target_dim) * emb_scale\n",
    "        \n",
    "        logits_1 = torch.matmul(preds_1, targets.permute(1, 0)) # 12 by 512, 512 by 72 --> 12 by 72\n",
    "        logits_2 = torch.matmul(preds_2, targets.permute(1, 0))\n",
    "        logits_3 = torch.matmul(preds_3, targets.permute(1, 0))\n",
    "        \n",
    "        total_elements = batch_dim * row_dim\n",
    "        b = np.arange(total_elements) / (row_dim)\n",
    "        b = b.astype(int)\n",
    "        col = np.arange(total_elements) % (row_dim)\n",
    "\n",
    "        labels_1 = b * col_dim * row_dim + 3 * row_dim + col\n",
    "        labels_2 = labels_1 + 6\n",
    "        labels_3 = labels_2 + 6\n",
    "        \n",
    "        loss += self.loss_func(logits_1, torch.LongTensor(labels_1).to(device))\n",
    "        loss += self.loss_func(logits_2, torch.LongTensor(labels_2).to(device))\n",
    "        loss += self.loss_func(logits_3, torch.LongTensor(labels_3).to(device))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         for i in range(steps_to_ignore, steps_to_predict):\n",
    "# #             pdb.set_trace()\n",
    "#             col_dim_i = col_dim - i - 1  # 6 - 2 - 1 = 3\n",
    "#             total_elements = batch_dim * col_dim_i * row_dim\n",
    "#             preds_i = self.conv_preds(context)\n",
    "#             preds_i = preds_i[:, :, :(i+1), :] * emb_scale   # [B, 64, 6, 6] ---> [B, 64, 3, 6]\n",
    "#             preds_i = preds_i.reshape(-1, target_dim)\n",
    "            \n",
    "#             logits = torch.matmul(preds_i, targets.permute(1, 0)) # 18 by 64, 64 by 36 ---> 18 by 36\n",
    "            \n",
    "#             b = np.arange(total_elements) / (col_dim_i * row_dim)\n",
    "#             b = b.astype(int)\n",
    "#             col = np.arange(total_elements) % (col_dim_i * row_dim)\n",
    "#             labels = b * col_dim * row_dim + (i + 1) * row_dim + col\n",
    "#             labels = torch.LongTensor(labels).to(device)\n",
    "#             logits = logits.to(device)\n",
    "            \n",
    "#             rand = np.random.choice(a=[False, True], size = (logits.shape[0],))\n",
    "#             logits = logits[rand, :]\n",
    "#             labels = labels[rand]\n",
    "#             loss += self.loss_func(logits, labels)\n",
    "        return loss\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_elements = 1 * 3 * 6\n",
    "b = np.arange(18) / (3 * 6)\n",
    "b = b.astype(int)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = np.arange(18) % (3 * 6)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = b * 6 * 6 + 3*6 + col\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation in the paper is unclear.\n",
    "# I'm going to go with WF. \n",
    "\n",
    "# NCE Loss\n",
    "# Questions: Is the dimension of Z (B*patches) or (B). \n",
    "#            I think it's (B, 6, 6, 4096)\n",
    "\n",
    "class CPCLossNCE(nn.Module):\n",
    "    \n",
    "    def nce_loss(self, z_hat, pos_scores, negative_samples, mask_mat, device, epoch_num, batch_num):\n",
    "        \n",
    "        z_hat = z_hat.to(device)\n",
    "        pos_scores = pos_scores.to(device)\n",
    "        negative_samples = negative_samples.to(device)\n",
    "        mask_mat = mask_mat.to(device)\n",
    "                \n",
    "        # (b, 1)\n",
    "        pos_scores = pos_scores.float()\n",
    "        batch_size, emb_dim = z_hat.size()\n",
    "        nb_feat_vectors = negative_samples.size(1) // batch_size # 36 of them, if 6 by 6 wireframes. \n",
    "        \n",
    "        # (b, b) -> (b, b, nb_feat_vectors)\n",
    "        # all zeros with ones in diagonal tensor... (ie: b1 b1 are all 1s, b1 b2 are all zeros)\n",
    "        mask_pos = mask_mat.unsqueeze(dim=2).expand(-1, -1, nb_feat_vectors).float()\n",
    "        \n",
    "        # negative mask\n",
    "        mask_neg = 1. - mask_pos\n",
    "        \n",
    "        # ----------------------\n",
    "        # ALL SCORES computation\n",
    "        # (visualize in your mind a batch size of 2, 36-length segments) \n",
    "        # (b, dim) x (dim, nb_feats*b) -> (b, b, nb_feats)\n",
    "        raw_scores = torch.mm(z_hat, negative_samples)\n",
    "        raw_scores = raw_scores.reshape(batch_size, batch_size, nb_feat_vectors).float()\n",
    "        \n",
    "        # EXTRACT NEGATIVE SCORES\n",
    "        # (batch_size, batch_size, nb_feat_vectors)\n",
    "        # HE'S TAKING THE NEGATIVE SAMPLES FROM THE OTHER MINIBATCHES\n",
    "        # A GIVEN Z_HAT IS ONLY MULTIPLIED BY Z'S FROM OTHER MINIBATCHES\n",
    "        neg_scores = (mask_neg * raw_scores)\n",
    "        # ----------------------\n",
    "        \n",
    "        # (b, b, nb_feat_vectors) -> (batch_size, batch_size * nb_feat_vectors) \n",
    "        neg_scores = neg_scores.reshape(batch_size, -1)\n",
    "        mask_neg = mask_neg.reshape(batch_size, -1)\n",
    "        \n",
    "        # STABLE SOFTMAX\n",
    "        # (n_batch_gpu, 1)\n",
    "        neg_maxes = torch.max(neg_scores, dim=1, keepdim=True)[0]\n",
    "        \n",
    "        # DENOMINATOR\n",
    "        # sum over only negative samples (none from the diagonal)\n",
    "        neg_sumexp = (mask_neg * torch.exp(neg_scores - neg_maxes)).sum(dim=1, keepdim=True)\n",
    "        all_logsumexp = torch.log(torch.exp(pos_scores - neg_maxes) + neg_sumexp)\n",
    "        \n",
    "        # NUMERATOR\n",
    "        # compute numerators for the NCE log-softmaxes\n",
    "        pos_shiftexp = pos_scores - neg_maxes\n",
    "        \n",
    "        # FULL NCE\n",
    "#         if epoch_num > 2 and batch_num == 100:\n",
    "#             pdb.set_trace()\n",
    "        nce_scores = pos_shiftexp - all_logsumexp\n",
    "        nce_scores = -nce_scores.mean()\n",
    "        \n",
    "#         if np.isnan(nce_scores.cpu().detach().numpy()):\n",
    "#             pdb.set_trace()\n",
    "#             print('boom - nceloss')\n",
    "            \n",
    "        \n",
    "        return nce_scores\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, Z, C, W_list, device, epoch_num, batch_num):\n",
    "        '''\n",
    "        param Z: latent vecs (B, D, H, W)\n",
    "        param C: context vecs (B, D, H, W)\n",
    "        param W_list: list of k-1 W projections\n",
    "        '''\n",
    "        \n",
    "        # (b, dim, w, h)\n",
    "        batch_size, emb_dim, h, w = Z.size()\n",
    "        \n",
    "        # (10 x 10 identity matrix)\n",
    "        diag_mat = torch.eye(batch_size)\n",
    "        diag_mat = diag_mat.float()\n",
    "        \n",
    "        losses = []\n",
    "        # calculate loss for each k\n",
    "        \n",
    "        # Below operations preserve raster order (for B, D, H, W) = (1, 5, 2, 2) check.\n",
    "        # Z_neg holds all z vecs. \n",
    "        Z_neg = Z.permute(1, 0, 2, 3).reshape(emb_dim, -1)\n",
    "        \n",
    "        \n",
    "        for i in range(0, h-1):\n",
    "            for j in range(0, w):\n",
    "                cij = C[:, :, i, j]   # B by D\n",
    "                \n",
    "                for k in range(i+1, h): # predict on all vectors in the same column, but below current wireframe. \n",
    "                    Wk = W_list[str(k)]\n",
    "                    \n",
    "                    z_hat_ikj = Wk(cij)\n",
    "                    zikj = Z[:, :, k, j]\n",
    "                    \n",
    "                    # BATCH DOT PRODUCT\n",
    "                    # (b, d) x (b, d) -> (b, 1)\n",
    "                    pos_scores = torch.bmm(z_hat_ikj.unsqueeze(1), zikj.unsqueeze(2))\n",
    "                    pos_scores = pos_scores.squeeze(-1).squeeze(-1)\n",
    "                    \n",
    "                    loss = self.nce_loss(z_hat_ikj, pos_scores, Z_neg, diag_mat, device, epoch_num, batch_num)\n",
    "                    if np.isinf(loss.item()):\n",
    "                        pdb.set_trace()\n",
    "                        print(\"inf -- inside inner for loop\")\n",
    "                    if np.isnan(loss.item()):\n",
    "                        pdb.set_trace()\n",
    "                        print(\"inside inner for loop\")\n",
    "                    losses.append(loss)\n",
    "                    \n",
    "                    \n",
    "        losses = torch.stack(losses)\n",
    "        loss = losses.mean()\n",
    "#         if np.isnan(loss.item()):\n",
    "#             pdb.set_trace()\n",
    "#             print('boom')\n",
    "        return loss           \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "boom = torch.rand(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "boom = boom.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(boom, 3, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_raster_patchify(img, size = 80, overlap = 32):\n",
    "    '''\n",
    "    Left-to-right, top to bottom.\n",
    "    Assumes img is (3, 240, 240).\n",
    "    '''\n",
    "    patches = []\n",
    "     \n",
    "    h = -32\n",
    "    w = -32\n",
    "    for i in range(6):\n",
    "        h = h + 32\n",
    "        for j in range(6):\n",
    "            w = w + 32\n",
    "            channel = np.random.randint(3)\n",
    "            processed_img = np.repeat(np.expand_dims(img[channel, h:h+size, w:w+size], axis=0), 3, axis=0)\n",
    "            if np.random.randint(2):\n",
    "                processed_img = np.flip(processed_img, axis=2)\n",
    "            patches.append(torch.tensor(np.ascontiguousarray(processed_img)))\n",
    "        w = -32\n",
    "            \n",
    "    return patches\n",
    "\n",
    "\n",
    "def val_raster_patchify(img, size = 80, overlap = 32):\n",
    "    '''\n",
    "    Left-to-right, top to bottom.\n",
    "    Assumes img is (3, 240, 240).\n",
    "    '''\n",
    "    patches = []\n",
    "     \n",
    "    h = -32\n",
    "    w = -32\n",
    "    for i in range(6):\n",
    "        h = h + 32\n",
    "        for j in range(6):\n",
    "            w = w + 32\n",
    "            patches.append(img[:, h:h+size, w:w+size])\n",
    "        w = -32\n",
    "            \n",
    "    return patches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(patches[35].permute(1, 2, 0))\n",
    "\n",
    "# plt.imshow(trainset[0][0].permute(1, 2, 0))\n",
    "# plt.scatter(80,80,color='r')\n",
    "# plt.scatter(80+32,80,color='r')\n",
    "# plt.scatter(80+64,80,color='r')\n",
    "# plt.scatter(80+96,80,color='r')\n",
    "# plt.scatter(80+128,80,color='r')\n",
    "# plt.scatter(80+160,80,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(img_list):\n",
    "    patches = []\n",
    "    for (img, label) in img_list:\n",
    "        img_patches = train_raster_patchify(img)\n",
    "        patches.append(torch.stack(img_patches))\n",
    "        \n",
    "    return patches\n",
    "\n",
    "def val_collate_fn(img_list):\n",
    "    patches = []\n",
    "    for (img, label) in img_list:\n",
    "        img_patches = val_raster_patchify(img)\n",
    "        patches.append(torch.stack(img_patches))\n",
    "        \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(240),\n",
    "    transforms.ColorJitter(brightness=(0.55, 1), contrast=(0.5, 1), saturation=(0.5, 1), hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "trainset = datasets.ImageFolder(\n",
    "    root = '/gpfs/data/geraslab/Vish/imagenette2-320/train/',\n",
    "    transform = data_transform\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=train_collate_fn)\n",
    "\n",
    "valset = datasets.ImageFolder(\n",
    "    root = '/gpfs/data/geraslab/Vish/imagenette2-320/val/',\n",
    "    transform = data_transform\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(valset, batch_size=32, shuffle=True, collate_fn=val_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_batchnorm(model):\n",
    "    model.bn1 = Identity()\n",
    "    model.layer1[0].bn1 = Identity()\n",
    "    model.layer1[0].bn2 = Identity()\n",
    "    model.layer1[0].bn3 = Identity()\n",
    "    model.layer1[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer1[1].bn1 = Identity()\n",
    "    model.layer1[1].bn2 = Identity()\n",
    "    model.layer1[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer1[2].bn1 = Identity()\n",
    "    model.layer1[2].bn2 = Identity()\n",
    "    model.layer1[2].bn3 = Identity()\n",
    "    \n",
    "    model.layer2[0].bn1 = Identity()\n",
    "    model.layer2[0].bn2 = Identity()\n",
    "    model.layer2[0].bn3 = Identity()\n",
    "    model.layer2[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer2[1].bn1 = Identity()\n",
    "    model.layer2[1].bn2 = Identity()\n",
    "    model.layer2[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer2[2].bn1 = Identity()\n",
    "    model.layer2[2].bn2 = Identity()\n",
    "    model.layer2[2].bn3 = Identity()\n",
    "    \n",
    "    model.layer2[3].bn1 = Identity()\n",
    "    model.layer2[3].bn2 = Identity()\n",
    "    model.layer2[3].bn3 = Identity()\n",
    "    \n",
    "    model.layer3[0].bn1 = Identity()\n",
    "    model.layer3[0].bn2 = Identity()\n",
    "    model.layer3[0].bn3 = Identity()\n",
    "    model.layer3[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer3[1].bn1 = Identity()\n",
    "    model.layer3[1].bn2 = Identity()\n",
    "    model.layer3[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer3[2].bn1 = Identity()\n",
    "    model.layer3[2].bn2 = Identity()\n",
    "    model.layer3[2].bn3 = Identity()\n",
    "    \n",
    "    \n",
    "    model.layer3[3].bn1 = Identity()\n",
    "    model.layer3[3].bn2 = Identity()\n",
    "    model.layer3[3].bn3 = Identity()\n",
    "    \n",
    "    \n",
    "    model.layer3[4].bn1 = Identity()\n",
    "    model.layer3[4].bn2 = Identity()\n",
    "    model.layer3[4].bn3 = Identity()\n",
    "    \n",
    "    model.layer3[5].bn1 = Identity()\n",
    "    model.layer3[5].bn2 = Identity()\n",
    "    model.layer3[5].bn3 = Identity()\n",
    "    \n",
    "    model.layer4[0].bn1 = Identity()\n",
    "    model.layer4[0].bn2 = Identity()\n",
    "    model.layer4[0].bn3 = Identity()\n",
    "    model.layer4[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer4[1].bn1 = Identity()\n",
    "    model.layer4[1].bn2 = Identity()\n",
    "    model.layer4[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer4[2].bn1 = Identity()\n",
    "    model.layer4[2].bn2 = Identity()\n",
    "    model.layer4[2].bn3 = Identity()\n",
    "    \n",
    "#     model.layer4[0].bn1 = Identity()\n",
    "#     model.layer4[0].bn2 = Identity()\n",
    "#     model.layer4[0].downsample[1] = Identity()\n",
    "#     model.layer4[1].bn1 = Identity()\n",
    "#     model.layer4[1].bn2 = Identity()\n",
    "#     model.layer4[2].bn1 = Identity()\n",
    "#     model.layer4[2].bn2 = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CPC, self).__init__()\n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.fc = Identity()\n",
    "        remove_batchnorm(self.encoder)\n",
    "#         self.pixel_cnn = PixelCNN(1024)\n",
    "        self.nce_loss = CPC_loss()\n",
    "#         self.nce_loss = CPCLossNCE()\n",
    "        \n",
    "#         # W transforms (k > 0)\n",
    "#         self.W_list = {}\n",
    "#         for k in range(1, 6):\n",
    "#             w = torch.nn.Linear(512, 512)\n",
    "#             self.W_list[str(k)] = w\n",
    "\n",
    "#         self.W_list = nn.ModuleDict(self.W_list)\n",
    "        \n",
    "\n",
    "    def forward(self, x, device, epoch_num, batch_num):\n",
    "        Z = []\n",
    "#         C = []\n",
    "        for img_patches in x:\n",
    "            img_patches = img_patches.to(device)\n",
    "            z = self.encoder(img_patches).squeeze()\n",
    "            z = z.unsqueeze(0).permute(0, 2, 1).reshape(1, 2048, 6, 6)\n",
    "            Z.append(z)\n",
    "#             c = self.pixel_cnn(z)\n",
    "#             C.append(c)\n",
    "\n",
    "        Z = torch.stack(Z).squeeze(1)\n",
    "#         C = torch.stack(C).squeeze(1)\n",
    "\n",
    "        loss = self.nce_loss(Z, device)\n",
    "        \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(dl, model, optimizer, device, epoch_num, phase = 'train'):\n",
    "    if phase == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "#         for m in model.modules():\n",
    "#             if isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.track_running_stats = False\n",
    "    losses = []\n",
    "    for i, x in enumerate(dl):\n",
    "        if phase == 'train':\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss = model(x, device, epoch_num, i)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if phase == 'train': \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(\"Batch: {}/{}, Loss: {}\".format(i, len(dl), loss.item())) \n",
    "            \n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs(epoch_num):\n",
    "    torch.cuda.set_device(6)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = CPC().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 2e-4, weight_decay=1e-5, eps=1e-8)\n",
    "    \n",
    "    # val loss: 0.02\n",
    "#     pretrained_dict = torch.load('paper_self_supervised_rc_best_val.pt')\n",
    "#     model_dict = model.state_dict()\n",
    "\n",
    "#     # 1. filter out unnecessary keys\n",
    "#     pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "#     # 2. overwrite entries in the existing state dict\n",
    "#     model_dict.update(pretrained_dict) \n",
    "#     # 3. load the new state dict\n",
    "#     model.load_state_dict(model_dict)\n",
    "    \n",
    "    best_val_loss = 1000000\n",
    "    for i in range(epoch_num):\n",
    "        print(\"Started epoch {}\\n\".format(i))\n",
    "        avg_train_loss = one_epoch(train_dl, model, optimizer, device, i, phase = 'train')\n",
    "        print(\"Average Epoch {} Loss: {}\\n\".format(i, avg_train_loss))\n",
    "        avg_val_loss = one_epoch(val_dl, model, optimizer, device, i, phase = 'val')\n",
    "        print(\"Validation Loss: {}\\n\".format(avg_val_loss))\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"paper_self_supervised_rc_best_val.pt\")\n",
    "            print(\"\\nSaved model with best validation loss: {}\".format(best_val_loss))\n",
    "        \n",
    "#         if i in [1, 10, 20, 25]:\n",
    "#             torch.save(model.state_dict(), \"paper_self_supervised_rc_{}.pt\".format(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started epoch 0\n",
      "\n",
      "Batch: 0/296, Loss: 37.18711853027344\n",
      "Batch: 50/296, Loss: 16.925752639770508\n",
      "Batch: 100/296, Loss: 16.807903289794922\n",
      "Batch: 150/296, Loss: 16.132875442504883\n",
      "Batch: 200/296, Loss: 15.945775032043457\n",
      "Batch: 250/296, Loss: 16.402841567993164\n",
      "Average Epoch 0 Loss: 17.155265415037\n",
      "\n",
      "Validation Loss: 16.20236622414938\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 16.20236622414938\n",
      "Started epoch 1\n",
      "\n",
      "Batch: 0/296, Loss: 16.379276275634766\n",
      "Batch: 50/296, Loss: 16.307296752929688\n",
      "Batch: 100/296, Loss: 15.5255126953125\n",
      "Batch: 150/296, Loss: 15.710958480834961\n",
      "Batch: 200/296, Loss: 15.41317367553711\n",
      "Batch: 250/296, Loss: 15.859216690063477\n",
      "Average Epoch 1 Loss: 15.754687937530312\n",
      "\n",
      "Validation Loss: 15.545803953961629\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 15.545803953961629\n",
      "Started epoch 2\n",
      "\n",
      "Batch: 0/296, Loss: 15.793050765991211\n",
      "Batch: 50/296, Loss: 11.178373336791992\n",
      "Batch: 100/296, Loss: 11.999295234680176\n",
      "Batch: 150/296, Loss: 9.980120658874512\n",
      "Batch: 200/296, Loss: 7.912424087524414\n",
      "Batch: 250/296, Loss: 8.180915832519531\n",
      "Average Epoch 2 Loss: 10.830080398031184\n",
      "\n",
      "Validation Loss: 7.5213328183181885\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 7.5213328183181885\n",
      "Started epoch 3\n",
      "\n",
      "Batch: 0/296, Loss: 9.64604377746582\n",
      "Batch: 50/296, Loss: 12.656705856323242\n",
      "Batch: 100/296, Loss: 5.106141567230225\n",
      "Batch: 150/296, Loss: 5.583399295806885\n",
      "Batch: 200/296, Loss: 5.9581170082092285\n",
      "Batch: 250/296, Loss: 6.106083393096924\n",
      "Average Epoch 3 Loss: 6.732073632446495\n",
      "\n",
      "Validation Loss: 3.6771634051470254\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 3.6771634051470254\n",
      "Started epoch 4\n",
      "\n",
      "Batch: 0/296, Loss: 3.816127300262451\n",
      "Batch: 50/296, Loss: 4.421763896942139\n",
      "Batch: 100/296, Loss: 4.6685404777526855\n",
      "Batch: 150/296, Loss: 2.6275546550750732\n",
      "Batch: 200/296, Loss: 4.004539966583252\n",
      "Batch: 250/296, Loss: 3.582855701446533\n",
      "Average Epoch 4 Loss: 3.528831440049249\n",
      "\n",
      "Validation Loss: 2.3305726254858623\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 2.3305726254858623\n",
      "Started epoch 5\n",
      "\n",
      "Batch: 0/296, Loss: 3.6803250312805176\n",
      "Batch: 50/296, Loss: 3.676805019378662\n",
      "Batch: 100/296, Loss: 1.4424525499343872\n",
      "Batch: 150/296, Loss: 2.25771164894104\n",
      "Batch: 200/296, Loss: 2.0441524982452393\n",
      "Batch: 250/296, Loss: 3.8262698650360107\n",
      "Average Epoch 5 Loss: 2.715310994032267\n",
      "\n",
      "Validation Loss: 1.9433275456350994\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 1.9433275456350994\n",
      "Started epoch 6\n",
      "\n",
      "Batch: 0/296, Loss: 1.0535809993743896\n",
      "Batch: 50/296, Loss: 2.378736972808838\n",
      "Batch: 100/296, Loss: 1.468935251235962\n",
      "Batch: 150/296, Loss: 3.5604302883148193\n",
      "Batch: 200/296, Loss: 2.3385186195373535\n",
      "Batch: 250/296, Loss: 3.185519218444824\n",
      "Average Epoch 6 Loss: 2.1062934316090636\n",
      "\n",
      "Validation Loss: 2.416458905227785\n",
      "\n",
      "Started epoch 7\n",
      "\n",
      "Batch: 0/296, Loss: 3.267284870147705\n",
      "Batch: 50/296, Loss: 1.5328024625778198\n",
      "Batch: 100/296, Loss: 1.26395845413208\n",
      "Batch: 150/296, Loss: 1.5841825008392334\n",
      "Batch: 200/296, Loss: 1.534152626991272\n",
      "Batch: 250/296, Loss: 2.5284366607666016\n",
      "Average Epoch 7 Loss: 1.6616654343701698\n",
      "\n",
      "Validation Loss: 0.9537107716730939\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 0.9537107716730939\n",
      "Started epoch 8\n",
      "\n",
      "Batch: 0/296, Loss: 0.8258822560310364\n"
     ]
    }
   ],
   "source": [
    "run_epochs(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(6)\n",
    "a.repeat(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 240, 240])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[50][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from torchvision ResNet, converted to v2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_channels, num_filters,\n",
    "                 first_layer_kernel_size, first_layer_conv_stride,\n",
    "                 blocks_per_layer_list, block_strides_list, block_fn,\n",
    "                 first_layer_padding=0,\n",
    "                 first_pool_size=None, first_pool_stride=None, first_pool_padding=0,\n",
    "                 growth_factor=2, norm_class=\"batch\", num_groups=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.first_conv = nn.Conv2d(\n",
    "            in_channels=input_channels, out_channels=num_filters,\n",
    "            kernel_size=first_layer_kernel_size,\n",
    "            stride=first_layer_conv_stride,\n",
    "            padding=first_layer_padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        # Diff: padding=SAME vs. padding=0\n",
    "        self.first_pool = nn.MaxPool2d(\n",
    "            kernel_size=first_pool_size,\n",
    "            stride=first_pool_stride,\n",
    "            padding=first_pool_padding,\n",
    "        )\n",
    "        self.norm_class = norm_class\n",
    "        self.num_groups = num_groups\n",
    "\n",
    "        block = self._resolve_block(block_fn)\n",
    "        self.layer_list = nn.ModuleList()\n",
    "        current_num_filters = num_filters\n",
    "        self.inplanes = num_filters\n",
    "        for i, (num_blocks, stride) in enumerate(zip(\n",
    "                blocks_per_layer_list, block_strides_list)):\n",
    "            self.layer_list.append(self._make_layer(\n",
    "                block=block,\n",
    "                planes=current_num_filters,\n",
    "                blocks=num_blocks,\n",
    "                stride=stride,\n",
    "            ))\n",
    "            current_num_filters *= growth_factor\n",
    "\n",
    "        self.final_bn = layers.resolve_norm_layer(\n",
    "            # current_num_filters // growth_factor\n",
    "            current_num_filters // growth_factor * block.expansion,\n",
    "            norm_class,\n",
    "            num_groups\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize()\n",
    "\n",
    "        # Expose attributes for downstream dimension computation\n",
    "        self.num_filters = num_filters\n",
    "        self.growth_factor = growth_factor\n",
    "        self.block = block\n",
    "        self.num_filter_last_seq = current_num_filters // growth_factor * block.expansion\n",
    "\n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        intermediate = []\n",
    "        h = self.first_conv(x)\n",
    "        h = self.first_pool(h)\n",
    "\n",
    "        if return_intermediate:\n",
    "            intermediate.append(h)\n",
    "        for i, layer in enumerate(self.layer_list):\n",
    "            h = layer(h)\n",
    "            if return_intermediate:\n",
    "                intermediate.append(h)\n",
    "\n",
    "        h = self.final_bn(h)\n",
    "        h = self.relu(h)\n",
    "\n",
    "        if return_intermediate:\n",
    "            return h, intermediate\n",
    "        else:\n",
    "            return h\n",
    "\n",
    "    @classmethod\n",
    "    def _resolve_block(cls, block_fn):\n",
    "        if block_fn == \"normal\":\n",
    "            return layers.BasicBlockV2_dbt\n",
    "        elif block_fn == \"bottleneck\":\n",
    "            return layers.BottleneckV2_dbt\n",
    "        else:\n",
    "            raise KeyError(block_fn)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        # downsample = None\n",
    "        # if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                      kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(planes * block.expansion),\n",
    "        )\n",
    "\n",
    "        layers_ = [\n",
    "            block(self.inplanes, planes, stride, downsample, self.norm_class, self.num_groups)\n",
    "        ]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers_.append(block(self.inplanes, planes, norm_class=self.norm_class, num_groups=self.num_groups))\n",
    "\n",
    "        return nn.Sequential(*layers_)\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            self._layer_init(m)\n",
    "\n",
    "    @classmethod\n",
    "    def _layer_init(cls, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # From original\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #             nn.init.xavier_normal_(m.weight)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.GroupNorm):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @classmethod\n",
    "    def from_parameters(cls, parameters):\n",
    "        return cls(\n",
    "            input_channels=parameters[\"input_channels\"],\n",
    "            num_filters=parameters[\"num_filters\"],\n",
    "            first_layer_kernel_size=parameters[\"first_layer_kernel_size\"],\n",
    "            first_layer_conv_stride=parameters[\"first_layer_conv_stride\"],\n",
    "            first_layer_padding=parameters.get(\"first_layer_padding\", 0),\n",
    "            blocks_per_layer_list=parameters[\"blocks_per_layer_list\"],\n",
    "            block_strides_list=parameters[\"block_strides_list\"],\n",
    "            block_fn=parameters[\"block_fn\"],\n",
    "            first_pool_size=parameters[\"first_pool_size\"],\n",
    "            first_pool_stride=parameters[\"first_pool_stride\"],\n",
    "            first_pool_padding=parameters.get(\"first_pool_padding\", 0),\n",
    "            growth_factor=parameters.get(\"growth_factor\", 2),\n",
    "            norm_class=parameters.get(\"norm_class\", \"batch\"),\n",
    "            num_groups=parameters.get(\"num_groups\", 1)\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_22(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            attention=False,\n",
    "            dropout=0.0,\n",
    "            hidden_size=256,\n",
    "\n",
    "            # resnet hyperparameters\n",
    "            #         input_channels=1,\n",
    "            first_layer_kernel_size=7,\n",
    "            first_layer_conv_stride=2,\n",
    "            first_pool_size=3,\n",
    "            first_pool_stride=2,\n",
    "            first_layer_padding=0,\n",
    "            first_pool_padding=0,\n",
    "            growth_factor=2,\n",
    "\n",
    "            # resnet22 settings\n",
    "            num_filters=16,\n",
    "            blocks_per_layer_list=[2, 2, 2, 2, 2],\n",
    "            block_strides_list=[1, 2, 2, 2, 2],\n",
    "            block_fn=\"normal\",\n",
    "            norm_class=\"group\",\n",
    "            num_groups=8,\n",
    "\n",
    "            num_image_slices_per_net=1,\n",
    "    ):\n",
    "        super(ResNet_22, self).__init__()\n",
    "\n",
    "        self.num_image_slices_per_net = num_image_slices_per_net\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.resnet = ResNet(\n",
    "            input_channels=3,\n",
    "            first_layer_kernel_size=first_layer_kernel_size,\n",
    "            first_layer_conv_stride=first_layer_conv_stride,\n",
    "            first_pool_size=first_pool_size,\n",
    "            first_pool_stride=first_pool_stride,\n",
    "            num_filters=num_filters,\n",
    "            blocks_per_layer_list=blocks_per_layer_list,\n",
    "            block_strides_list=block_strides_list,\n",
    "            block_fn=block_fn,\n",
    "            first_layer_padding=first_layer_padding,\n",
    "            first_pool_padding=first_pool_padding,\n",
    "            growth_factor=growth_factor,\n",
    "            norm_class=norm_class,\n",
    "            num_groups=num_groups,\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # use avgpool rather than torch.mean\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        h = self.resnet(x)\n",
    "        # Shape of pooled_h is [4, 256, 1, 1]\n",
    "        pooled_h = self.avgpool(h)\n",
    "        return pooled_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNet_22' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e02a0ee98ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet_22\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResNet_22' is not defined"
     ]
    }
   ],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = ResNet_22()\n",
    "model.eval()\n",
    "\n",
    "boom = model(trainset[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 1, 1])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boom.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
