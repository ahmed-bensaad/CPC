{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import collections as col\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pdb\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet34, resnet50\n",
    "import torch.nn.functional as F\n",
    "from pytorch_datasets import DiagnosticInpainted\n",
    "import models\n",
    "import layers\n",
    "import utilities.reading_images as reading_images\n",
    "from utilities.loading import get_single_image\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay so I've figured out that it's the Batchnorm layer that's causing this. \n",
    "# I basically trained a model on Imagewoof and saved several checkpoints. \n",
    "# At epoch 30, the model gave me 0.06 loss. That should correlate with close to 100% train accuracy, \n",
    "# especially since the classes are balanced. \n",
    "# It did not. Train and val accuracies are all ~15%. \n",
    "# I then used this saved model and printed its loss in eval mode. It was 4.095. Obviously way off. \n",
    "# Several people online encounter this problem. Drastic difference in behavior in BatchNorm between .train()\n",
    "# and .eval() time. \n",
    "# Soumith suggests increasing the momentum parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        \n",
    "        # Conv2d: (input_channels, output_channels, kernel_size, padding)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, (1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConstantPad2d((1, 1, 0, 0), 0),\n",
    "            nn.Conv2d(256, 256, (1, 3)),\n",
    "            nn.ConstantPad2d((0, 0, 0, 1), 0),\n",
    "            nn.Conv2d(256, 256, (2, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, (1, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, latents):\n",
    "        \n",
    "        # latents: [B, C, H, W]\n",
    "        cres = latents\n",
    "        \n",
    "        for _ in range(5):\n",
    "            c = self.model(cres)\n",
    "            cres = cres + c\n",
    "        cres = self.relu(cres)\n",
    "        return cres      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_raster_patchify(img, size = 80, overlap = 32):\n",
    "    '''\n",
    "    Left-to-right, top to bottom.\n",
    "    Assumes img is (3, 240, 240).\n",
    "    '''\n",
    "    patches = []\n",
    "     \n",
    "    h = -32\n",
    "    w = -32\n",
    "    for i in range(6):\n",
    "        h = h + 32\n",
    "        for j in range(6):\n",
    "            w = w + 32\n",
    "            channel = np.random.randint(3)\n",
    "            processed_img = np.repeat(np.expand_dims(img[channel, h:h+size, w:w+size], axis=0), 3, axis=0)\n",
    "            if np.random.randint(2):\n",
    "                processed_img = np.flip(processed_img, axis=2)\n",
    "            patches.append(torch.tensor(np.ascontiguousarray(processed_img)))\n",
    "        w = -32\n",
    "            \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_raster_patchify(img, size = 80, overlap = 32):\n",
    "    '''\n",
    "    Left-to-right, top to bottom.\n",
    "    Assumes img is (3, 240, 240).\n",
    "    '''\n",
    "    patches = []\n",
    "     \n",
    "    h = -32\n",
    "    w = -32\n",
    "    for i in range(6):\n",
    "        h = h + 32\n",
    "        for j in range(6):\n",
    "            w = w + 32\n",
    "            patches.append(img[:, h:h+size, w:w+size])\n",
    "        w = -32\n",
    "            \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_collate_fn(img_list):\n",
    "    patches = []\n",
    "    labels = []\n",
    "    for (img, label) in img_list:\n",
    "        img_patches = val_raster_patchify(img)\n",
    "        patches.append(torch.stack(img_patches))\n",
    "        labels.append(label)\n",
    "        \n",
    "    return patches, labels\n",
    "\n",
    "def train_collate_fn(img_list):\n",
    "    patches = []\n",
    "    labels = []\n",
    "    for (img, label) in img_list:\n",
    "        img_patches = train_raster_patchify(img)\n",
    "        patches.append(torch.stack(img_patches))\n",
    "        labels.append(label)\n",
    "        \n",
    "    return patches, labels\n",
    "\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(240),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "trainset = datasets.ImageFolder(\n",
    "    root = '/gpfs/data/geraslab/Vish/imagenette2-320/train/',\n",
    "    transform = data_transform\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=train_collate_fn)\n",
    "\n",
    "valset = datasets.ImageFolder(\n",
    "    root = '/gpfs/data/geraslab/Vish/imagenette2-320/val/',\n",
    "    transform = data_transform\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(valset, batch_size=32, shuffle=True, collate_fn=val_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_batchnorm(model):\n",
    "    model.bn1 = Identity()\n",
    "    model.layer1[0].bn1 = Identity()\n",
    "    model.layer1[0].bn2 = Identity()\n",
    "    model.layer1[0].bn3 = Identity()\n",
    "    model.layer1[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer1[1].bn1 = Identity()\n",
    "    model.layer1[1].bn2 = Identity()\n",
    "    model.layer1[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer1[2].bn1 = Identity()\n",
    "    model.layer1[2].bn2 = Identity()\n",
    "    model.layer1[2].bn3 = Identity()\n",
    "    \n",
    "    model.layer2[0].bn1 = Identity()\n",
    "    model.layer2[0].bn2 = Identity()\n",
    "    model.layer2[0].bn3 = Identity()\n",
    "    model.layer2[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer2[1].bn1 = Identity()\n",
    "    model.layer2[1].bn2 = Identity()\n",
    "    model.layer2[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer2[2].bn1 = Identity()\n",
    "    model.layer2[2].bn2 = Identity()\n",
    "    model.layer2[2].bn3 = Identity()\n",
    "    \n",
    "    model.layer2[3].bn1 = Identity()\n",
    "    model.layer2[3].bn2 = Identity()\n",
    "    model.layer2[3].bn3 = Identity()\n",
    "    \n",
    "    model.layer3[0].bn1 = Identity()\n",
    "    model.layer3[0].bn2 = Identity()\n",
    "    model.layer3[0].bn3 = Identity()\n",
    "    model.layer3[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer3[1].bn1 = Identity()\n",
    "    model.layer3[1].bn2 = Identity()\n",
    "    model.layer3[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer3[2].bn1 = Identity()\n",
    "    model.layer3[2].bn2 = Identity()\n",
    "    model.layer3[2].bn3 = Identity()\n",
    "    \n",
    "    \n",
    "    model.layer3[3].bn1 = Identity()\n",
    "    model.layer3[3].bn2 = Identity()\n",
    "    model.layer3[3].bn3 = Identity()\n",
    "    \n",
    "    \n",
    "    model.layer3[4].bn1 = Identity()\n",
    "    model.layer3[4].bn2 = Identity()\n",
    "    model.layer3[4].bn3 = Identity()\n",
    "    \n",
    "    model.layer3[5].bn1 = Identity()\n",
    "    model.layer3[5].bn2 = Identity()\n",
    "    model.layer3[5].bn3 = Identity()\n",
    "    \n",
    "    model.layer4[0].bn1 = Identity()\n",
    "    model.layer4[0].bn2 = Identity()\n",
    "    model.layer4[0].bn3 = Identity()\n",
    "    model.layer4[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer4[1].bn1 = Identity()\n",
    "    model.layer4[1].bn2 = Identity()\n",
    "    model.layer4[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer4[2].bn1 = Identity()\n",
    "    model.layer4[2].bn2 = Identity()\n",
    "    model.layer4[2].bn3 = Identity()\n",
    "    \n",
    "#     model.layer4[0].bn1 = Identity()\n",
    "#     model.layer4[0].bn2 = Identity()\n",
    "#     model.layer4[0].downsample[1] = Identity()\n",
    "#     model.layer4[1].bn1 = Identity()\n",
    "#     model.layer4[1].bn2 = Identity()\n",
    "#     model.layer4[2].bn1 = Identity()\n",
    "#     model.layer4[2].bn2 = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC_Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CPC_Linear, self).__init__()\n",
    "        self.encoder = resnet50()\n",
    "        self.encoder.fc = Identity()\n",
    "        remove_batchnorm(self.encoder)\n",
    "#         self.bn = nn.GroupNorm(32, 512)\n",
    "        self.bn = nn.BatchNorm2d(2048)\n",
    "        self.conv_1 = nn.Conv2d(2048, 10, (1, 1))\n",
    "        self.avg_pool = nn.AvgPool2d(6, 6)\n",
    "\n",
    "    def forward(self, x, device):\n",
    "        Z = []\n",
    "        for img_patches in x:\n",
    "            img_patches = img_patches.to(device)\n",
    "            z = self.encoder(img_patches).squeeze()\n",
    "            z = z.unsqueeze(0).permute(0, 2, 1).reshape(1, 2048, 6, 6)\n",
    "            Z.append(z)\n",
    "\n",
    "        Z = torch.stack(Z).squeeze(1)\n",
    "\n",
    "        output = self.avg_pool(self.conv_1(self.bn(Z))).squeeze(2).squeeze(2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(dl, model, loss_func, optimizer, device, phase = 'train'):\n",
    "    if phase == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "#         for m in model.modules():\n",
    "#             if isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.track_running_stats = False\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    for i, (x, labels) in enumerate(dl):\n",
    "        if phase == 'train':\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        labels = torch.from_numpy(np.stack(labels)).to(device)\n",
    "        preds_logit = model(x, device)\n",
    "        loss = loss_func(preds_logit, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if phase == 'train': \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(\"Batch: {}/{}, Loss: {}\".format(i, len(dl), loss.item())) \n",
    "        else:\n",
    "            preds_label = torch.argmax(preds_logit, dim=1)\n",
    "            correct += sum(preds_label == labels)\n",
    "            \n",
    "    \n",
    "    if phase == 'val':\n",
    "        return correct, np.mean(losses)\n",
    "    elif phase == 'train':\n",
    "        return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(7)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = CPC_Linear()\n",
    "# pretrained_dict = torch.load('pretrained_imagewoof_bn_0.5_20.pt')\n",
    "# model_dict = model.state_dict()\n",
    "\n",
    "# # 1. filter out unnecessary keys\n",
    "# pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# # 2. overwrite entries in the existing state dict\n",
    "# model_dict.update(pretrained_dict) \n",
    "# # 3. load the new state dict\n",
    "# model.load_state_dict(model_dict)\n",
    "\n",
    "# model = model.to(device)\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 1e-3, eps=1e-8)\n",
    "\n",
    "# correct, average_loss = one_epoch(train_dl, model, loss_func, optimizer, device, phase = 'val')\n",
    "\n",
    "# print(correct)\n",
    "# print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs(epoch_num):\n",
    "    torch.cuda.set_device(1)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = CPC_Linear()\n",
    "    pretrained_dict = torch.load('paper_self_supervised_rc_best_val.pt')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict) \n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    model = model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-3, eps=1e-8)\n",
    "    \n",
    "    # ----------------------------------------------\n",
    "    # FREEZE ENCODER\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for i in range(epoch_num):\n",
    "        epoch_loss = one_epoch(train_dl, model, loss_func, optimizer, device, phase = 'train')\n",
    "        print(\"Average Epoch {} Loss: {}\".format(i, epoch_loss))\n",
    "        correct, _ = one_epoch(train_dl, model, loss_func, optimizer, device, phase = 'val')\n",
    "        print(\"Train Accuracy: {}\".format(1. * correct / len(trainset)))\n",
    "        correct, _ = one_epoch(val_dl, model, loss_func, optimizer, device, phase = 'val')\n",
    "        print(\"Validation Accuracy: {}\".format(1. * correct / len(valset)))\n",
    "        \n",
    "        if i in [1, 10, 20, 30]:\n",
    "            torch.save(model.state_dict(), \"pretrained_imagewoof_batch_norm_frozen_{}.pt\".format(i))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CPC-pretrained weights, fine-tuning all. Validation Accuracy (best):  38%, within 23 epochs. \n",
    "# Using CPC-pretrained weights, frozen encoder. Validation Accuracy: \n",
    "# They use resnet-50 as the backbone. Maybe switch to that? \n",
    "# But I'm doing resnet-22 for each patch of the image. \n",
    "\n",
    "# I changed to resnet34. Frozen Encoder, validation accuracy: 16% through 9 epochs. \n",
    "# Random Color dropping (RC), Frozen Encoder, validation accuracy: 16% through 9 epochs. \n",
    "# NO pre-training, random initialized encoder + FROZEN. validation accuracy: 13% through 25 epochs. \n",
    "# NO pre-training, random initialized encoder + FINE_TUNE ALL. validation accuracy: 30% through 33 epochs.\n",
    "# Pre-trained rc_15_epochs, Frozen Encoder. Validation accuracy: \n",
    "# Pre-trained rc_15_epochs, Fine-tune all. Validation accuracy.\n",
    "\n",
    "# Imagenette\n",
    "# self-supervised. Frozen Encoder. Train/Val Accuracy:              36%/31%. \n",
    "# self-supervised 1024. Frozen Encoder. Train/Val Accuracy:         40%/35% (Through 10 epochs). \n",
    "# self-supervised 1024, color jitter. Frozen Encoder. Train/Val:    40%/38% (Through 34 epochs). \n",
    "# self-supervised 1024 tp 64, color jitter. Frozen Encoder. Train/Val:    43%/39% (Through 40 epochs). \n",
    "# self-supervised 2048 (32), color jitter. Frozen Encoder. Train/Val:    46%/41% (Through 40 epochs). \n",
    "\n",
    "# self-supervised 2048 but train 1024? (24), Frozen Encoder. Train/Val:  50%/45% (Through 25 epochs)\n",
    "\n",
    "# self-supervised 2048 but train 2048 (16), Frozen Encoder. Train/Val:  \n",
    "\n",
    "\n",
    "# Imagewoof\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/296, Loss: 2.323939323425293\n",
      "Batch: 50/296, Loss: 2.0211145877838135\n",
      "Batch: 100/296, Loss: 1.9750717878341675\n",
      "Batch: 150/296, Loss: 2.328050136566162\n",
      "Batch: 200/296, Loss: 1.8049954175949097\n",
      "Batch: 250/296, Loss: 1.7194204330444336\n",
      "Average Epoch 0 Loss: 1.9562066290024165\n",
      "Train Accuracy: 0.3847291171550751\n",
      "Validation Accuracy: 0.3556687831878662\n",
      "Batch: 0/296, Loss: 1.649522066116333\n",
      "Batch: 50/296, Loss: 1.8599255084991455\n",
      "Batch: 100/296, Loss: 1.8637504577636719\n",
      "Batch: 150/296, Loss: 1.8403440713882446\n",
      "Batch: 200/296, Loss: 1.706679344177246\n",
      "Batch: 250/296, Loss: 1.9440103769302368\n",
      "Average Epoch 1 Loss: 1.8250173217541463\n",
      "Train Accuracy: 0.4075404107570648\n",
      "Validation Accuracy: 0.3824203610420227\n",
      "Batch: 0/296, Loss: 1.9295743703842163\n",
      "Batch: 50/296, Loss: 1.6258264780044556\n",
      "Batch: 100/296, Loss: 1.7695704698562622\n",
      "Batch: 150/296, Loss: 2.1802866458892822\n",
      "Batch: 200/296, Loss: 1.6295177936553955\n",
      "Batch: 250/296, Loss: 1.9402633905410767\n",
      "Average Epoch 2 Loss: 1.7720013558864594\n",
      "Train Accuracy: 0.4160946309566498\n",
      "Validation Accuracy: 0.3936305642127991\n",
      "Batch: 0/296, Loss: 1.893974781036377\n",
      "Batch: 50/296, Loss: 1.8569588661193848\n",
      "Batch: 100/296, Loss: 2.342331647872925\n",
      "Batch: 150/296, Loss: 1.6051836013793945\n",
      "Batch: 200/296, Loss: 1.7752960920333862\n",
      "Batch: 250/296, Loss: 1.839692234992981\n",
      "Average Epoch 3 Loss: 1.7370255053848833\n",
      "Train Accuracy: 0.43795543909072876\n",
      "Validation Accuracy: 0.397197425365448\n",
      "Batch: 0/296, Loss: 1.7702580690383911\n",
      "Batch: 50/296, Loss: 1.6634074449539185\n",
      "Batch: 100/296, Loss: 1.7865005731582642\n",
      "Batch: 150/296, Loss: 2.0849828720092773\n",
      "Batch: 200/296, Loss: 1.7202690839767456\n",
      "Batch: 250/296, Loss: 1.8939054012298584\n",
      "Average Epoch 4 Loss: 1.725755181666967\n",
      "Train Accuracy: 0.44862183928489685\n",
      "Validation Accuracy: 0.42038214206695557\n",
      "Batch: 0/296, Loss: 1.5669811964035034\n",
      "Batch: 50/296, Loss: 1.8277499675750732\n",
      "Batch: 100/296, Loss: 1.8351773023605347\n",
      "Batch: 150/296, Loss: 1.7825015783309937\n",
      "Batch: 200/296, Loss: 1.506251335144043\n",
      "Batch: 250/296, Loss: 1.6075655221939087\n",
      "Average Epoch 5 Loss: 1.699909503798227\n",
      "Train Accuracy: 0.4465096592903137\n",
      "Validation Accuracy: 0.4117197096347809\n",
      "Batch: 0/296, Loss: 1.2973300218582153\n",
      "Batch: 50/296, Loss: 1.5018627643585205\n",
      "Batch: 100/296, Loss: 1.7127083539962769\n",
      "Batch: 150/296, Loss: 2.4215314388275146\n",
      "Batch: 200/296, Loss: 1.4819475412368774\n",
      "Batch: 250/296, Loss: 1.7662224769592285\n",
      "Average Epoch 6 Loss: 1.6756472446628519\n",
      "Train Accuracy: 0.44534799456596375\n",
      "Validation Accuracy: 0.41656050086021423\n",
      "Batch: 0/296, Loss: 2.1910605430603027\n",
      "Batch: 50/296, Loss: 1.5555641651153564\n",
      "Batch: 100/296, Loss: 1.7555140256881714\n",
      "Batch: 150/296, Loss: 1.6497153043746948\n",
      "Batch: 200/296, Loss: 1.7456579208374023\n",
      "Batch: 250/296, Loss: 1.4972573518753052\n",
      "Average Epoch 7 Loss: 1.673733591227918\n",
      "Train Accuracy: 0.46266767382621765\n",
      "Validation Accuracy: 0.43159234523773193\n",
      "Batch: 0/296, Loss: 1.8001850843429565\n",
      "Batch: 50/296, Loss: 1.515964150428772\n",
      "Batch: 100/296, Loss: 1.8239789009094238\n",
      "Batch: 150/296, Loss: 1.6994903087615967\n",
      "Batch: 200/296, Loss: 1.4663923978805542\n",
      "Batch: 250/296, Loss: 1.683972716331482\n",
      "Average Epoch 8 Loss: 1.6564460747145318\n",
      "Train Accuracy: 0.4644629955291748\n",
      "Validation Accuracy: 0.4313375651836395\n",
      "Batch: 0/296, Loss: 1.4468656778335571\n",
      "Batch: 50/296, Loss: 1.9200855493545532\n",
      "Batch: 100/296, Loss: 1.6028602123260498\n",
      "Batch: 150/296, Loss: 1.4047492742538452\n",
      "Batch: 200/296, Loss: 1.4913699626922607\n",
      "Batch: 250/296, Loss: 1.7635605335235596\n",
      "Average Epoch 9 Loss: 1.6495747147379696\n",
      "Train Accuracy: 0.4619283974170685\n",
      "Validation Accuracy: 0.4308280050754547\n",
      "Batch: 0/296, Loss: 1.4889349937438965\n",
      "Batch: 50/296, Loss: 1.2570586204528809\n",
      "Batch: 100/296, Loss: 1.5488988161087036\n",
      "Batch: 150/296, Loss: 1.7736334800720215\n",
      "Batch: 200/296, Loss: 1.6795437335968018\n",
      "Batch: 250/296, Loss: 1.9188288450241089\n",
      "Average Epoch 10 Loss: 1.635422967978426\n",
      "Train Accuracy: 0.47724154591560364\n",
      "Validation Accuracy: 0.4364331066608429\n",
      "Batch: 0/296, Loss: 1.6727001667022705\n",
      "Batch: 50/296, Loss: 1.8153032064437866\n",
      "Batch: 100/296, Loss: 1.6279594898223877\n",
      "Batch: 150/296, Loss: 1.1948540210723877\n",
      "Batch: 200/296, Loss: 1.6222631931304932\n",
      "Batch: 250/296, Loss: 1.8392895460128784\n",
      "Average Epoch 11 Loss: 1.630810320780084\n",
      "Train Accuracy: 0.47988173365592957\n",
      "Validation Accuracy: 0.4461146295070648\n",
      "Batch: 0/296, Loss: 1.7410151958465576\n",
      "Batch: 50/296, Loss: 1.6201401948928833\n",
      "Batch: 100/296, Loss: 1.4866573810577393\n",
      "Batch: 150/296, Loss: 1.8013429641723633\n",
      "Batch: 200/296, Loss: 1.1684575080871582\n",
      "Batch: 250/296, Loss: 1.9948405027389526\n",
      "Average Epoch 12 Loss: 1.6250231298240456\n",
      "Train Accuracy: 0.465096652507782\n",
      "Validation Accuracy: 0.4341401159763336\n",
      "Batch: 0/296, Loss: 1.5818357467651367\n",
      "Batch: 50/296, Loss: 1.6752945184707642\n",
      "Batch: 100/296, Loss: 1.4879740476608276\n",
      "Batch: 150/296, Loss: 1.5084174871444702\n",
      "Batch: 200/296, Loss: 1.577811360359192\n",
      "Batch: 250/296, Loss: 1.5661919116973877\n",
      "Average Epoch 13 Loss: 1.6282172045997672\n",
      "Train Accuracy: 0.4711162745952606\n",
      "Validation Accuracy: 0.4308280050754547\n",
      "Batch: 0/296, Loss: 1.5322904586791992\n",
      "Batch: 50/296, Loss: 1.4588019847869873\n",
      "Batch: 100/296, Loss: 1.6731271743774414\n",
      "Batch: 150/296, Loss: 1.3568267822265625\n",
      "Batch: 200/296, Loss: 1.6492239236831665\n",
      "Batch: 250/296, Loss: 1.4466623067855835\n",
      "Average Epoch 14 Loss: 1.616467059061334\n",
      "Train Accuracy: 0.4720667600631714\n",
      "Validation Accuracy: 0.4341401159763336\n",
      "Batch: 0/296, Loss: 1.6368260383605957\n",
      "Batch: 50/296, Loss: 2.188596248626709\n",
      "Batch: 100/296, Loss: 2.008610963821411\n",
      "Batch: 150/296, Loss: 1.8925819396972656\n",
      "Batch: 200/296, Loss: 1.4492566585540771\n",
      "Batch: 250/296, Loss: 1.149511456489563\n",
      "Average Epoch 15 Loss: 1.6066803046174951\n",
      "Train Accuracy: 0.478297621011734\n",
      "Validation Accuracy: 0.4420381784439087\n",
      "Batch: 0/296, Loss: 1.5578908920288086\n",
      "Batch: 50/296, Loss: 1.381644606590271\n",
      "Batch: 100/296, Loss: 1.761792540550232\n",
      "Batch: 150/296, Loss: 1.5786734819412231\n",
      "Batch: 200/296, Loss: 1.5337480306625366\n",
      "Batch: 250/296, Loss: 1.5035934448242188\n",
      "Average Epoch 16 Loss: 1.595364800981573\n",
      "Train Accuracy: 0.47079944610595703\n",
      "Validation Accuracy: 0.4328662157058716\n",
      "Batch: 0/296, Loss: 1.8783214092254639\n",
      "Batch: 50/296, Loss: 1.1093393564224243\n",
      "Batch: 100/296, Loss: 1.5725078582763672\n",
      "Batch: 150/296, Loss: 1.4330767393112183\n",
      "Batch: 200/296, Loss: 1.6683681011199951\n",
      "Batch: 250/296, Loss: 1.4361084699630737\n",
      "Average Epoch 17 Loss: 1.6090148572583456\n",
      "Train Accuracy: 0.478297621011734\n",
      "Validation Accuracy: 0.4341401159763336\n",
      "Batch: 0/296, Loss: 1.2694804668426514\n",
      "Batch: 50/296, Loss: 1.8794372081756592\n",
      "Batch: 100/296, Loss: 1.1723016500473022\n",
      "Batch: 150/296, Loss: 1.6683349609375\n",
      "Batch: 200/296, Loss: 1.4441097974777222\n",
      "Batch: 250/296, Loss: 1.947774887084961\n",
      "Average Epoch 18 Loss: 1.5855916130381662\n",
      "Train Accuracy: 0.49149858951568604\n",
      "Validation Accuracy: 0.44968149065971375\n",
      "Batch: 0/296, Loss: 1.580701470375061\n",
      "Batch: 50/296, Loss: 1.4190673828125\n",
      "Batch: 100/296, Loss: 1.5320770740509033\n",
      "Batch: 150/296, Loss: 1.550784707069397\n",
      "Batch: 200/296, Loss: 1.5959707498550415\n",
      "Batch: 250/296, Loss: 1.4963598251342773\n",
      "Average Epoch 19 Loss: 1.594671821272051\n",
      "Train Accuracy: 0.4866406321525574\n",
      "Validation Accuracy: 0.4387260973453522\n",
      "Batch: 0/296, Loss: 1.4707484245300293\n",
      "Batch: 50/296, Loss: 1.515203595161438\n",
      "Batch: 100/296, Loss: 1.4955193996429443\n",
      "Batch: 150/296, Loss: 1.4538768529891968\n",
      "Batch: 200/296, Loss: 1.6632143259048462\n",
      "Batch: 250/296, Loss: 1.991416096687317\n",
      "Average Epoch 20 Loss: 1.5916499173319019\n",
      "Train Accuracy: 0.4894920289516449\n",
      "Validation Accuracy: 0.4476432800292969\n",
      "Batch: 0/296, Loss: 1.50103759765625\n",
      "Batch: 50/296, Loss: 1.6676901578903198\n",
      "Batch: 100/296, Loss: 1.6991289854049683\n",
      "Batch: 150/296, Loss: 1.6020478010177612\n",
      "Batch: 200/296, Loss: 1.618957757949829\n",
      "Batch: 250/296, Loss: 1.7133737802505493\n",
      "Average Epoch 21 Loss: 1.58551107588652\n",
      "Train Accuracy: 0.4902312755584717\n",
      "Validation Accuracy: 0.4458598494529724\n",
      "Batch: 0/296, Loss: 1.4834345579147339\n",
      "Batch: 50/296, Loss: 1.772134780883789\n",
      "Batch: 100/296, Loss: 1.5383001565933228\n",
      "Batch: 150/296, Loss: 1.7341946363449097\n",
      "Batch: 200/296, Loss: 1.4268044233322144\n",
      "Batch: 250/296, Loss: 1.3955248594284058\n",
      "Average Epoch 22 Loss: 1.5762150001686972\n",
      "Train Accuracy: 0.48833033442497253\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.4450955092906952\n",
      "Batch: 0/296, Loss: 1.9683516025543213\n",
      "Batch: 50/296, Loss: 1.245261549949646\n",
      "Batch: 100/296, Loss: 1.5035545825958252\n",
      "Batch: 150/296, Loss: 1.533055067062378\n",
      "Batch: 200/296, Loss: 1.6730352640151978\n",
      "Batch: 250/296, Loss: 1.2822511196136475\n",
      "Average Epoch 23 Loss: 1.574874371692941\n",
      "Train Accuracy: 0.5004752278327942\n",
      "Validation Accuracy: 0.4486624002456665\n",
      "Batch: 0/296, Loss: 1.7769140005111694\n",
      "Batch: 50/296, Loss: 1.9017562866210938\n",
      "Batch: 100/296, Loss: 1.4387032985687256\n",
      "Batch: 150/296, Loss: 1.402949333190918\n",
      "Batch: 200/296, Loss: 1.555773138999939\n",
      "Batch: 250/296, Loss: 1.7426345348358154\n",
      "Average Epoch 24 Loss: 1.5745059062500257\n",
      "Train Accuracy: 0.492765873670578\n",
      "Validation Accuracy: 0.4448407292366028\n",
      "Batch: 0/296, Loss: 1.4886101484298706\n",
      "Batch: 50/296, Loss: 1.7159714698791504\n",
      "Batch: 100/296, Loss: 1.4792829751968384\n",
      "Batch: 150/296, Loss: 1.4341018199920654\n",
      "Batch: 200/296, Loss: 1.3759485483169556\n",
      "Batch: 250/296, Loss: 1.3293310403823853\n",
      "Average Epoch 25 Loss: 1.5751773602253683\n",
      "Train Accuracy: 0.5024818181991577\n",
      "Validation Accuracy: 0.4537579417228699\n",
      "Batch: 0/296, Loss: 1.516831636428833\n",
      "Batch: 50/296, Loss: 1.792788028717041\n",
      "Batch: 100/296, Loss: 1.44273841381073\n",
      "Batch: 150/296, Loss: 1.693535327911377\n",
      "Batch: 200/296, Loss: 1.4588017463684082\n",
      "Batch: 250/296, Loss: 1.563669204711914\n",
      "Average Epoch 26 Loss: 1.5679712651951894\n",
      "Train Accuracy: 0.49815186858177185\n",
      "Validation Accuracy: 0.4494267404079437\n",
      "Batch: 0/296, Loss: 1.6204559803009033\n",
      "Batch: 50/296, Loss: 1.3826823234558105\n",
      "Batch: 100/296, Loss: 1.560718297958374\n",
      "Batch: 150/296, Loss: 1.7348384857177734\n",
      "Batch: 200/296, Loss: 1.5630525350570679\n",
      "Batch: 250/296, Loss: 1.7557343244552612\n",
      "Average Epoch 27 Loss: 1.5566628708227261\n",
      "Train Accuracy: 0.4829443395137787\n",
      "Validation Accuracy: 0.4397451877593994\n",
      "Batch: 0/296, Loss: 1.2799402475357056\n",
      "Batch: 50/296, Loss: 1.4544227123260498\n",
      "Batch: 100/296, Loss: 1.5966366529464722\n",
      "Batch: 150/296, Loss: 1.4437966346740723\n",
      "Batch: 200/296, Loss: 1.4267429113388062\n",
      "Batch: 250/296, Loss: 1.3244531154632568\n",
      "Average Epoch 28 Loss: 1.5568223619783246\n",
      "Train Accuracy: 0.5019537806510925\n",
      "Validation Accuracy: 0.451719731092453\n",
      "Batch: 0/296, Loss: 1.5658116340637207\n",
      "Batch: 50/296, Loss: 1.4666537046432495\n",
      "Batch: 100/296, Loss: 1.4005790948867798\n",
      "Batch: 150/296, Loss: 1.4149398803710938\n",
      "Batch: 200/296, Loss: 1.4813122749328613\n",
      "Batch: 250/296, Loss: 1.6746419668197632\n",
      "Average Epoch 29 Loss: 1.5557645531119526\n",
      "Train Accuracy: 0.4902312755584717\n",
      "Validation Accuracy: 0.45248404145240784\n",
      "Batch: 0/296, Loss: 1.6211339235305786\n",
      "Batch: 50/296, Loss: 1.1901090145111084\n",
      "Batch: 100/296, Loss: 1.424337387084961\n",
      "Batch: 150/296, Loss: 2.1017208099365234\n",
      "Batch: 200/296, Loss: 1.4174048900604248\n",
      "Batch: 250/296, Loss: 1.4507869482040405\n",
      "Average Epoch 30 Loss: 1.5609414273419895\n",
      "Train Accuracy: 0.4959340989589691\n",
      "Validation Accuracy: 0.4514649510383606\n",
      "Batch: 0/296, Loss: 1.5600425004959106\n",
      "Batch: 100/296, Loss: 1.7543443441390991\n",
      "Batch: 150/296, Loss: 1.1655584573745728\n",
      "Batch: 200/296, Loss: 1.389595627784729\n",
      "Batch: 250/296, Loss: 1.2598861455917358\n",
      "Average Epoch 31 Loss: 1.5549908997239292\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-d42e7244a0b6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_epochs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m40\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-13-8440955c6bef>\u001b[0m in \u001b[0;36mrun_epochs\u001b[0;34m(epoch_num)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Epoch {} Loss: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m         \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Train Accuracy: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1.\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mcorrect\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0mcorrect\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mone_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_dl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-5ca70191ec0f>\u001b[0m in \u001b[0;36mone_epoch\u001b[0;34m(dl, model, loss_func, optimizer, device, phase)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mpreds_logit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_logit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/capstone/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    539\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 541\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    542\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    543\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-363cc648734d>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mimg_patches\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mimg_patches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg_patches\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg_patches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpermute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_epochs(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from torchvision ResNet, converted to v2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_channels, num_filters,\n",
    "                 first_layer_kernel_size, first_layer_conv_stride,\n",
    "                 blocks_per_layer_list, block_strides_list, block_fn,\n",
    "                 first_layer_padding=0,\n",
    "                 first_pool_size=None, first_pool_stride=None, first_pool_padding=0,\n",
    "                 growth_factor=2, norm_class=\"batch\", num_groups=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.first_conv = nn.Conv2d(\n",
    "            in_channels=input_channels, out_channels=num_filters,\n",
    "            kernel_size=first_layer_kernel_size,\n",
    "            stride=first_layer_conv_stride,\n",
    "            padding=first_layer_padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        # Diff: padding=SAME vs. padding=0\n",
    "        self.first_pool = nn.MaxPool2d(\n",
    "            kernel_size=first_pool_size,\n",
    "            stride=first_pool_stride,\n",
    "            padding=first_pool_padding,\n",
    "        )\n",
    "        self.norm_class = norm_class\n",
    "        self.num_groups = num_groups\n",
    "\n",
    "        block = self._resolve_block(block_fn)\n",
    "        self.layer_list = nn.ModuleList()\n",
    "        current_num_filters = num_filters\n",
    "        self.inplanes = num_filters\n",
    "        for i, (num_blocks, stride) in enumerate(zip(\n",
    "                blocks_per_layer_list, block_strides_list)):\n",
    "            self.layer_list.append(self._make_layer(\n",
    "                block=block,\n",
    "                planes=current_num_filters,\n",
    "                blocks=num_blocks,\n",
    "                stride=stride,\n",
    "            ))\n",
    "            current_num_filters *= growth_factor\n",
    "\n",
    "        self.final_bn = layers.resolve_norm_layer(\n",
    "            # current_num_filters // growth_factor\n",
    "            current_num_filters // growth_factor * block.expansion,\n",
    "            norm_class,\n",
    "            num_groups\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize()\n",
    "\n",
    "        # Expose attributes for downstream dimension computation\n",
    "        self.num_filters = num_filters\n",
    "        self.growth_factor = growth_factor\n",
    "        self.block = block\n",
    "        self.num_filter_last_seq = current_num_filters // growth_factor * block.expansion\n",
    "\n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        intermediate = []\n",
    "        h = self.first_conv(x)\n",
    "        h = self.first_pool(h)\n",
    "\n",
    "        if return_intermediate:\n",
    "            intermediate.append(h)\n",
    "        for i, layer in enumerate(self.layer_list):\n",
    "            h = layer(h)\n",
    "            if return_intermediate:\n",
    "                intermediate.append(h)\n",
    "\n",
    "        h = self.final_bn(h)\n",
    "        h = self.relu(h)\n",
    "\n",
    "        if return_intermediate:\n",
    "            return h, intermediate\n",
    "        else:\n",
    "            return h\n",
    "\n",
    "    @classmethod\n",
    "    def _resolve_block(cls, block_fn):\n",
    "        if block_fn == \"normal\":\n",
    "            return layers.BasicBlockV2_dbt\n",
    "        elif block_fn == \"bottleneck\":\n",
    "            return layers.BottleneckV2_dbt\n",
    "        else:\n",
    "            raise KeyError(block_fn)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        # downsample = None\n",
    "        # if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                      kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(planes * block.expansion),\n",
    "        )\n",
    "\n",
    "        layers_ = [\n",
    "            block(self.inplanes, planes, stride, downsample, self.norm_class, self.num_groups)\n",
    "        ]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers_.append(block(self.inplanes, planes, norm_class=self.norm_class, num_groups=self.num_groups))\n",
    "\n",
    "        return nn.Sequential(*layers_)\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            self._layer_init(m)\n",
    "\n",
    "    @classmethod\n",
    "    def _layer_init(cls, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # From original\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #             nn.init.xavier_normal_(m.weight)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.GroupNorm):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @classmethod\n",
    "    def from_parameters(cls, parameters):\n",
    "        return cls(\n",
    "            input_channels=parameters[\"input_channels\"],\n",
    "            num_filters=parameters[\"num_filters\"],\n",
    "            first_layer_kernel_size=parameters[\"first_layer_kernel_size\"],\n",
    "            first_layer_conv_stride=parameters[\"first_layer_conv_stride\"],\n",
    "            first_layer_padding=parameters.get(\"first_layer_padding\", 0),\n",
    "            blocks_per_layer_list=parameters[\"blocks_per_layer_list\"],\n",
    "            block_strides_list=parameters[\"block_strides_list\"],\n",
    "            block_fn=parameters[\"block_fn\"],\n",
    "            first_pool_size=parameters[\"first_pool_size\"],\n",
    "            first_pool_stride=parameters[\"first_pool_stride\"],\n",
    "            first_pool_padding=parameters.get(\"first_pool_padding\", 0),\n",
    "            growth_factor=parameters.get(\"growth_factor\", 2),\n",
    "            norm_class=parameters.get(\"norm_class\", \"batch\"),\n",
    "            num_groups=parameters.get(\"num_groups\", 1)\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_22(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            attention=False,\n",
    "            dropout=0.0,\n",
    "            hidden_size=256,\n",
    "\n",
    "            # resnet hyperparameters\n",
    "            #         input_channels=1,\n",
    "            first_layer_kernel_size=7,\n",
    "            first_layer_conv_stride=2,\n",
    "            first_pool_size=3,\n",
    "            first_pool_stride=2,\n",
    "            first_layer_padding=0,\n",
    "            first_pool_padding=0,\n",
    "            growth_factor=2,\n",
    "\n",
    "            # resnet22 settings\n",
    "            num_filters=16,\n",
    "            blocks_per_layer_list=[2, 2, 2, 2, 2],\n",
    "            block_strides_list=[1, 2, 2, 2, 2],\n",
    "            block_fn=\"normal\",\n",
    "            norm_class=\"group\",\n",
    "            num_groups=8,\n",
    "\n",
    "            num_image_slices_per_net=1,\n",
    "    ):\n",
    "        super(ResNet_22, self).__init__()\n",
    "\n",
    "        self.num_image_slices_per_net = num_image_slices_per_net\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.resnet = ResNet(\n",
    "            input_channels=3,\n",
    "            first_layer_kernel_size=first_layer_kernel_size,\n",
    "            first_layer_conv_stride=first_layer_conv_stride,\n",
    "            first_pool_size=first_pool_size,\n",
    "            first_pool_stride=first_pool_stride,\n",
    "            num_filters=num_filters,\n",
    "            blocks_per_layer_list=blocks_per_layer_list,\n",
    "            block_strides_list=block_strides_list,\n",
    "            block_fn=block_fn,\n",
    "            first_layer_padding=first_layer_padding,\n",
    "            first_pool_padding=first_pool_padding,\n",
    "            growth_factor=growth_factor,\n",
    "            norm_class=norm_class,\n",
    "            num_groups=num_groups,\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # use avgpool rather than torch.mean\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        h = self.resnet(x)\n",
    "        # Shape of pooled_h is [4, 256, 1, 1]\n",
    "        pooled_h = self.avgpool(h)\n",
    "        return pooled_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
