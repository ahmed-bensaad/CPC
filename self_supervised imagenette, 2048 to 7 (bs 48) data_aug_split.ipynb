{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import collections as col\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pdb\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from pytorch_datasets import DiagnosticInpainted\n",
    "import models\n",
    "import layers\n",
    "import utilities.reading_images as reading_images\n",
    "from utilities.loading import get_single_image\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        \n",
    "        # Conv2d: (input_channels, output_channels, kernel_size, padding)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(latent_dim, latent_dim, (1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConstantPad2d((1, 1, 0, 0), 0),\n",
    "            nn.Conv2d(latent_dim, latent_dim, (1, 3)),\n",
    "            nn.ConstantPad2d((0, 0, 0, 1), 0),\n",
    "            nn.Conv2d(latent_dim, latent_dim, (2, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(latent_dim, latent_dim, (1, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, latents):\n",
    "        \n",
    "        # latents: [B, C, H, W]\n",
    "        cres = latents\n",
    "        \n",
    "        for _ in range(5):\n",
    "            c = self.model(cres)\n",
    "            cres = cres + c\n",
    "        cres = self.relu(cres)\n",
    "        return cres      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boom = torch.tensor(np.arange(20).reshape(2, 10))\n",
    "boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9]],\n",
       "\n",
       "        [[10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boom.reshape(2, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC_loss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CPC_loss, self).__init__()\n",
    "        self.pixel_cnn = PixelCNN(2048)\n",
    "        self.target_to_32 = nn.Conv2d(2048, 7, kernel_size = (1, 1))\n",
    "        \n",
    "        self.conv_1 = nn.Conv2d(2048, 7, kernel_size = (1, 1))\n",
    "        self.conv_2 = nn.Conv2d(2048, 7, kernel_size = (1, 1))\n",
    "        self.conv_3 = nn.Conv2d(2048, 7, kernel_size = (1, 1))\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, latents, device, target_dim = 7, steps_to_ignore = 2, steps_to_predict = 3, emb_scale = 0.1):\n",
    "        # latents: [B, D, H, W]\n",
    "        # aka:     [B, 512, 6, 6]\n",
    "        loss = 0.0\n",
    "        latents = latents.to(device)\n",
    "        context = self.pixel_cnn(latents) # These are the c's (apply pixelCNN to Z's)\n",
    "        targets = self.target_to_32(latents)\n",
    "#         targets = latents\n",
    "        \n",
    "        batch_dim, emb_dim, col_dim, row_dim = targets.shape\n",
    "        targets = targets.reshape(-1, target_dim)\n",
    "        \n",
    "        # Trying to do the arbitrary context vector\n",
    "        index = np.random.choice(a = [0, 1, 2])\n",
    "        context = context[:, :, index, :].unsqueeze(3) # [2, 512, 6, 1]\n",
    "        \n",
    "        \n",
    "        preds_1 = self.conv_1(context).reshape(-1, target_dim) * emb_scale\n",
    "        preds_2 = self.conv_2(context).reshape(-1, target_dim) * emb_scale\n",
    "        preds_3 = self.conv_3(context).reshape(-1, target_dim) * emb_scale\n",
    "        \n",
    "        logits_1 = torch.matmul(preds_1, targets.permute(1, 0)) # 12 by 512, 512 by 72 --> 12 by 72\n",
    "        logits_2 = torch.matmul(preds_2, targets.permute(1, 0))\n",
    "        logits_3 = torch.matmul(preds_3, targets.permute(1, 0))\n",
    "        \n",
    "        total_elements = batch_dim * row_dim\n",
    "        b = np.arange(total_elements) / (row_dim)\n",
    "        b = b.astype(int)\n",
    "        col = np.arange(total_elements) % (row_dim)\n",
    "\n",
    "        labels_1 = b * col_dim * row_dim + 3 * row_dim + col\n",
    "        labels_2 = labels_1 + 6\n",
    "        labels_3 = labels_2 + 6\n",
    "        \n",
    "        loss += self.loss_func(logits_1, torch.LongTensor(labels_1).to(device))\n",
    "        loss += self.loss_func(logits_2, torch.LongTensor(labels_2).to(device))\n",
    "        loss += self.loss_func(logits_3, torch.LongTensor(labels_3).to(device))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         for i in range(steps_to_ignore, steps_to_predict):\n",
    "# #             pdb.set_trace()\n",
    "#             col_dim_i = col_dim - i - 1  # 6 - 2 - 1 = 3\n",
    "#             total_elements = batch_dim * col_dim_i * row_dim\n",
    "#             preds_i = self.conv_preds(context)\n",
    "#             preds_i = preds_i[:, :, :(i+1), :] * emb_scale   # [B, 64, 6, 6] ---> [B, 64, 3, 6]\n",
    "#             preds_i = preds_i.reshape(-1, target_dim)\n",
    "            \n",
    "#             logits = torch.matmul(preds_i, targets.permute(1, 0)) # 18 by 64, 64 by 36 ---> 18 by 36\n",
    "            \n",
    "#             b = np.arange(total_elements) / (col_dim_i * row_dim)\n",
    "#             b = b.astype(int)\n",
    "#             col = np.arange(total_elements) % (col_dim_i * row_dim)\n",
    "#             labels = b * col_dim * row_dim + (i + 1) * row_dim + col\n",
    "#             labels = torch.LongTensor(labels).to(device)\n",
    "#             logits = logits.to(device)\n",
    "            \n",
    "#             rand = np.random.choice(a=[False, True], size = (logits.shape[0],))\n",
    "#             logits = logits[rand, :]\n",
    "#             labels = labels[rand]\n",
    "#             loss += self.loss_func(logits, labels)\n",
    "        return loss\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_elements = 1 * 3 * 6\n",
    "b = np.arange(18) / (3 * 6)\n",
    "b = b.astype(int)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = np.arange(18) % (3 * 6)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = b * 6 * 6 + 3*6 + col\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation in the paper is unclear.\n",
    "# I'm going to go with WF. \n",
    "\n",
    "# NCE Loss\n",
    "# Questions: Is the dimension of Z (B*patches) or (B). \n",
    "#            I think it's (B, 6, 6, 4096)\n",
    "\n",
    "class CPCLossNCE(nn.Module):\n",
    "    \n",
    "    def nce_loss(self, z_hat, pos_scores, negative_samples, mask_mat, device, epoch_num, batch_num):\n",
    "        \n",
    "        z_hat = z_hat.to(device)\n",
    "        pos_scores = pos_scores.to(device)\n",
    "        negative_samples = negative_samples.to(device)\n",
    "        mask_mat = mask_mat.to(device)\n",
    "                \n",
    "        # (b, 1)\n",
    "        pos_scores = pos_scores.float()\n",
    "        batch_size, emb_dim = z_hat.size()\n",
    "        nb_feat_vectors = negative_samples.size(1) // batch_size # 36 of them, if 6 by 6 wireframes. \n",
    "        \n",
    "        # (b, b) -> (b, b, nb_feat_vectors)\n",
    "        # all zeros with ones in diagonal tensor... (ie: b1 b1 are all 1s, b1 b2 are all zeros)\n",
    "        mask_pos = mask_mat.unsqueeze(dim=2).expand(-1, -1, nb_feat_vectors).float()\n",
    "        \n",
    "        # negative mask\n",
    "        mask_neg = 1. - mask_pos\n",
    "        \n",
    "        # ----------------------\n",
    "        # ALL SCORES computation\n",
    "        # (visualize in your mind a batch size of 2, 36-length segments) \n",
    "        # (b, dim) x (dim, nb_feats*b) -> (b, b, nb_feats)\n",
    "        raw_scores = torch.mm(z_hat, negative_samples)\n",
    "        raw_scores = raw_scores.reshape(batch_size, batch_size, nb_feat_vectors).float()\n",
    "        \n",
    "        # EXTRACT NEGATIVE SCORES\n",
    "        # (batch_size, batch_size, nb_feat_vectors)\n",
    "        # HE'S TAKING THE NEGATIVE SAMPLES FROM THE OTHER MINIBATCHES\n",
    "        # A GIVEN Z_HAT IS ONLY MULTIPLIED BY Z'S FROM OTHER MINIBATCHES\n",
    "        neg_scores = (mask_neg * raw_scores)\n",
    "        # ----------------------\n",
    "        \n",
    "        # (b, b, nb_feat_vectors) -> (batch_size, batch_size * nb_feat_vectors) \n",
    "        neg_scores = neg_scores.reshape(batch_size, -1)\n",
    "        mask_neg = mask_neg.reshape(batch_size, -1)\n",
    "        \n",
    "        # STABLE SOFTMAX\n",
    "        # (n_batch_gpu, 1)\n",
    "        neg_maxes = torch.max(neg_scores, dim=1, keepdim=True)[0]\n",
    "        \n",
    "        # DENOMINATOR\n",
    "        # sum over only negative samples (none from the diagonal)\n",
    "        neg_sumexp = (mask_neg * torch.exp(neg_scores - neg_maxes)).sum(dim=1, keepdim=True)\n",
    "        all_logsumexp = torch.log(torch.exp(pos_scores - neg_maxes) + neg_sumexp)\n",
    "        \n",
    "        # NUMERATOR\n",
    "        # compute numerators for the NCE log-softmaxes\n",
    "        pos_shiftexp = pos_scores - neg_maxes\n",
    "        \n",
    "        # FULL NCE\n",
    "#         if epoch_num > 2 and batch_num == 100:\n",
    "#             pdb.set_trace()\n",
    "        nce_scores = pos_shiftexp - all_logsumexp\n",
    "        nce_scores = -nce_scores.mean()\n",
    "        \n",
    "#         if np.isnan(nce_scores.cpu().detach().numpy()):\n",
    "#             pdb.set_trace()\n",
    "#             print('boom - nceloss')\n",
    "            \n",
    "        \n",
    "        return nce_scores\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, Z, C, W_list, device, epoch_num, batch_num):\n",
    "        '''\n",
    "        param Z: latent vecs (B, D, H, W)\n",
    "        param C: context vecs (B, D, H, W)\n",
    "        param W_list: list of k-1 W projections\n",
    "        '''\n",
    "        \n",
    "        # (b, dim, w, h)\n",
    "        batch_size, emb_dim, h, w = Z.size()\n",
    "        \n",
    "        # (10 x 10 identity matrix)\n",
    "        diag_mat = torch.eye(batch_size)\n",
    "        diag_mat = diag_mat.float()\n",
    "        \n",
    "        losses = []\n",
    "        # calculate loss for each k\n",
    "        \n",
    "        # Below operations preserve raster order (for B, D, H, W) = (1, 5, 2, 2) check.\n",
    "        # Z_neg holds all z vecs. \n",
    "        Z_neg = Z.permute(1, 0, 2, 3).reshape(emb_dim, -1)\n",
    "        \n",
    "        \n",
    "        for i in range(0, h-1):\n",
    "            for j in range(0, w):\n",
    "                cij = C[:, :, i, j]   # B by D\n",
    "                \n",
    "                for k in range(i+1, h): # predict on all vectors in the same column, but below current wireframe. \n",
    "                    Wk = W_list[str(k)]\n",
    "                    \n",
    "                    z_hat_ikj = Wk(cij)\n",
    "                    zikj = Z[:, :, k, j]\n",
    "                    \n",
    "                    # BATCH DOT PRODUCT\n",
    "                    # (b, d) x (b, d) -> (b, 1)\n",
    "                    pos_scores = torch.bmm(z_hat_ikj.unsqueeze(1), zikj.unsqueeze(2))\n",
    "                    pos_scores = pos_scores.squeeze(-1).squeeze(-1)\n",
    "                    \n",
    "                    loss = self.nce_loss(z_hat_ikj, pos_scores, Z_neg, diag_mat, device, epoch_num, batch_num)\n",
    "                    if np.isinf(loss.item()):\n",
    "                        pdb.set_trace()\n",
    "                        print(\"inf -- inside inner for loop\")\n",
    "                    if np.isnan(loss.item()):\n",
    "                        pdb.set_trace()\n",
    "                        print(\"inside inner for loop\")\n",
    "                    losses.append(loss)\n",
    "                    \n",
    "                    \n",
    "        losses = torch.stack(losses)\n",
    "        loss = losses.mean()\n",
    "#         if np.isnan(loss.item()):\n",
    "#             pdb.set_trace()\n",
    "#             print('boom')\n",
    "        return loss           \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "boom = torch.rand(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "boom = boom.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(boom, 3, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_raster_patchify(img, size = 80, overlap = 32):\n",
    "    '''\n",
    "    Left-to-right, top to bottom.\n",
    "    Assumes img is (3, 240, 240).\n",
    "    '''\n",
    "    patches = []\n",
    "     \n",
    "    h = -32\n",
    "    w = -32\n",
    "    for i in range(6):\n",
    "        h = h + 32\n",
    "        for j in range(6):\n",
    "            w = w + 32\n",
    "            channel = np.random.randint(3)\n",
    "            processed_img = np.repeat(np.expand_dims(img[channel, h:h+size, w:w+size], axis=0), 3, axis=0)\n",
    "            if np.random.randint(2):\n",
    "                processed_img = np.flip(processed_img, axis=2)\n",
    "            patches.append(torch.tensor(np.ascontiguousarray(processed_img)))\n",
    "        w = -32\n",
    "            \n",
    "    return patches\n",
    "\n",
    "\n",
    "def val_raster_patchify(img, size = 80, overlap = 32):\n",
    "    '''\n",
    "    Left-to-right, top to bottom.\n",
    "    Assumes img is (3, 240, 240).\n",
    "    '''\n",
    "    patches = []\n",
    "     \n",
    "    h = -32\n",
    "    w = -32\n",
    "    for i in range(6):\n",
    "        h = h + 32\n",
    "        for j in range(6):\n",
    "            w = w + 32\n",
    "            patches.append(img[:, h:h+size, w:w+size])\n",
    "        w = -32\n",
    "            \n",
    "    return patches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(patches[35].permute(1, 2, 0))\n",
    "\n",
    "# plt.imshow(trainset[0][0].permute(1, 2, 0))\n",
    "# plt.scatter(80,80,color='r')\n",
    "# plt.scatter(80+32,80,color='r')\n",
    "# plt.scatter(80+64,80,color='r')\n",
    "# plt.scatter(80+96,80,color='r')\n",
    "# plt.scatter(80+128,80,color='r')\n",
    "# plt.scatter(80+160,80,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(img_list):\n",
    "    patches = []\n",
    "    for (img, label) in img_list:\n",
    "        img_patches = train_raster_patchify(img)\n",
    "        patches.append(torch.stack(img_patches))\n",
    "        \n",
    "    return patches\n",
    "\n",
    "def val_collate_fn(img_list):\n",
    "    patches = []\n",
    "    for (img, label) in img_list:\n",
    "        img_patches = val_raster_patchify(img)\n",
    "        patches.append(torch.stack(img_patches))\n",
    "        \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.RandomCrop(240),\n",
    "    transforms.ColorJitter(brightness=(0.55, 1), contrast=(0.5, 1), saturation=(0.5, 1), hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(240),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "trainset = datasets.ImageFolder(\n",
    "    root = '/gpfs/data/geraslab/Vish/imagenette2-320/train/',\n",
    "    transform = train_transform\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(trainset, batch_size=48, shuffle=True, collate_fn=train_collate_fn)\n",
    "\n",
    "valset = datasets.ImageFolder(\n",
    "    root = '/gpfs/data/geraslab/Vish/imagenette2-320/val/',\n",
    "    transform = val_transform\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(valset, batch_size=48, shuffle=True, collate_fn=val_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_batchnorm(model):\n",
    "    model.bn1 = Identity()\n",
    "    model.layer1[0].bn1 = Identity()\n",
    "    model.layer1[0].bn2 = Identity()\n",
    "    model.layer1[0].bn3 = Identity()\n",
    "    model.layer1[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer1[1].bn1 = Identity()\n",
    "    model.layer1[1].bn2 = Identity()\n",
    "    model.layer1[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer1[2].bn1 = Identity()\n",
    "    model.layer1[2].bn2 = Identity()\n",
    "    model.layer1[2].bn3 = Identity()\n",
    "    \n",
    "    model.layer2[0].bn1 = Identity()\n",
    "    model.layer2[0].bn2 = Identity()\n",
    "    model.layer2[0].bn3 = Identity()\n",
    "    model.layer2[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer2[1].bn1 = Identity()\n",
    "    model.layer2[1].bn2 = Identity()\n",
    "    model.layer2[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer2[2].bn1 = Identity()\n",
    "    model.layer2[2].bn2 = Identity()\n",
    "    model.layer2[2].bn3 = Identity()\n",
    "    \n",
    "    model.layer2[3].bn1 = Identity()\n",
    "    model.layer2[3].bn2 = Identity()\n",
    "    model.layer2[3].bn3 = Identity()\n",
    "    \n",
    "    model.layer3[0].bn1 = Identity()\n",
    "    model.layer3[0].bn2 = Identity()\n",
    "    model.layer3[0].bn3 = Identity()\n",
    "    model.layer3[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer3[1].bn1 = Identity()\n",
    "    model.layer3[1].bn2 = Identity()\n",
    "    model.layer3[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer3[2].bn1 = Identity()\n",
    "    model.layer3[2].bn2 = Identity()\n",
    "    model.layer3[2].bn3 = Identity()\n",
    "    \n",
    "    \n",
    "    model.layer3[3].bn1 = Identity()\n",
    "    model.layer3[3].bn2 = Identity()\n",
    "    model.layer3[3].bn3 = Identity()\n",
    "    \n",
    "    \n",
    "    model.layer3[4].bn1 = Identity()\n",
    "    model.layer3[4].bn2 = Identity()\n",
    "    model.layer3[4].bn3 = Identity()\n",
    "    \n",
    "    model.layer3[5].bn1 = Identity()\n",
    "    model.layer3[5].bn2 = Identity()\n",
    "    model.layer3[5].bn3 = Identity()\n",
    "    \n",
    "#     model.layer4 = Identity()\n",
    "    \n",
    "    model.layer4[0].bn1 = Identity()\n",
    "    model.layer4[0].bn2 = Identity()\n",
    "    model.layer4[0].bn3 = Identity()\n",
    "    model.layer4[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer4[1].bn1 = Identity()\n",
    "    model.layer4[1].bn2 = Identity()\n",
    "    model.layer4[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer4[2].bn1 = Identity()\n",
    "    model.layer4[2].bn2 = Identity()\n",
    "    model.layer4[2].bn3 = Identity()\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "#     model.layer4[0].bn1 = Identity()\n",
    "#     model.layer4[0].bn2 = Identity()\n",
    "#     model.layer4[0].downsample[1] = Identity()\n",
    "#     model.layer4[1].bn1 = Identity()\n",
    "#     model.layer4[1].bn2 = Identity()\n",
    "#     model.layer4[2].bn1 = Identity()\n",
    "#     model.layer4[2].bn2 = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CPC, self).__init__()\n",
    "        self.encoder = torchvision.models.resnet50()\n",
    "        self.encoder.fc = Identity()\n",
    "        remove_batchnorm(self.encoder)\n",
    "#         self.pixel_cnn = PixelCNN(1024)\n",
    "        self.nce_loss = CPC_loss()\n",
    "#         self.nce_loss = CPCLossNCE()\n",
    "        \n",
    "#         # W transforms (k > 0)\n",
    "#         self.W_list = {}\n",
    "#         for k in range(1, 6):\n",
    "#             w = torch.nn.Linear(512, 512)\n",
    "#             self.W_list[str(k)] = w\n",
    "\n",
    "#         self.W_list = nn.ModuleDict(self.W_list)\n",
    "        \n",
    "\n",
    "    def forward(self, x, device, epoch_num, batch_num):\n",
    "        Z = []\n",
    "#         C = []\n",
    "        for img_patches in x:\n",
    "            img_patches = img_patches.to(device)\n",
    "            z = self.encoder(img_patches).squeeze()\n",
    "            z = z.unsqueeze(0).permute(0, 2, 1).reshape(1, 2048, 6, 6)\n",
    "            Z.append(z)\n",
    "#             c = self.pixel_cnn(z)\n",
    "#             C.append(c)\n",
    "\n",
    "        Z = torch.stack(Z).squeeze(1)\n",
    "#         C = torch.stack(C).squeeze(1)\n",
    "\n",
    "        loss = self.nce_loss(Z, device)\n",
    "        \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(dl, model, optimizer, device, epoch_num, phase = 'train'):\n",
    "    if phase == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "#         for m in model.modules():\n",
    "#             if isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.track_running_stats = False\n",
    "    losses = []\n",
    "    for i, x in enumerate(dl):\n",
    "        if phase == 'train':\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss = model(x, device, epoch_num, i)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if phase == 'train': \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(\"Batch: {}/{}, Loss: {}\".format(i, len(dl), loss.item())) \n",
    "            \n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs(epoch_num):\n",
    "    torch.cuda.set_device(6)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = CPC().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 2e-4, weight_decay=1e-5, eps=1e-8)\n",
    "    \n",
    "    # val loss: 0.02\n",
    "#     pretrained_dict = torch.load('paper_self_supervised_rc_best_val.pt')\n",
    "#     model_dict = model.state_dict()\n",
    "\n",
    "#     # 1. filter out unnecessary keys\n",
    "#     pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "#     # 2. overwrite entries in the existing state dict\n",
    "#     model_dict.update(pretrained_dict) \n",
    "#     # 3. load the new state dict\n",
    "#     model.load_state_dict(model_dict)\n",
    "    \n",
    "    best_val_loss = 1000000\n",
    "    for i in range(epoch_num):\n",
    "        print(\"Started epoch {}\\n\".format(i))\n",
    "        avg_train_loss = one_epoch(train_dl, model, optimizer, device, i, phase = 'train')\n",
    "        print(\"Average Epoch {} Loss: {}\\n\".format(i, avg_train_loss))\n",
    "        avg_val_loss = one_epoch(val_dl, model, optimizer, device, i, phase = 'val')\n",
    "        print(\"Validation Loss: {}\\n\".format(avg_val_loss))\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"paper_self_supervised_rc_best_val.pt\")\n",
    "            print(\"\\nSaved model with best validation loss: {}\".format(best_val_loss))\n",
    "        \n",
    "#         if i in [1, 10, 20, 25]:\n",
    "#             torch.save(model.state_dict(), \"paper_self_supervised_rc_{}.pt\".format(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started epoch 0\n",
      "\n",
      "Batch: 0/198, Loss: 33.275901794433594\n",
      "Batch: 50/198, Loss: 20.052383422851562\n",
      "Batch: 100/198, Loss: 18.8383846282959\n",
      "Batch: 150/198, Loss: 18.408245086669922\n",
      "Average Epoch 0 Loss: 19.914401690165203\n",
      "\n",
      "Validation Loss: 19.49367869772562\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 19.49367869772562\n",
      "Started epoch 1\n",
      "\n",
      "Batch: 0/198, Loss: 18.515146255493164\n",
      "Batch: 50/198, Loss: 18.833221435546875\n",
      "Batch: 100/198, Loss: 18.224397659301758\n",
      "Batch: 150/198, Loss: 18.088422775268555\n",
      "Average Epoch 1 Loss: 18.115549092340952\n",
      "\n",
      "Validation Loss: 18.38685610236191\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 18.38685610236191\n",
      "Started epoch 2\n",
      "\n",
      "Batch: 0/198, Loss: 17.6875\n",
      "Batch: 50/198, Loss: 17.938867568969727\n",
      "Batch: 100/198, Loss: 18.03594970703125\n",
      "Batch: 150/198, Loss: 17.379573822021484\n",
      "Average Epoch 2 Loss: 17.785961565345225\n",
      "\n",
      "Validation Loss: 17.72115477120004\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 17.72115477120004\n",
      "Started epoch 3\n",
      "\n",
      "Batch: 0/198, Loss: 16.753578186035156\n",
      "Batch: 50/198, Loss: 16.773727416992188\n",
      "Batch: 100/198, Loss: 15.259005546569824\n",
      "Batch: 150/198, Loss: 16.506153106689453\n",
      "Average Epoch 3 Loss: 15.741432782375451\n",
      "\n",
      "Validation Loss: 12.469649733566657\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 12.469649733566657\n",
      "Started epoch 4\n",
      "\n",
      "Batch: 0/198, Loss: 13.082928657531738\n",
      "Batch: 50/198, Loss: 12.677739143371582\n",
      "Batch: 100/198, Loss: 13.035877227783203\n",
      "Batch: 150/198, Loss: 12.579082489013672\n",
      "Average Epoch 4 Loss: 13.099400597389298\n",
      "\n",
      "Validation Loss: 13.0874357107209\n",
      "\n",
      "Started epoch 5\n",
      "\n",
      "Batch: 0/198, Loss: 11.442359924316406\n",
      "Batch: 50/198, Loss: 11.492303848266602\n",
      "Batch: 100/198, Loss: 12.713083267211914\n",
      "Batch: 150/198, Loss: 11.431035995483398\n",
      "Average Epoch 5 Loss: 12.096036434173584\n",
      "\n",
      "Validation Loss: 11.01897853758277\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 11.01897853758277\n",
      "Started epoch 6\n",
      "\n",
      "Batch: 0/198, Loss: 12.721752166748047\n",
      "Batch: 50/198, Loss: 10.848739624023438\n",
      "Batch: 100/198, Loss: 10.235025405883789\n",
      "Batch: 150/198, Loss: 11.378802299499512\n",
      "Average Epoch 6 Loss: 11.208104612851384\n",
      "\n",
      "Validation Loss: 11.599403904705513\n",
      "\n",
      "Started epoch 7\n",
      "\n",
      "Batch: 0/198, Loss: 12.666292190551758\n",
      "Batch: 50/198, Loss: 11.669672966003418\n",
      "Batch: 100/198, Loss: 10.285090446472168\n",
      "Batch: 150/198, Loss: 9.757925987243652\n",
      "Average Epoch 7 Loss: 10.19840036016522\n",
      "\n",
      "Validation Loss: 9.220942247204665\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 9.220942247204665\n",
      "Started epoch 8\n",
      "\n",
      "Batch: 0/198, Loss: 10.637081146240234\n",
      "Batch: 50/198, Loss: 8.489282608032227\n",
      "Batch: 100/198, Loss: 9.971051216125488\n",
      "Batch: 150/198, Loss: 9.090625762939453\n",
      "Average Epoch 8 Loss: 9.361080280458085\n",
      "\n",
      "Validation Loss: 9.324360591609304\n",
      "\n",
      "Started epoch 9\n",
      "\n",
      "Batch: 0/198, Loss: 10.900863647460938\n",
      "Batch: 50/198, Loss: 8.701096534729004\n",
      "Batch: 100/198, Loss: 7.887183666229248\n",
      "Batch: 150/198, Loss: 8.128989219665527\n",
      "Average Epoch 9 Loss: 9.054537361318415\n",
      "\n",
      "Validation Loss: 7.38380531566899\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 7.38380531566899\n",
      "Started epoch 10\n",
      "\n",
      "Batch: 0/198, Loss: 8.029648780822754\n",
      "Batch: 50/198, Loss: 7.991002082824707\n",
      "Batch: 100/198, Loss: 7.560405731201172\n",
      "Batch: 150/198, Loss: 9.534481048583984\n",
      "Average Epoch 10 Loss: 8.61451272771816\n",
      "\n",
      "Validation Loss: 9.878256472145639\n",
      "\n",
      "Started epoch 11\n",
      "\n",
      "Batch: 0/198, Loss: 8.947944641113281\n",
      "Batch: 50/198, Loss: 8.91015625\n",
      "Batch: 100/198, Loss: 7.882835388183594\n",
      "Batch: 150/198, Loss: 7.986145973205566\n",
      "Average Epoch 11 Loss: 8.497218288556494\n",
      "\n",
      "Validation Loss: 7.470267301652489\n",
      "\n",
      "Started epoch 12\n",
      "\n",
      "Batch: 0/198, Loss: 6.603875160217285\n",
      "Batch: 50/198, Loss: 9.353200912475586\n",
      "Batch: 100/198, Loss: 8.405767440795898\n",
      "Batch: 150/198, Loss: 9.215095520019531\n",
      "Average Epoch 12 Loss: 8.416118063107886\n",
      "\n",
      "Validation Loss: 8.213090698893478\n",
      "\n",
      "Started epoch 13\n",
      "\n",
      "Batch: 0/198, Loss: 8.785690307617188\n",
      "Batch: 50/198, Loss: 8.750207901000977\n",
      "Batch: 100/198, Loss: 7.873908042907715\n",
      "Batch: 150/198, Loss: 7.503523826599121\n",
      "Average Epoch 13 Loss: 8.4060377195628\n",
      "\n",
      "Validation Loss: 7.510199721266583\n",
      "\n",
      "Started epoch 14\n",
      "\n",
      "Batch: 0/198, Loss: 7.566222667694092\n",
      "Batch: 50/198, Loss: 10.740569114685059\n",
      "Batch: 100/198, Loss: 7.1409502029418945\n",
      "Batch: 150/198, Loss: 6.317739963531494\n",
      "Average Epoch 14 Loss: 8.052438971972224\n",
      "\n",
      "Validation Loss: 7.173966303104308\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 7.173966303104308\n",
      "Started epoch 15\n",
      "\n",
      "Batch: 0/198, Loss: 7.080878257751465\n",
      "Batch: 50/198, Loss: 8.062202453613281\n",
      "Batch: 100/198, Loss: 7.819366931915283\n",
      "Batch: 150/198, Loss: 6.439169406890869\n",
      "Average Epoch 15 Loss: 7.785916771551575\n",
      "\n",
      "Validation Loss: 8.153998415644576\n",
      "\n",
      "Started epoch 16\n",
      "\n",
      "Batch: 0/198, Loss: 8.497529029846191\n",
      "Batch: 50/198, Loss: 8.312010765075684\n",
      "Batch: 100/198, Loss: 6.281434059143066\n",
      "Batch: 150/198, Loss: 10.757707595825195\n",
      "Average Epoch 16 Loss: 8.127087925419662\n",
      "\n",
      "Validation Loss: 7.520515447709618\n",
      "\n",
      "Started epoch 17\n",
      "\n",
      "Batch: 0/198, Loss: 9.425467491149902\n",
      "Batch: 50/198, Loss: 6.885674476623535\n",
      "Batch: 100/198, Loss: 8.493196487426758\n",
      "Batch: 150/198, Loss: 7.337888717651367\n",
      "Average Epoch 17 Loss: 7.907300956321485\n",
      "\n",
      "Validation Loss: 7.20071489636491\n",
      "\n",
      "Started epoch 18\n",
      "\n",
      "Batch: 0/198, Loss: 7.032702922821045\n",
      "Batch: 50/198, Loss: 7.882952690124512\n",
      "Batch: 100/198, Loss: 6.5115885734558105\n",
      "Batch: 150/198, Loss: 7.093172073364258\n",
      "Average Epoch 18 Loss: 7.471057419825082\n",
      "\n",
      "Validation Loss: 7.189091577762511\n",
      "\n",
      "Started epoch 19\n",
      "\n",
      "Batch: 0/198, Loss: 7.061788558959961\n",
      "Batch: 50/198, Loss: 8.116655349731445\n",
      "Batch: 100/198, Loss: 6.42848539352417\n",
      "Batch: 150/198, Loss: 6.364734649658203\n",
      "Average Epoch 19 Loss: 7.51531058128434\n",
      "\n",
      "Validation Loss: 7.533878942815269\n",
      "\n",
      "Started epoch 20\n",
      "\n",
      "Batch: 0/198, Loss: 6.555942535400391\n",
      "Batch: 50/198, Loss: 6.039233207702637\n",
      "Batch: 100/198, Loss: 7.372038841247559\n",
      "Batch: 150/198, Loss: 6.626039028167725\n",
      "Average Epoch 20 Loss: 7.2646070730806604\n",
      "\n",
      "Validation Loss: 6.550536231296818\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 6.550536231296818\n",
      "Started epoch 21\n",
      "\n",
      "Batch: 0/198, Loss: 7.631376266479492\n",
      "Batch: 50/198, Loss: 7.317805290222168\n",
      "Batch: 100/198, Loss: 7.255488872528076\n",
      "Batch: 150/198, Loss: 6.5656962394714355\n",
      "Average Epoch 21 Loss: 7.2839055735655505\n",
      "\n",
      "Validation Loss: 6.127070531612489\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 6.127070531612489\n",
      "Started epoch 22\n",
      "\n",
      "Batch: 0/198, Loss: 6.635692119598389\n",
      "Batch: 50/198, Loss: 7.742856979370117\n",
      "Batch: 100/198, Loss: 6.429569721221924\n",
      "Batch: 150/198, Loss: 9.320697784423828\n",
      "Average Epoch 22 Loss: 7.23334446579519\n",
      "\n",
      "Validation Loss: 6.389828135327595\n",
      "\n",
      "Started epoch 23\n",
      "\n",
      "Batch: 0/198, Loss: 6.843329906463623\n",
      "Batch: 50/198, Loss: 6.511075019836426\n",
      "Batch: 100/198, Loss: 6.418526649475098\n",
      "Batch: 150/198, Loss: 6.762850284576416\n",
      "Average Epoch 23 Loss: 6.974012107560129\n",
      "\n",
      "Validation Loss: 7.5449738851407675\n",
      "\n",
      "Started epoch 24\n",
      "\n",
      "Batch: 0/198, Loss: 8.913599967956543\n",
      "Batch: 50/198, Loss: 7.894594192504883\n",
      "Batch: 100/198, Loss: 9.233896255493164\n",
      "Batch: 150/198, Loss: 7.846523284912109\n",
      "Average Epoch 24 Loss: 6.958461814456516\n",
      "\n",
      "Validation Loss: 7.681462543766673\n",
      "\n",
      "Started epoch 25\n",
      "\n",
      "Batch: 0/198, Loss: 6.212890625\n",
      "Batch: 50/198, Loss: 7.430689811706543\n",
      "Batch: 100/198, Loss: 8.122162818908691\n",
      "Batch: 150/198, Loss: 7.8579254150390625\n",
      "Average Epoch 25 Loss: 6.816232854669744\n",
      "\n",
      "Validation Loss: 6.227696314090636\n",
      "\n",
      "Started epoch 26\n",
      "\n",
      "Batch: 0/198, Loss: 8.095054626464844\n",
      "Batch: 50/198, Loss: 19.24559211730957\n",
      "Batch: 100/198, Loss: 16.17499542236328\n",
      "Batch: 150/198, Loss: 12.311601638793945\n",
      "Average Epoch 26 Loss: 13.526666727933017\n",
      "\n",
      "Validation Loss: 11.315550676206263\n",
      "\n",
      "Started epoch 27\n",
      "\n",
      "Batch: 0/198, Loss: 10.07275390625\n",
      "Batch: 50/198, Loss: 9.887614250183105\n",
      "Batch: 100/198, Loss: 9.794039726257324\n",
      "Batch: 150/198, Loss: 7.775162696838379\n",
      "Average Epoch 27 Loss: 8.507353530989754\n",
      "\n",
      "Validation Loss: 7.829519655646347\n",
      "\n",
      "Started epoch 28\n",
      "\n",
      "Batch: 0/198, Loss: 9.551628112792969\n",
      "Batch: 50/198, Loss: 8.259998321533203\n",
      "Batch: 100/198, Loss: 7.293882369995117\n",
      "Batch: 150/198, Loss: 6.23594856262207\n",
      "Average Epoch 28 Loss: 7.109738862875736\n",
      "\n",
      "Validation Loss: 6.520794316035945\n",
      "\n",
      "Started epoch 29\n",
      "\n",
      "Batch: 0/198, Loss: 8.089532852172852\n",
      "Batch: 50/198, Loss: 7.5569634437561035\n",
      "Batch: 100/198, Loss: 7.917978763580322\n",
      "Batch: 150/198, Loss: 6.317594528198242\n",
      "Average Epoch 29 Loss: 6.951424719107272\n",
      "\n",
      "Validation Loss: 6.431241099427386\n",
      "\n",
      "Started epoch 30\n",
      "\n",
      "Batch: 0/198, Loss: 5.860376358032227\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 50/198, Loss: 5.2138752937316895\n",
      "Batch: 100/198, Loss: 8.216883659362793\n",
      "Batch: 150/198, Loss: 8.214268684387207\n",
      "Average Epoch 30 Loss: 6.679572796580767\n",
      "\n",
      "Validation Loss: 6.094531960603668\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 6.094531960603668\n",
      "Started epoch 31\n",
      "\n",
      "Batch: 0/198, Loss: 5.06997013092041\n",
      "Batch: 50/198, Loss: 8.045022010803223\n",
      "Batch: 100/198, Loss: 7.319525718688965\n",
      "Batch: 150/198, Loss: 5.980809688568115\n",
      "Average Epoch 31 Loss: 6.9040131087255\n",
      "\n",
      "Validation Loss: 6.874177002325291\n",
      "\n",
      "Started epoch 32\n",
      "\n",
      "Batch: 0/198, Loss: 8.223392486572266\n",
      "Batch: 50/198, Loss: 7.722299575805664\n",
      "Batch: 100/198, Loss: 7.684998512268066\n",
      "Batch: 150/198, Loss: 7.0967912673950195\n",
      "Average Epoch 32 Loss: 6.680213174434623\n",
      "\n",
      "Validation Loss: 6.571352842377453\n",
      "\n",
      "Started epoch 33\n",
      "\n",
      "Batch: 0/198, Loss: 6.17097282409668\n",
      "Batch: 50/198, Loss: 5.920660972595215\n",
      "Batch: 100/198, Loss: 6.495434284210205\n",
      "Batch: 150/198, Loss: 5.339947700500488\n",
      "Average Epoch 33 Loss: 6.479903731683288\n",
      "\n",
      "Validation Loss: 6.174666259346939\n",
      "\n",
      "Started epoch 34\n",
      "\n",
      "Batch: 0/198, Loss: 6.209919452667236\n",
      "Batch: 50/198, Loss: 6.87211799621582\n",
      "Batch: 100/198, Loss: 6.378462791442871\n",
      "Batch: 150/198, Loss: 7.663748741149902\n",
      "Average Epoch 34 Loss: 6.789832264485986\n",
      "\n",
      "Validation Loss: 6.479616182606395\n",
      "\n",
      "Started epoch 35\n",
      "\n",
      "Batch: 0/198, Loss: 5.105355262756348\n",
      "Batch: 50/198, Loss: 7.137117385864258\n",
      "Batch: 100/198, Loss: 8.033103942871094\n",
      "Batch: 150/198, Loss: 8.716148376464844\n",
      "Average Epoch 35 Loss: 6.620142044443073\n",
      "\n",
      "Validation Loss: 6.339322090148926\n",
      "\n",
      "Started epoch 36\n",
      "\n",
      "Batch: 0/198, Loss: 7.989554405212402\n",
      "Batch: 50/198, Loss: 5.375357151031494\n",
      "Batch: 100/198, Loss: 7.775714874267578\n",
      "Batch: 150/198, Loss: 6.458693027496338\n",
      "Average Epoch 36 Loss: 6.560280566263681\n",
      "\n",
      "Validation Loss: 6.530570576830608\n",
      "\n",
      "Started epoch 37\n",
      "\n",
      "Batch: 0/198, Loss: 6.547762870788574\n",
      "Batch: 50/198, Loss: 5.555729389190674\n",
      "Batch: 100/198, Loss: 5.615082263946533\n",
      "Batch: 150/198, Loss: 6.565932273864746\n",
      "Average Epoch 37 Loss: 6.536316479095305\n",
      "\n",
      "Validation Loss: 5.869926359595322\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 5.869926359595322\n",
      "Started epoch 38\n",
      "\n",
      "Batch: 0/198, Loss: 5.370865345001221\n",
      "Batch: 50/198, Loss: 6.501934051513672\n",
      "Batch: 100/198, Loss: 5.896760940551758\n",
      "Batch: 150/198, Loss: 5.219433784484863\n",
      "Average Epoch 38 Loss: 6.304209258821276\n",
      "\n",
      "Validation Loss: 6.440958180078646\n",
      "\n",
      "Started epoch 39\n",
      "\n",
      "Batch: 0/198, Loss: 4.663110733032227\n",
      "Batch: 50/198, Loss: 6.379118919372559\n",
      "Batch: 100/198, Loss: 7.970834255218506\n",
      "Batch: 150/198, Loss: 8.251886367797852\n",
      "Average Epoch 39 Loss: 6.4459819131427345\n",
      "\n",
      "Validation Loss: 5.962836550503242\n",
      "\n",
      "Started epoch 40\n",
      "\n",
      "Batch: 0/198, Loss: 5.903029441833496\n",
      "Batch: 50/198, Loss: 7.866462707519531\n",
      "Batch: 100/198, Loss: 6.012616157531738\n",
      "Batch: 150/198, Loss: 6.430727958679199\n",
      "Average Epoch 40 Loss: 6.303767300615407\n",
      "\n",
      "Validation Loss: 6.013321789299569\n",
      "\n",
      "Started epoch 41\n",
      "\n",
      "Batch: 0/198, Loss: 4.951101779937744\n",
      "Batch: 50/198, Loss: 8.607915878295898\n",
      "Batch: 100/198, Loss: 7.855291366577148\n",
      "Batch: 150/198, Loss: 6.536538124084473\n",
      "Average Epoch 41 Loss: 6.286946874676329\n",
      "\n",
      "Validation Loss: 6.546990633010864\n",
      "\n",
      "Started epoch 42\n",
      "\n",
      "Batch: 0/198, Loss: 7.749530792236328\n",
      "Batch: 50/198, Loss: 6.295602798461914\n",
      "Batch: 100/198, Loss: 7.470716953277588\n",
      "Batch: 150/198, Loss: 5.398281574249268\n",
      "Average Epoch 42 Loss: 6.47800421233129\n",
      "\n",
      "Validation Loss: 6.354409892384599\n",
      "\n",
      "Started epoch 43\n",
      "\n",
      "Batch: 0/198, Loss: 5.745091915130615\n",
      "Batch: 50/198, Loss: 8.058743476867676\n",
      "Batch: 100/198, Loss: 4.580501556396484\n",
      "Batch: 150/198, Loss: 7.599052429199219\n",
      "Average Epoch 43 Loss: 6.3790413249622695\n",
      "\n",
      "Validation Loss: 7.257668303280342\n",
      "\n",
      "Started epoch 44\n",
      "\n",
      "Batch: 0/198, Loss: 5.625826835632324\n",
      "Batch: 50/198, Loss: 5.345822334289551\n",
      "Batch: 100/198, Loss: 7.964122772216797\n",
      "Batch: 150/198, Loss: 6.2265214920043945\n",
      "Average Epoch 44 Loss: 6.478589425183306\n",
      "\n",
      "Validation Loss: 6.661665771065689\n",
      "\n",
      "Started epoch 45\n",
      "\n",
      "Batch: 0/198, Loss: 7.884458065032959\n",
      "Batch: 50/198, Loss: 6.606662750244141\n",
      "Batch: 100/198, Loss: 4.899114608764648\n",
      "Batch: 150/198, Loss: 5.556819915771484\n",
      "Average Epoch 45 Loss: 6.307721390868679\n",
      "\n",
      "Validation Loss: 6.259864318661574\n",
      "\n",
      "Started epoch 46\n",
      "\n",
      "Batch: 0/198, Loss: 5.488215923309326\n",
      "Batch: 50/198, Loss: 5.1789937019348145\n",
      "Batch: 100/198, Loss: 6.615016937255859\n",
      "Batch: 150/198, Loss: 5.812447547912598\n",
      "Average Epoch 46 Loss: 6.316932955173531\n",
      "\n",
      "Validation Loss: 5.142482123723844\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 5.142482123723844\n",
      "Started epoch 47\n",
      "\n",
      "Batch: 0/198, Loss: 5.6833648681640625\n",
      "Batch: 50/198, Loss: 7.699073791503906\n",
      "Batch: 100/198, Loss: 5.5262770652771\n",
      "Batch: 150/198, Loss: 5.928837776184082\n",
      "Average Epoch 47 Loss: 6.114577770233154\n",
      "\n",
      "Validation Loss: 5.948944673305604\n",
      "\n",
      "Started epoch 48\n",
      "\n",
      "Batch: 0/198, Loss: 7.068419456481934\n",
      "Batch: 50/198, Loss: 5.600368976593018\n",
      "Batch: 100/198, Loss: 7.012618064880371\n",
      "Batch: 150/198, Loss: 5.3333306312561035\n",
      "Average Epoch 48 Loss: 6.17385413189127\n",
      "\n",
      "Validation Loss: 5.196889284180432\n",
      "\n",
      "Started epoch 49\n",
      "\n",
      "Batch: 0/198, Loss: 5.769538879394531\n",
      "Batch: 50/198, Loss: 4.801569938659668\n",
      "Batch: 100/198, Loss: 5.601894378662109\n",
      "Batch: 150/198, Loss: 7.928310394287109\n",
      "Average Epoch 49 Loss: 6.061123864819305\n",
      "\n",
      "Validation Loss: 5.456381099980052\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_epochs(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(6)\n",
    "a.repeat(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 240, 240])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[50][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from torchvision ResNet, converted to v2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_channels, num_filters,\n",
    "                 first_layer_kernel_size, first_layer_conv_stride,\n",
    "                 blocks_per_layer_list, block_strides_list, block_fn,\n",
    "                 first_layer_padding=0,\n",
    "                 first_pool_size=None, first_pool_stride=None, first_pool_padding=0,\n",
    "                 growth_factor=2, norm_class=\"batch\", num_groups=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.first_conv = nn.Conv2d(\n",
    "            in_channels=input_channels, out_channels=num_filters,\n",
    "            kernel_size=first_layer_kernel_size,\n",
    "            stride=first_layer_conv_stride,\n",
    "            padding=first_layer_padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        # Diff: padding=SAME vs. padding=0\n",
    "        self.first_pool = nn.MaxPool2d(\n",
    "            kernel_size=first_pool_size,\n",
    "            stride=first_pool_stride,\n",
    "            padding=first_pool_padding,\n",
    "        )\n",
    "        self.norm_class = norm_class\n",
    "        self.num_groups = num_groups\n",
    "\n",
    "        block = self._resolve_block(block_fn)\n",
    "        self.layer_list = nn.ModuleList()\n",
    "        current_num_filters = num_filters\n",
    "        self.inplanes = num_filters\n",
    "        for i, (num_blocks, stride) in enumerate(zip(\n",
    "                blocks_per_layer_list, block_strides_list)):\n",
    "            self.layer_list.append(self._make_layer(\n",
    "                block=block,\n",
    "                planes=current_num_filters,\n",
    "                blocks=num_blocks,\n",
    "                stride=stride,\n",
    "            ))\n",
    "            current_num_filters *= growth_factor\n",
    "\n",
    "        self.final_bn = layers.resolve_norm_layer(\n",
    "            # current_num_filters // growth_factor\n",
    "            current_num_filters // growth_factor * block.expansion,\n",
    "            norm_class,\n",
    "            num_groups\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize()\n",
    "\n",
    "        # Expose attributes for downstream dimension computation\n",
    "        self.num_filters = num_filters\n",
    "        self.growth_factor = growth_factor\n",
    "        self.block = block\n",
    "        self.num_filter_last_seq = current_num_filters // growth_factor * block.expansion\n",
    "\n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        intermediate = []\n",
    "        h = self.first_conv(x)\n",
    "        h = self.first_pool(h)\n",
    "\n",
    "        if return_intermediate:\n",
    "            intermediate.append(h)\n",
    "        for i, layer in enumerate(self.layer_list):\n",
    "            h = layer(h)\n",
    "            if return_intermediate:\n",
    "                intermediate.append(h)\n",
    "\n",
    "        h = self.final_bn(h)\n",
    "        h = self.relu(h)\n",
    "\n",
    "        if return_intermediate:\n",
    "            return h, intermediate\n",
    "        else:\n",
    "            return h\n",
    "\n",
    "    @classmethod\n",
    "    def _resolve_block(cls, block_fn):\n",
    "        if block_fn == \"normal\":\n",
    "            return layers.BasicBlockV2_dbt\n",
    "        elif block_fn == \"bottleneck\":\n",
    "            return layers.BottleneckV2_dbt\n",
    "        else:\n",
    "            raise KeyError(block_fn)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        # downsample = None\n",
    "        # if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                      kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(planes * block.expansion),\n",
    "        )\n",
    "\n",
    "        layers_ = [\n",
    "            block(self.inplanes, planes, stride, downsample, self.norm_class, self.num_groups)\n",
    "        ]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers_.append(block(self.inplanes, planes, norm_class=self.norm_class, num_groups=self.num_groups))\n",
    "\n",
    "        return nn.Sequential(*layers_)\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            self._layer_init(m)\n",
    "\n",
    "    @classmethod\n",
    "    def _layer_init(cls, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # From original\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #             nn.init.xavier_normal_(m.weight)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.GroupNorm):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @classmethod\n",
    "    def from_parameters(cls, parameters):\n",
    "        return cls(\n",
    "            input_channels=parameters[\"input_channels\"],\n",
    "            num_filters=parameters[\"num_filters\"],\n",
    "            first_layer_kernel_size=parameters[\"first_layer_kernel_size\"],\n",
    "            first_layer_conv_stride=parameters[\"first_layer_conv_stride\"],\n",
    "            first_layer_padding=parameters.get(\"first_layer_padding\", 0),\n",
    "            blocks_per_layer_list=parameters[\"blocks_per_layer_list\"],\n",
    "            block_strides_list=parameters[\"block_strides_list\"],\n",
    "            block_fn=parameters[\"block_fn\"],\n",
    "            first_pool_size=parameters[\"first_pool_size\"],\n",
    "            first_pool_stride=parameters[\"first_pool_stride\"],\n",
    "            first_pool_padding=parameters.get(\"first_pool_padding\", 0),\n",
    "            growth_factor=parameters.get(\"growth_factor\", 2),\n",
    "            norm_class=parameters.get(\"norm_class\", \"batch\"),\n",
    "            num_groups=parameters.get(\"num_groups\", 1)\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_22(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            attention=False,\n",
    "            dropout=0.0,\n",
    "            hidden_size=256,\n",
    "\n",
    "            # resnet hyperparameters\n",
    "            #         input_channels=1,\n",
    "            first_layer_kernel_size=7,\n",
    "            first_layer_conv_stride=2,\n",
    "            first_pool_size=3,\n",
    "            first_pool_stride=2,\n",
    "            first_layer_padding=0,\n",
    "            first_pool_padding=0,\n",
    "            growth_factor=2,\n",
    "\n",
    "            # resnet22 settings\n",
    "            num_filters=16,\n",
    "            blocks_per_layer_list=[2, 2, 2, 2, 2],\n",
    "            block_strides_list=[1, 2, 2, 2, 2],\n",
    "            block_fn=\"normal\",\n",
    "            norm_class=\"group\",\n",
    "            num_groups=8,\n",
    "\n",
    "            num_image_slices_per_net=1,\n",
    "    ):\n",
    "        super(ResNet_22, self).__init__()\n",
    "\n",
    "        self.num_image_slices_per_net = num_image_slices_per_net\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.resnet = ResNet(\n",
    "            input_channels=3,\n",
    "            first_layer_kernel_size=first_layer_kernel_size,\n",
    "            first_layer_conv_stride=first_layer_conv_stride,\n",
    "            first_pool_size=first_pool_size,\n",
    "            first_pool_stride=first_pool_stride,\n",
    "            num_filters=num_filters,\n",
    "            blocks_per_layer_list=blocks_per_layer_list,\n",
    "            block_strides_list=block_strides_list,\n",
    "            block_fn=block_fn,\n",
    "            first_layer_padding=first_layer_padding,\n",
    "            first_pool_padding=first_pool_padding,\n",
    "            growth_factor=growth_factor,\n",
    "            norm_class=norm_class,\n",
    "            num_groups=num_groups,\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # use avgpool rather than torch.mean\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        h = self.resnet(x)\n",
    "        # Shape of pooled_h is [4, 256, 1, 1]\n",
    "        pooled_h = self.avgpool(h)\n",
    "        return pooled_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNet_22' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e02a0ee98ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet_22\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResNet_22' is not defined"
     ]
    }
   ],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = ResNet_22()\n",
    "model.eval()\n",
    "\n",
    "boom = model(trainset[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 1, 1])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boom.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
