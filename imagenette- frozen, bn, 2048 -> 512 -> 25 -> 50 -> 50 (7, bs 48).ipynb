{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import collections as col\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pdb\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import torch.optim as optim\n",
    "from torchvision.models import resnet34, resnet50\n",
    "import torch.nn.functional as F\n",
    "import models\n",
    "import layers\n",
    "import utilities.reading_images as reading_images\n",
    "from utilities.loading import get_single_image\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Okay so I've figured out that it's the Batchnorm layer that's causing this. \n",
    "# I basically trained a model on Imagewoof and saved several checkpoints. \n",
    "# At epoch 30, the model gave me 0.06 loss. That should correlate with close to 100% train accuracy, \n",
    "# especially since the classes are balanced. \n",
    "# It did not. Train and val accuracies are all ~15%. \n",
    "# I then used this saved model and printed its loss in eval mode. It was 4.095. Obviously way off. \n",
    "# Several people online encounter this problem. Drastic difference in behavior in BatchNorm between .train()\n",
    "# and .eval() time. \n",
    "# Soumith suggests increasing the momentum parameter. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        \n",
    "        # Conv2d: (input_channels, output_channels, kernel_size, padding)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, (1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConstantPad2d((1, 1, 0, 0), 0),\n",
    "            nn.Conv2d(256, 256, (1, 3)),\n",
    "            nn.ConstantPad2d((0, 0, 0, 1), 0),\n",
    "            nn.Conv2d(256, 256, (2, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, (1, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, latents):\n",
    "        \n",
    "        # latents: [B, C, H, W]\n",
    "        cres = latents\n",
    "        \n",
    "        for _ in range(5):\n",
    "            c = self.model(cres)\n",
    "            cres = cres + c\n",
    "        cres = self.relu(cres)\n",
    "        return cres      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_raster_patchify(img, size = 80, overlap = 32):\n",
    "    '''\n",
    "    Left-to-right, top to bottom.\n",
    "    Assumes img is (3, 240, 240).\n",
    "    '''\n",
    "    patches = []\n",
    "     \n",
    "    h = -32\n",
    "    w = -32\n",
    "    for i in range(6):\n",
    "        h = h + 32\n",
    "        for j in range(6):\n",
    "            w = w + 32\n",
    "            channel = np.random.randint(3)\n",
    "            processed_img = np.repeat(np.expand_dims(img[channel, h:h+size, w:w+size], axis=0), 3, axis=0)\n",
    "            if np.random.randint(2):\n",
    "                processed_img = np.flip(processed_img, axis=2)\n",
    "            patches.append(torch.tensor(np.ascontiguousarray(processed_img)))\n",
    "        w = -32\n",
    "            \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_raster_patchify(img, size = 80, overlap = 32):\n",
    "    '''\n",
    "    Left-to-right, top to bottom.\n",
    "    Assumes img is (3, 240, 240).\n",
    "    '''\n",
    "    patches = []\n",
    "     \n",
    "    h = -32\n",
    "    w = -32\n",
    "    for i in range(6):\n",
    "        h = h + 32\n",
    "        for j in range(6):\n",
    "            w = w + 32\n",
    "            patches.append(img[:, h:h+size, w:w+size])\n",
    "        w = -32\n",
    "            \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def val_collate_fn(img_list):\n",
    "    patches = []\n",
    "    labels = []\n",
    "    for (img, label) in img_list:\n",
    "        img_patches = val_raster_patchify(img)\n",
    "        patches.append(torch.stack(img_patches))\n",
    "        labels.append(label)\n",
    "        \n",
    "    return patches, labels\n",
    "\n",
    "def train_collate_fn(img_list):\n",
    "    patches = []\n",
    "    labels = []\n",
    "    for (img, label) in img_list:\n",
    "        img_patches = train_raster_patchify(img)\n",
    "        patches.append(torch.stack(img_patches))\n",
    "        labels.append(label)\n",
    "        \n",
    "    return patches, labels\n",
    "\n",
    "# train_transform = transforms.Compose([\n",
    "#     transforms.Resize(256),\n",
    "#     transforms.RandomCrop(240),\n",
    "#     transforms.ColorJitter(brightness=(0.55, 1), contrast=(0.5, 1), saturation=(0.5, 1), hue=0.1),\n",
    "#     transforms.ToTensor(),\n",
    "# #     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "# ])\n",
    "\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(240),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "trainset = datasets.ImageFolder(\n",
    "    root = '/gpfs/data/geraslab/Vish/imagenette2-320/train/',\n",
    "    transform = data_transform\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=train_collate_fn)\n",
    "\n",
    "valset = datasets.ImageFolder(\n",
    "    root = '/gpfs/data/geraslab/Vish/imagenette2-320/val/',\n",
    "    transform = data_transform\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(valset, batch_size=32, shuffle=True, collate_fn=val_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_batchnorm(model):\n",
    "    model.bn1 = Identity()\n",
    "    model.layer1[0].bn1 = Identity()\n",
    "    model.layer1[0].bn2 = Identity()\n",
    "    model.layer1[0].bn3 = Identity()\n",
    "    model.layer1[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer1[1].bn1 = Identity()\n",
    "    model.layer1[1].bn2 = Identity()\n",
    "    model.layer1[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer1[2].bn1 = Identity()\n",
    "    model.layer1[2].bn2 = Identity()\n",
    "    model.layer1[2].bn3 = Identity()\n",
    "    \n",
    "    model.layer2[0].bn1 = Identity()\n",
    "    model.layer2[0].bn2 = Identity()\n",
    "    model.layer2[0].bn3 = Identity()\n",
    "    model.layer2[0].downsample[1] = Identity()\n",
    "    \n",
    "    model.layer2[1].bn1 = Identity()\n",
    "    model.layer2[1].bn2 = Identity()\n",
    "    model.layer2[1].bn3 = Identity()\n",
    "    \n",
    "    model.layer2[2].bn1 = Identity()\n",
    "    model.layer2[2].bn2 = Identity()\n",
    "    model.layer2[2].bn3 = Identity()\n",
    "    \n",
    "    model.layer2[3].bn1 = Identity()\n",
    "    model.layer2[3].bn2 = Identity()\n",
    "    model.layer2[3].bn3 = Identity()\n",
    "    \n",
    "#     model.layer3[0].bn1 = Identity()\n",
    "#     model.layer3[0].bn2 = Identity()\n",
    "#     model.layer3[0].bn3 = Identity()\n",
    "#     model.layer3[0].downsample[1] = Identity()\n",
    "    \n",
    "#     model.layer3[1].bn1 = Identity()\n",
    "#     model.layer3[1].bn2 = Identity()\n",
    "#     model.layer3[1].bn3 = Identity()\n",
    "    \n",
    "#     model.layer3[2].bn1 = Identity()\n",
    "#     model.layer3[2].bn2 = Identity()\n",
    "#     model.layer3[2].bn3 = Identity()\n",
    "    \n",
    "    \n",
    "#     model.layer3[3].bn1 = Identity()\n",
    "#     model.layer3[3].bn2 = Identity()\n",
    "#     model.layer3[3].bn3 = Identity()\n",
    "    \n",
    "    \n",
    "#     model.layer3[4].bn1 = Identity()\n",
    "#     model.layer3[4].bn2 = Identity()\n",
    "#     model.layer3[4].bn3 = Identity()\n",
    "    \n",
    "#     model.layer3[5].bn1 = Identity()\n",
    "#     model.layer3[5].bn2 = Identity()\n",
    "#     model.layer3[5].bn3 = Identity()\n",
    "\n",
    "    model.layer3 = Identity()\n",
    "    \n",
    "    model.layer4 = Identity()\n",
    "    \n",
    "#     model.layer4[0].bn1 = Identity()\n",
    "#     model.layer4[0].bn2 = Identity()\n",
    "#     model.layer4[0].bn3 = Identity()\n",
    "#     model.layer4[0].downsample[1] = Identity()\n",
    "    \n",
    "#     model.layer4[1].bn1 = Identity()\n",
    "#     model.layer4[1].bn2 = Identity()\n",
    "#     model.layer4[1].bn3 = Identity()\n",
    "    \n",
    "#     model.layer4[2].bn1 = Identity()\n",
    "#     model.layer4[2].bn2 = Identity()\n",
    "#     model.layer4[2].bn3 = Identity()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "#     model.layer4[0].bn1 = Identity()\n",
    "#     model.layer4[0].bn2 = Identity()\n",
    "#     model.layer4[0].downsample[1] = Identity()\n",
    "#     model.layer4[1].bn1 = Identity()\n",
    "#     model.layer4[1].bn2 = Identity()\n",
    "#     model.layer4[2].bn1 = Identity()\n",
    "#     model.layer4[2].bn2 = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC_Linear(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CPC_Linear, self).__init__()\n",
    "        self.encoder = resnet50()\n",
    "        self.encoder.fc = Identity()\n",
    "        remove_batchnorm(self.encoder)\n",
    "#         self.bn = nn.GroupNorm(32, 512)\n",
    "        self.bn = nn.BatchNorm2d(512)\n",
    "        self.conv_1 = nn.Conv2d(512, 25, (1, 1))\n",
    "        self.avg_pool = nn.AvgPool2d(6, 6)\n",
    "        \n",
    "        self.bn_50_1 = nn.BatchNorm1d(50)\n",
    "        self.bn_50_2 = nn.BatchNorm1d(50)\n",
    "        \n",
    "        self.lin_1 = nn.Linear(25*6*6, 50)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.lin_2 = nn.Linear(50, 50)\n",
    "        self.lin_3 = nn.Linear(50, 10)\n",
    "#         self.dropout = nn.Dropout(p=0.2)\n",
    "\n",
    "    def forward(self, x, device):\n",
    "        Z = []\n",
    "        for img_patches in x:\n",
    "            img_patches = img_patches.to(device)\n",
    "            z = self.encoder(img_patches).squeeze()\n",
    "            z = z.unsqueeze(0).permute(0, 2, 1).reshape(1, 512, 6, 6)\n",
    "            Z.append(z)\n",
    "\n",
    "        Z = torch.stack(Z).squeeze(1)\n",
    "        \n",
    "        x = self.conv_1(self.bn(Z))\n",
    "        x = x.view(-1, 25*6*6)\n",
    "        \n",
    "        x = self.relu(self.bn_50_1(self.lin_1(x)))\n",
    "        x = self.relu(self.bn_50_2(self.lin_2(x)))\n",
    "                \n",
    "        output = self.lin_3(x)\n",
    "        \n",
    "        \n",
    "\n",
    "        # output = self.avg_pool(self.conv_1(self.bn(Z))).squeeze(2).squeeze(2)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(dl, model, loss_func, optimizer, device, phase = 'train'):\n",
    "    if phase == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "#         for m in model.modules():\n",
    "#             if isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.track_running_stats = False\n",
    "    losses = []\n",
    "    correct = 0\n",
    "    for i, (x, labels) in enumerate(dl):\n",
    "        if phase == 'train':\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        labels = torch.from_numpy(np.stack(labels)).to(device)\n",
    "        preds_logit = model(x, device)\n",
    "        loss = loss_func(preds_logit, labels)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if phase == 'train': \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(\"Batch: {}/{}, Loss: {}\".format(i, len(dl), loss.item())) \n",
    "        else:\n",
    "            preds_label = torch.argmax(preds_logit, dim=1)\n",
    "            correct += sum(preds_label == labels)\n",
    "            \n",
    "    \n",
    "    if phase == 'val':\n",
    "        return correct, np.mean(losses)\n",
    "    elif phase == 'train':\n",
    "        return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.cuda.set_device(7)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# model = CPC_Linear()\n",
    "# pretrained_dict = torch.load('pretrained_imagewoof_bn_0.5_20.pt')\n",
    "# model_dict = model.state_dict()\n",
    "\n",
    "# # 1. filter out unnecessary keys\n",
    "# pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "# # 2. overwrite entries in the existing state dict\n",
    "# model_dict.update(pretrained_dict) \n",
    "# # 3. load the new state dict\n",
    "# model.load_state_dict(model_dict)\n",
    "\n",
    "# model = model.to(device)\n",
    "# loss_func = nn.CrossEntropyLoss()\n",
    "# optimizer = optim.Adam(model.parameters(), lr = 1e-3, eps=1e-8)\n",
    "\n",
    "# correct, average_loss = one_epoch(train_dl, model, loss_func, optimizer, device, phase = 'val')\n",
    "\n",
    "# print(correct)\n",
    "# print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs(epoch_num):\n",
    "    torch.cuda.set_device(4)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = CPC_Linear()\n",
    "    pretrained_dict = torch.load('paper_self_supervised_rc_best_val.pt')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # 1. filter out unnecessary keys\n",
    "    pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "    # 2. overwrite entries in the existing state dict\n",
    "    model_dict.update(pretrained_dict) \n",
    "    # 3. load the new state dict\n",
    "    model.load_state_dict(model_dict)\n",
    "\n",
    "    model = model.to(device)\n",
    "    loss_func = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 1e-3, eps=1e-8)\n",
    "    \n",
    "    # ----------------------------------------------\n",
    "    # FREEZE ENCODER\n",
    "    for param in model.encoder.parameters():\n",
    "        param.requires_grad = False\n",
    "    \n",
    "    for i in range(epoch_num):\n",
    "        epoch_loss = one_epoch(train_dl, model, loss_func, optimizer, device, phase = 'train')\n",
    "        print(\"Average Epoch {} Loss: {}\".format(i, epoch_loss))\n",
    "        correct, _ = one_epoch(train_dl, model, loss_func, optimizer, device, phase = 'val')\n",
    "        print(\"Train Accuracy: {}\".format(1. * correct / len(trainset)))\n",
    "        correct, _ = one_epoch(val_dl, model, loss_func, optimizer, device, phase = 'val')\n",
    "        print(\"Validation Accuracy: {}\".format(1. * correct / len(valset)))\n",
    "        \n",
    "        if i in [1, 10, 20, 30]:\n",
    "            torch.save(model.state_dict(), \"pretrained_imagewoof_batch_norm_frozen_{}.pt\".format(i))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using CPC-pretrained weights, fine-tuning all. Validation Accuracy (best):  38%, within 23 epochs. \n",
    "# Using CPC-pretrained weights, frozen encoder. Validation Accuracy: \n",
    "# They use resnet-50 as the backbone. Maybe switch to that? \n",
    "# But I'm doing resnet-22 for each patch of the image. \n",
    "\n",
    "# I changed to resnet34. Frozen Encoder, validation accuracy: 16% through 9 epochs. \n",
    "# Random Color dropping (RC), Frozen Encoder, validation accuracy: 16% through 9 epochs. \n",
    "# NO pre-training, random initialized encoder + FROZEN. validation accuracy: 13% through 25 epochs. \n",
    "# NO pre-training, random initialized encoder + FINE_TUNE ALL. validation accuracy: 30% through 33 epochs.\n",
    "# Pre-trained rc_15_epochs, Frozen Encoder. Validation accuracy: \n",
    "# Pre-trained rc_15_epochs, Fine-tune all. Validation accuracy.\n",
    "\n",
    "# self-supervised. Frozen Encoder. Train/Val Accuracy:              36%/31%. \n",
    "# self-supervised 1024 (64), color jitter. Frozen Encoder. Train/Val:     43%/39% (Through 40 epochs). \n",
    "# self-supervised 2048 (64), color jitter. Frozen Encoder. Train/Val:     44%/41% (Through 40 epochs). \n",
    "# self-supervised 2048 (32), color jitter. Frozen Encoder. Train/Val:     46%/41% (Through 40 epochs). \n",
    "\n",
    "# self-supervised 2048 but train 1024? (24), Frozen Encoder. Train/Val:   50%/45% (Through 25 epochs)\n",
    "# self-supervised 2048 but train 1024? (16), Frozen Encoder. Train/Val:   52%/48% (Through 40 epochs)\n",
    "\n",
    "# self-supervised 1024 but train 1024 (16), Frozen Encoder. Train/Val:    44%/40% (Thru 40 epochs)\n",
    "\n",
    "# self-supervised 1024 but train 512 (7), Frozen Encoder. Train/Val:      54%/50% (Thru 40 epochs)\n",
    "# self-supervised 2048 but train 1024 (7), Frozen Encoder. Train/Val:     53%/48% (Thru 40 epochs)\n",
    "# self-supervised 2048 but train 512 (7), Frozen Encoder. Train/Val:      54%/50% (Thru 40 epochs)\n",
    "\n",
    "# self-supervised 2048 but train 1024 (12, bs 48), Frozen enc. Train/Val: 56%/47% (Thru 40 epochs)\n",
    "# self-supervised 2048 but train 512 (12, bs 48), Frozen enc. Train/Val:  53%/47% (Thru 40 epochs)\n",
    "\n",
    "# self-supervised 2048 but train 1024 (8, bs 48), Frozen Enc. Train/Val:  55%/50% (Thru 40 epochs)\n",
    "# self-supervised 2048 but train 512 (8, bs 48), Frozen Enc. Train/Val:   54%/50% (Thru 40 epochs)\n",
    "\n",
    "# self-supervised 2048 but train 512 (7, bs 48), Frozen Enc. Train/Val:   53%/50% (Thru 31 epochs)\n",
    "\n",
    "# self-supervised 2048 --> 512 --> 50 (7, bs 48), Frozen Enc. Train/Val:  68%/55% (Thru 10 epochs)\n",
    "# self-supervised 2048 --> 512 --> 50 (7, bs 48) (dropout 0.3),Train/Val: 72%/56% (Thru 40 epochs)\n",
    "# self-supervised 2048 -> 512 -> 50 -> 50 (7, bs 48) (dropout 0.1),Train/Val: 70%/56% (Thru 18 epochs)\n",
    "# self-supervised 2048 ->512-> -> 25 -> 50 -> 50 (7, bs 48) (dropout 0.1),Train/Val: 82%/56% (Thru 40 epochs)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 0/296, Loss: 2.3252148628234863\n",
      "Batch: 50/296, Loss: 2.200488805770874\n",
      "Batch: 100/296, Loss: 1.8522738218307495\n",
      "Batch: 150/296, Loss: 1.7442010641098022\n",
      "Batch: 200/296, Loss: 1.855542778968811\n",
      "Batch: 250/296, Loss: 1.538062572479248\n",
      "Average Epoch 0 Loss: 1.874844025518443\n",
      "Train Accuracy: 0.4758686423301697\n",
      "Validation Accuracy: 0.4366878867149353\n",
      "Batch: 0/296, Loss: 1.7733978033065796\n",
      "Batch: 50/296, Loss: 1.4563891887664795\n",
      "Batch: 100/296, Loss: 1.4562599658966064\n",
      "Batch: 150/296, Loss: 1.563275933265686\n",
      "Batch: 200/296, Loss: 1.4059970378875732\n",
      "Batch: 250/296, Loss: 1.3049472570419312\n",
      "Average Epoch 1 Loss: 1.59215742188531\n",
      "Train Accuracy: 0.5221248269081116\n",
      "Validation Accuracy: 0.4807642996311188\n",
      "Batch: 0/296, Loss: 1.6640311479568481\n",
      "Batch: 50/296, Loss: 1.2082107067108154\n",
      "Batch: 100/296, Loss: 1.5915204286575317\n",
      "Batch: 150/296, Loss: 1.7755053043365479\n",
      "Batch: 200/296, Loss: 1.3111125230789185\n",
      "Batch: 250/296, Loss: 1.436311960220337\n",
      "Average Epoch 2 Loss: 1.4693560253929447\n",
      "Train Accuracy: 0.555813729763031\n",
      "Validation Accuracy: 0.5115923285484314\n",
      "Batch: 0/296, Loss: 1.3699779510498047\n",
      "Batch: 50/296, Loss: 1.035219669342041\n",
      "Batch: 100/296, Loss: 1.1048775911331177\n",
      "Batch: 150/296, Loss: 1.4124188423156738\n",
      "Batch: 200/296, Loss: 1.2807413339614868\n",
      "Batch: 250/296, Loss: 1.367626667022705\n",
      "Average Epoch 3 Loss: 1.3748539366029404\n",
      "Train Accuracy: 0.5976343750953674\n",
      "Validation Accuracy: 0.5426751375198364\n",
      "Batch: 0/296, Loss: 1.2024848461151123\n",
      "Batch: 50/296, Loss: 1.4871864318847656\n",
      "Batch: 100/296, Loss: 1.3297054767608643\n",
      "Batch: 150/296, Loss: 1.1133863925933838\n",
      "Batch: 200/296, Loss: 1.3143173456192017\n",
      "Batch: 250/296, Loss: 1.8085070848464966\n",
      "Average Epoch 4 Loss: 1.2969299204446174\n",
      "Train Accuracy: 0.6123138666152954\n",
      "Validation Accuracy: 0.541146457195282\n",
      "Batch: 0/296, Loss: 1.5723422765731812\n",
      "Batch: 50/296, Loss: 0.9665476083755493\n",
      "Batch: 100/296, Loss: 1.483374834060669\n",
      "Batch: 150/296, Loss: 1.216050148010254\n",
      "Batch: 200/296, Loss: 1.414280652999878\n",
      "Batch: 250/296, Loss: 1.2912803888320923\n",
      "Average Epoch 5 Loss: 1.2562873468205735\n",
      "Train Accuracy: 0.6182278990745544\n",
      "Validation Accuracy: 0.5393630266189575\n",
      "Batch: 0/296, Loss: 0.8534630537033081\n",
      "Batch: 50/296, Loss: 1.1378507614135742\n",
      "Batch: 100/296, Loss: 1.2031663656234741\n",
      "Batch: 150/296, Loss: 1.1895561218261719\n",
      "Batch: 200/296, Loss: 1.39875328540802\n",
      "Batch: 250/296, Loss: 1.140677809715271\n",
      "Average Epoch 6 Loss: 1.2027336866468996\n",
      "Train Accuracy: 0.655190646648407\n",
      "Validation Accuracy: 0.565859854221344\n",
      "Batch: 0/296, Loss: 0.7879659533500671\n",
      "Batch: 50/296, Loss: 1.120296597480774\n",
      "Batch: 100/296, Loss: 1.2820814847946167\n",
      "Batch: 150/296, Loss: 1.1061866283416748\n",
      "Batch: 200/296, Loss: 0.9733772873878479\n",
      "Batch: 250/296, Loss: 1.0930835008621216\n",
      "Average Epoch 7 Loss: 1.1580958424790486\n",
      "Train Accuracy: 0.6787411570549011\n",
      "Validation Accuracy: 0.5681528449058533\n",
      "Batch: 0/296, Loss: 0.8641782999038696\n",
      "Batch: 50/296, Loss: 1.0978055000305176\n",
      "Batch: 100/296, Loss: 0.9970816969871521\n",
      "Batch: 150/296, Loss: 1.477651834487915\n",
      "Batch: 200/296, Loss: 0.9393333792686462\n",
      "Batch: 250/296, Loss: 1.2000948190689087\n",
      "Average Epoch 8 Loss: 1.1142587238872372\n"
     ]
    }
   ],
   "source": [
    "run_epochs(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from torchvision ResNet, converted to v2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_channels, num_filters,\n",
    "                 first_layer_kernel_size, first_layer_conv_stride,\n",
    "                 blocks_per_layer_list, block_strides_list, block_fn,\n",
    "                 first_layer_padding=0,\n",
    "                 first_pool_size=None, first_pool_stride=None, first_pool_padding=0,\n",
    "                 growth_factor=2, norm_class=\"batch\", num_groups=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.first_conv = nn.Conv2d(\n",
    "            in_channels=input_channels, out_channels=num_filters,\n",
    "            kernel_size=first_layer_kernel_size,\n",
    "            stride=first_layer_conv_stride,\n",
    "            padding=first_layer_padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        # Diff: padding=SAME vs. padding=0\n",
    "        self.first_pool = nn.MaxPool2d(\n",
    "            kernel_size=first_pool_size,\n",
    "            stride=first_pool_stride,\n",
    "            padding=first_pool_padding,\n",
    "        )\n",
    "        self.norm_class = norm_class\n",
    "        self.num_groups = num_groups\n",
    "\n",
    "        block = self._resolve_block(block_fn)\n",
    "        self.layer_list = nn.ModuleList()\n",
    "        current_num_filters = num_filters\n",
    "        self.inplanes = num_filters\n",
    "        for i, (num_blocks, stride) in enumerate(zip(\n",
    "                blocks_per_layer_list, block_strides_list)):\n",
    "            self.layer_list.append(self._make_layer(\n",
    "                block=block,\n",
    "                planes=current_num_filters,\n",
    "                blocks=num_blocks,\n",
    "                stride=stride,\n",
    "            ))\n",
    "            current_num_filters *= growth_factor\n",
    "\n",
    "        self.final_bn = layers.resolve_norm_layer(\n",
    "            # current_num_filters // growth_factor\n",
    "            current_num_filters // growth_factor * block.expansion,\n",
    "            norm_class,\n",
    "            num_groups\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize()\n",
    "\n",
    "        # Expose attributes for downstream dimension computation\n",
    "        self.num_filters = num_filters\n",
    "        self.growth_factor = growth_factor\n",
    "        self.block = block\n",
    "        self.num_filter_last_seq = current_num_filters // growth_factor * block.expansion\n",
    "\n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        intermediate = []\n",
    "        h = self.first_conv(x)\n",
    "        h = self.first_pool(h)\n",
    "\n",
    "        if return_intermediate:\n",
    "            intermediate.append(h)\n",
    "        for i, layer in enumerate(self.layer_list):\n",
    "            h = layer(h)\n",
    "            if return_intermediate:\n",
    "                intermediate.append(h)\n",
    "\n",
    "        h = self.final_bn(h)\n",
    "        h = self.relu(h)\n",
    "\n",
    "        if return_intermediate:\n",
    "            return h, intermediate\n",
    "        else:\n",
    "            return h\n",
    "\n",
    "    @classmethod\n",
    "    def _resolve_block(cls, block_fn):\n",
    "        if block_fn == \"normal\":\n",
    "            return layers.BasicBlockV2_dbt\n",
    "        elif block_fn == \"bottleneck\":\n",
    "            return layers.BottleneckV2_dbt\n",
    "        else:\n",
    "            raise KeyError(block_fn)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        # downsample = None\n",
    "        # if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                      kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(planes * block.expansion),\n",
    "        )\n",
    "\n",
    "        layers_ = [\n",
    "            block(self.inplanes, planes, stride, downsample, self.norm_class, self.num_groups)\n",
    "        ]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers_.append(block(self.inplanes, planes, norm_class=self.norm_class, num_groups=self.num_groups))\n",
    "\n",
    "        return nn.Sequential(*layers_)\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            self._layer_init(m)\n",
    "\n",
    "    @classmethod\n",
    "    def _layer_init(cls, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # From original\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #             nn.init.xavier_normal_(m.weight)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.GroupNorm):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @classmethod\n",
    "    def from_parameters(cls, parameters):\n",
    "        return cls(\n",
    "            input_channels=parameters[\"input_channels\"],\n",
    "            num_filters=parameters[\"num_filters\"],\n",
    "            first_layer_kernel_size=parameters[\"first_layer_kernel_size\"],\n",
    "            first_layer_conv_stride=parameters[\"first_layer_conv_stride\"],\n",
    "            first_layer_padding=parameters.get(\"first_layer_padding\", 0),\n",
    "            blocks_per_layer_list=parameters[\"blocks_per_layer_list\"],\n",
    "            block_strides_list=parameters[\"block_strides_list\"],\n",
    "            block_fn=parameters[\"block_fn\"],\n",
    "            first_pool_size=parameters[\"first_pool_size\"],\n",
    "            first_pool_stride=parameters[\"first_pool_stride\"],\n",
    "            first_pool_padding=parameters.get(\"first_pool_padding\", 0),\n",
    "            growth_factor=parameters.get(\"growth_factor\", 2),\n",
    "            norm_class=parameters.get(\"norm_class\", \"batch\"),\n",
    "            num_groups=parameters.get(\"num_groups\", 1)\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_22(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            attention=False,\n",
    "            dropout=0.0,\n",
    "            hidden_size=256,\n",
    "\n",
    "            # resnet hyperparameters\n",
    "            #         input_channels=1,\n",
    "            first_layer_kernel_size=7,\n",
    "            first_layer_conv_stride=2,\n",
    "            first_pool_size=3,\n",
    "            first_pool_stride=2,\n",
    "            first_layer_padding=0,\n",
    "            first_pool_padding=0,\n",
    "            growth_factor=2,\n",
    "\n",
    "            # resnet22 settings\n",
    "            num_filters=16,\n",
    "            blocks_per_layer_list=[2, 2, 2, 2, 2],\n",
    "            block_strides_list=[1, 2, 2, 2, 2],\n",
    "            block_fn=\"normal\",\n",
    "            norm_class=\"group\",\n",
    "            num_groups=8,\n",
    "\n",
    "            num_image_slices_per_net=1,\n",
    "    ):\n",
    "        super(ResNet_22, self).__init__()\n",
    "\n",
    "        self.num_image_slices_per_net = num_image_slices_per_net\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.resnet = ResNet(\n",
    "            input_channels=3,\n",
    "            first_layer_kernel_size=first_layer_kernel_size,\n",
    "            first_layer_conv_stride=first_layer_conv_stride,\n",
    "            first_pool_size=first_pool_size,\n",
    "            first_pool_stride=first_pool_stride,\n",
    "            num_filters=num_filters,\n",
    "            blocks_per_layer_list=blocks_per_layer_list,\n",
    "            block_strides_list=block_strides_list,\n",
    "            block_fn=block_fn,\n",
    "            first_layer_padding=first_layer_padding,\n",
    "            first_pool_padding=first_pool_padding,\n",
    "            growth_factor=growth_factor,\n",
    "            norm_class=norm_class,\n",
    "            num_groups=num_groups,\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # use avgpool rather than torch.mean\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        h = self.resnet(x)\n",
    "        # Shape of pooled_h is [4, 256, 1, 1]\n",
    "        pooled_h = self.avgpool(h)\n",
    "        return pooled_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
