{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import collections as col\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pdb\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "from pytorch_datasets import DiagnosticInpainted\n",
    "import models\n",
    "import layers\n",
    "import utilities.reading_images as reading_images\n",
    "from utilities.loading import get_single_image\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        \n",
    "        # Conv2d: (input_channels, output_channels, kernel_size, padding)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(latent_dim, latent_dim, (1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConstantPad2d((1, 1, 0, 0), 0),\n",
    "            nn.Conv2d(latent_dim, latent_dim, (1, 3)),\n",
    "            nn.ConstantPad2d((0, 0, 0, 1), 0),\n",
    "            nn.Conv2d(latent_dim, latent_dim, (2, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(latent_dim, latent_dim, (1, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, latents):\n",
    "        \n",
    "        # latents: [B, C, H, W]\n",
    "        cres = latents\n",
    "        \n",
    "        for _ in range(5):\n",
    "            c = self.model(cres)\n",
    "            cres = cres + c\n",
    "        cres = self.relu(cres)\n",
    "        return cres      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boom = torch.tensor(np.arange(20).reshape(2, 10))\n",
    "boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9]],\n",
       "\n",
       "        [[10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boom.reshape(2, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, 6, 6, 6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(6, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC_loss(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(CPC_loss, self).__init__()\n",
    "        self.pixel_cnn = PixelCNN(512)\n",
    "#         self.conv = nn.Conv2d(512, 64, kernel_size = (1, 1))\n",
    "        self.conv_1 = nn.Conv2d(512, 512, kernel_size = (1, 1))\n",
    "        self.conv_2 = nn.Conv2d(512, 512, kernel_size = (1, 1))\n",
    "        self.conv_3 = nn.Conv2d(512, 512, kernel_size = (1, 1))\n",
    "        self.loss_func = nn.CrossEntropyLoss()\n",
    "    \n",
    "    def forward(self, latents, device, target_dim = 64, steps_to_ignore = 2, steps_to_predict = 3, emb_scale = 0.1):\n",
    "        # latents: [B, D, H, W]\n",
    "        # aka:     [B, 512, 6, 6]\n",
    "        loss = 0.0\n",
    "        latents = latents.to(device)\n",
    "        context = self.pixel_cnn(latents) # These are the c's (apply pixelCNN to Z's)\n",
    "#         targets = self.conv(latents)\n",
    "        targets = latents\n",
    "        \n",
    "        batch_dim, target_dim, col_dim, row_dim = targets.shape\n",
    "        targets = targets.reshape(-1, target_dim)\n",
    "        \n",
    "        # Trying to do the arbitrary context vector\n",
    "        index = np.random.choice(a = [0, 1, 2])\n",
    "        context = context[:, :, index, :].unsqueeze(3) # [2, 512, 6, 1]\n",
    "        \n",
    "        \n",
    "        preds_1 = self.conv_1(context).reshape(-1, 512) * emb_scale\n",
    "        preds_2 = self.conv_2(context).reshape(-1, 512) * emb_scale\n",
    "        preds_3 = self.conv_3(context).reshape(-1, 512) * emb_scale\n",
    "        \n",
    "        logits_1 = torch.matmul(preds_1, targets.permute(1, 0)) # 12 by 512, 512 by 72 --> 12 by 72\n",
    "        logits_2 = torch.matmul(preds_2, targets.permute(1, 0))\n",
    "        logits_3 = torch.matmul(preds_3, targets.permute(1, 0))\n",
    "        \n",
    "        total_elements = batch_dim * row_dim\n",
    "        b = np.arange(total_elements) / (row_dim)\n",
    "        b = b.astype(int)\n",
    "        col = np.arange(total_elements) % (row_dim)\n",
    "\n",
    "        labels_1 = b * col_dim * row_dim + 3 * row_dim + col\n",
    "        labels_2 = labels_1 + 6\n",
    "        labels_3 = labels_2 + 6\n",
    "        \n",
    "        loss += self.loss_func(logits_1, torch.LongTensor(labels_1).to(device))\n",
    "        loss += self.loss_func(logits_2, torch.LongTensor(labels_2).to(device))\n",
    "        loss += self.loss_func(logits_3, torch.LongTensor(labels_3).to(device))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "#         for i in range(steps_to_ignore, steps_to_predict):\n",
    "# #             pdb.set_trace()\n",
    "#             col_dim_i = col_dim - i - 1  # 6 - 2 - 1 = 3\n",
    "#             total_elements = batch_dim * col_dim_i * row_dim\n",
    "#             preds_i = self.conv_preds(context)\n",
    "#             preds_i = preds_i[:, :, :(i+1), :] * emb_scale   # [B, 64, 6, 6] ---> [B, 64, 3, 6]\n",
    "#             preds_i = preds_i.reshape(-1, target_dim)\n",
    "            \n",
    "#             logits = torch.matmul(preds_i, targets.permute(1, 0)) # 18 by 64, 64 by 36 ---> 18 by 36\n",
    "            \n",
    "#             b = np.arange(total_elements) / (col_dim_i * row_dim)\n",
    "#             b = b.astype(int)\n",
    "#             col = np.arange(total_elements) % (col_dim_i * row_dim)\n",
    "#             labels = b * col_dim * row_dim + (i + 1) * row_dim + col\n",
    "#             labels = torch.LongTensor(labels).to(device)\n",
    "#             logits = logits.to(device)\n",
    "            \n",
    "#             rand = np.random.choice(a=[False, True], size = (logits.shape[0],))\n",
    "#             logits = logits[rand, :]\n",
    "#             labels = labels[rand]\n",
    "#             loss += self.loss_func(logits, labels)\n",
    "        return loss\n",
    "            \n",
    "            \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "       34, 35])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_elements = 1 * 3 * 6\n",
    "b = np.arange(18) / (3 * 6)\n",
    "b = b.astype(int)\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "       17])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col = np.arange(18) % (3 * 6)\n",
    "col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels = b * 6 * 6 + 3*6 + col\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implementation in the paper is unclear.\n",
    "# I'm going to go with WF. \n",
    "\n",
    "# NCE Loss\n",
    "# Questions: Is the dimension of Z (B*patches) or (B). \n",
    "#            I think it's (B, 6, 6, 4096)\n",
    "\n",
    "class CPCLossNCE(nn.Module):\n",
    "    \n",
    "    def nce_loss(self, z_hat, pos_scores, negative_samples, mask_mat, device, epoch_num, batch_num):\n",
    "        \n",
    "        z_hat = z_hat.to(device)\n",
    "        pos_scores = pos_scores.to(device)\n",
    "        negative_samples = negative_samples.to(device)\n",
    "        mask_mat = mask_mat.to(device)\n",
    "                \n",
    "        # (b, 1)\n",
    "        pos_scores = pos_scores.float()\n",
    "        batch_size, emb_dim = z_hat.size()\n",
    "        nb_feat_vectors = negative_samples.size(1) // batch_size # 36 of them, if 6 by 6 wireframes. \n",
    "        \n",
    "        # (b, b) -> (b, b, nb_feat_vectors)\n",
    "        # all zeros with ones in diagonal tensor... (ie: b1 b1 are all 1s, b1 b2 are all zeros)\n",
    "        mask_pos = mask_mat.unsqueeze(dim=2).expand(-1, -1, nb_feat_vectors).float()\n",
    "        \n",
    "        # negative mask\n",
    "        mask_neg = 1. - mask_pos\n",
    "        \n",
    "        # ----------------------\n",
    "        # ALL SCORES computation\n",
    "        # (visualize in your mind a batch size of 2, 36-length segments) \n",
    "        # (b, dim) x (dim, nb_feats*b) -> (b, b, nb_feats)\n",
    "        raw_scores = torch.mm(z_hat, negative_samples)\n",
    "        raw_scores = raw_scores.reshape(batch_size, batch_size, nb_feat_vectors).float()\n",
    "        \n",
    "        # EXTRACT NEGATIVE SCORES\n",
    "        # (batch_size, batch_size, nb_feat_vectors)\n",
    "        # HE'S TAKING THE NEGATIVE SAMPLES FROM THE OTHER MINIBATCHES\n",
    "        # A GIVEN Z_HAT IS ONLY MULTIPLIED BY Z'S FROM OTHER MINIBATCHES\n",
    "        neg_scores = (mask_neg * raw_scores)\n",
    "        # ----------------------\n",
    "        \n",
    "        # (b, b, nb_feat_vectors) -> (batch_size, batch_size * nb_feat_vectors) \n",
    "        neg_scores = neg_scores.reshape(batch_size, -1)\n",
    "        mask_neg = mask_neg.reshape(batch_size, -1)\n",
    "        \n",
    "        # STABLE SOFTMAX\n",
    "        # (n_batch_gpu, 1)\n",
    "        neg_maxes = torch.max(neg_scores, dim=1, keepdim=True)[0]\n",
    "        \n",
    "        # DENOMINATOR\n",
    "        # sum over only negative samples (none from the diagonal)\n",
    "        neg_sumexp = (mask_neg * torch.exp(neg_scores - neg_maxes)).sum(dim=1, keepdim=True)\n",
    "        all_logsumexp = torch.log(torch.exp(pos_scores - neg_maxes) + neg_sumexp)\n",
    "        \n",
    "        # NUMERATOR\n",
    "        # compute numerators for the NCE log-softmaxes\n",
    "        pos_shiftexp = pos_scores - neg_maxes\n",
    "        \n",
    "        # FULL NCE\n",
    "#         if epoch_num > 2 and batch_num == 100:\n",
    "#             pdb.set_trace()\n",
    "        nce_scores = pos_shiftexp - all_logsumexp\n",
    "        nce_scores = -nce_scores.mean()\n",
    "        \n",
    "#         if np.isnan(nce_scores.cpu().detach().numpy()):\n",
    "#             pdb.set_trace()\n",
    "#             print('boom - nceloss')\n",
    "            \n",
    "        \n",
    "        return nce_scores\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, Z, C, W_list, device, epoch_num, batch_num):\n",
    "        '''\n",
    "        param Z: latent vecs (B, D, H, W)\n",
    "        param C: context vecs (B, D, H, W)\n",
    "        param W_list: list of k-1 W projections\n",
    "        '''\n",
    "        \n",
    "        # (b, dim, w, h)\n",
    "        batch_size, emb_dim, h, w = Z.size()\n",
    "        \n",
    "        # (10 x 10 identity matrix)\n",
    "        diag_mat = torch.eye(batch_size)\n",
    "        diag_mat = diag_mat.float()\n",
    "        \n",
    "        losses = []\n",
    "        # calculate loss for each k\n",
    "        \n",
    "        # Below operations preserve raster order (for B, D, H, W) = (1, 5, 2, 2) check.\n",
    "        # Z_neg holds all z vecs. \n",
    "        Z_neg = Z.permute(1, 0, 2, 3).reshape(emb_dim, -1)\n",
    "        \n",
    "        \n",
    "        for i in range(0, h-1):\n",
    "            for j in range(0, w):\n",
    "                cij = C[:, :, i, j]   # B by D\n",
    "                \n",
    "                for k in range(i+1, h): # predict on all vectors in the same column, but below current wireframe. \n",
    "                    Wk = W_list[str(k)]\n",
    "                    \n",
    "                    z_hat_ikj = Wk(cij)\n",
    "                    zikj = Z[:, :, k, j]\n",
    "                    \n",
    "                    # BATCH DOT PRODUCT\n",
    "                    # (b, d) x (b, d) -> (b, 1)\n",
    "                    pos_scores = torch.bmm(z_hat_ikj.unsqueeze(1), zikj.unsqueeze(2))\n",
    "                    pos_scores = pos_scores.squeeze(-1).squeeze(-1)\n",
    "                    \n",
    "                    loss = self.nce_loss(z_hat_ikj, pos_scores, Z_neg, diag_mat, device, epoch_num, batch_num)\n",
    "                    if np.isinf(loss.item()):\n",
    "                        pdb.set_trace()\n",
    "                        print(\"inf -- inside inner for loop\")\n",
    "                    if np.isnan(loss.item()):\n",
    "                        pdb.set_trace()\n",
    "                        print(\"inside inner for loop\")\n",
    "                    losses.append(loss)\n",
    "                    \n",
    "                    \n",
    "        losses = torch.stack(losses)\n",
    "        loss = losses.mean()\n",
    "#         if np.isnan(loss.item()):\n",
    "#             pdb.set_trace()\n",
    "#             print('boom')\n",
    "        return loss           \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "boom = torch.rand(2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "boom = boom.unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 2, 2])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.repeat(boom, 3, axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_raster_patchify(img, size = 80, overlap = 32):\n",
    "    '''\n",
    "    Left-to-right, top to bottom.\n",
    "    Assumes img is (3, 240, 240).\n",
    "    '''\n",
    "    patches = []\n",
    "     \n",
    "    h = -32\n",
    "    w = -32\n",
    "    for i in range(6):\n",
    "        h = h + 32\n",
    "        for j in range(6):\n",
    "            w = w + 32\n",
    "            channel = np.random.randint(3)\n",
    "            processed_img = np.repeat(np.expand_dims(img[channel, h:h+size, w:w+size], axis=0), 3, axis=0)\n",
    "            if np.random.randint(2):\n",
    "                processed_img = np.flip(processed_img, axis=2)\n",
    "            patches.append(torch.tensor(np.ascontiguousarray(processed_img)))\n",
    "        w = -32\n",
    "            \n",
    "    return patches\n",
    "\n",
    "\n",
    "def val_raster_patchify(img, size = 80, overlap = 32):\n",
    "    '''\n",
    "    Left-to-right, top to bottom.\n",
    "    Assumes img is (3, 240, 240).\n",
    "    '''\n",
    "    patches = []\n",
    "     \n",
    "    h = -32\n",
    "    w = -32\n",
    "    for i in range(6):\n",
    "        h = h + 32\n",
    "        for j in range(6):\n",
    "            w = w + 32\n",
    "            patches.append(img[:, h:h+size, w:w+size])\n",
    "        w = -32\n",
    "            \n",
    "    return patches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(patches[35].permute(1, 2, 0))\n",
    "\n",
    "# plt.imshow(trainset[0][0].permute(1, 2, 0))\n",
    "# plt.scatter(80,80,color='r')\n",
    "# plt.scatter(80+32,80,color='r')\n",
    "# plt.scatter(80+64,80,color='r')\n",
    "# plt.scatter(80+96,80,color='r')\n",
    "# plt.scatter(80+128,80,color='r')\n",
    "# plt.scatter(80+160,80,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_collate_fn(img_list):\n",
    "    patches = []\n",
    "    for (img, label) in img_list:\n",
    "        img_patches = train_raster_patchify(img)\n",
    "        patches.append(torch.stack(img_patches))\n",
    "        \n",
    "    return patches\n",
    "\n",
    "def val_collate_fn(img_list):\n",
    "    patches = []\n",
    "    for (img, label) in img_list:\n",
    "        img_patches = val_raster_patchify(img)\n",
    "        patches.append(torch.stack(img_patches))\n",
    "        \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(240),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "trainset = datasets.ImageFolder(\n",
    "    root = '/gpfs/data/geraslab/Vish/imagenette2-320/train/',\n",
    "    transform = data_transform\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(trainset, batch_size=32, shuffle=True, collate_fn=train_collate_fn)\n",
    "\n",
    "valset = datasets.ImageFolder(\n",
    "    root = '/gpfs/data/geraslab/Vish/imagenette2-320/val/',\n",
    "    transform = data_transform\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(valset, batch_size=32, shuffle=True, collate_fn=val_collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_batchnorm(model):\n",
    "    model.bn1 = Identity()\n",
    "    model.layer1[0].bn1 = Identity()\n",
    "    model.layer1[0].bn2 = Identity()\n",
    "    model.layer1[1].bn1 = Identity()\n",
    "    model.layer1[1].bn2 = Identity()\n",
    "    model.layer1[2].bn1 = Identity()\n",
    "    model.layer1[2].bn2 = Identity()\n",
    "    \n",
    "    model.layer2[0].bn1 = Identity()\n",
    "    model.layer2[0].bn2 = Identity()\n",
    "    model.layer2[0].downsample[1] = Identity()\n",
    "    model.layer2[1].bn1 = Identity()\n",
    "    model.layer2[1].bn2 = Identity()\n",
    "    model.layer2[2].bn1 = Identity()\n",
    "    model.layer2[2].bn2 = Identity()\n",
    "    model.layer2[3].bn1 = Identity()\n",
    "    model.layer2[3].bn2 = Identity()\n",
    "    \n",
    "    model.layer3[0].bn1 = Identity()\n",
    "    model.layer3[0].bn2 = Identity()\n",
    "    model.layer3[0].downsample[1] = Identity()\n",
    "    model.layer3[1].bn1 = Identity()\n",
    "    model.layer3[1].bn2 = Identity()\n",
    "    model.layer3[2].bn1 = Identity()\n",
    "    model.layer3[2].bn2 = Identity()\n",
    "    model.layer3[3].bn1 = Identity()\n",
    "    model.layer3[3].bn2 = Identity()\n",
    "    model.layer3[4].bn1 = Identity()\n",
    "    model.layer3[4].bn2 = Identity()\n",
    "    model.layer3[5].bn1 = Identity()\n",
    "    model.layer3[5].bn2 = Identity()\n",
    "    \n",
    "    model.layer4[0].bn1 = Identity()\n",
    "    model.layer4[0].bn2 = Identity()\n",
    "    model.layer4[0].downsample[1] = Identity()\n",
    "    model.layer4[1].bn1 = Identity()\n",
    "    model.layer4[1].bn2 = Identity()\n",
    "    model.layer4[2].bn1 = Identity()\n",
    "    model.layer4[2].bn2 = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CPC(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CPC, self).__init__()\n",
    "        self.encoder = torchvision.models.resnet34()\n",
    "        self.encoder.fc = Identity()\n",
    "        remove_batchnorm(self.encoder)\n",
    "        self.pixel_cnn = PixelCNN(512)\n",
    "        self.nce_loss = CPC_loss()\n",
    "#         self.nce_loss = CPCLossNCE()\n",
    "        \n",
    "#         # W transforms (k > 0)\n",
    "#         self.W_list = {}\n",
    "#         for k in range(1, 6):\n",
    "#             w = torch.nn.Linear(512, 512)\n",
    "#             self.W_list[str(k)] = w\n",
    "\n",
    "#         self.W_list = nn.ModuleDict(self.W_list)\n",
    "        \n",
    "\n",
    "    def forward(self, x, device, epoch_num, batch_num):\n",
    "        Z = []\n",
    "        C = []\n",
    "        for img_patches in x:\n",
    "            img_patches = img_patches.to(device)\n",
    "            z = self.encoder(img_patches).squeeze()\n",
    "            z = z.unsqueeze(0).permute(0, 2, 1).reshape(1, 512, 6, 6)\n",
    "            Z.append(z)\n",
    "            c = self.pixel_cnn(z)\n",
    "            C.append(c)\n",
    "\n",
    "        Z = torch.stack(Z).squeeze(1)\n",
    "        C = torch.stack(C).squeeze(1)\n",
    "\n",
    "        loss = self.nce_loss(Z, device)\n",
    "        \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_epoch(dl, model, optimizer, device, epoch_num, phase = 'train'):\n",
    "    if phase == 'train':\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "#         for m in model.modules():\n",
    "#             if isinstance(m, nn.BatchNorm2d):\n",
    "#                 m.track_running_stats = False\n",
    "    losses = []\n",
    "    for i, x in enumerate(dl):\n",
    "        if phase == 'train':\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        loss = model(x, device, epoch_num, i)\n",
    "        losses.append(loss.item())\n",
    "\n",
    "        if phase == 'train': \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if i % 50 == 0:\n",
    "                print(\"Batch: {}/{}, Loss: {}\".format(i, len(dl), loss.item())) \n",
    "            \n",
    "    \n",
    "    return np.mean(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epochs(epoch_num):\n",
    "    torch.cuda.set_device(6)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    model = CPC().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr = 2e-4, weight_decay=1e-5, eps=1e-8)\n",
    "    \n",
    "#     pretrained_dict = torch.load('self_supervised_rc_15.pt')\n",
    "#     model_dict = model.state_dict()\n",
    "\n",
    "#     # 1. filter out unnecessary keys\n",
    "#     pretrained_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict}\n",
    "#     # 2. overwrite entries in the existing state dict\n",
    "#     model_dict.update(pretrained_dict) \n",
    "#     # 3. load the new state dict\n",
    "#     model.load_state_dict(model_dict)\n",
    "    \n",
    "    best_val_loss = 1000000\n",
    "    for i in range(epoch_num):\n",
    "        print(\"Started epoch {}\\n\".format(i))\n",
    "        avg_train_loss = one_epoch(train_dl, model, optimizer, device, i, phase = 'train')\n",
    "        print(\"Average Epoch {} Loss: {}\\n\".format(i, avg_train_loss))\n",
    "        avg_val_loss = one_epoch(val_dl, model, optimizer, device, i, phase = 'val')\n",
    "        print(\"Validation Loss: {}\\n\".format(avg_val_loss))\n",
    "        \n",
    "        if avg_val_loss < best_val_loss:\n",
    "            best_val_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), \"paper_self_supervised_rc_best_val.pt\")\n",
    "            print(\"\\nSaved model with best validation loss: {}\".format(best_val_loss))\n",
    "        \n",
    "        if i in [1, 10, 20, 25]:\n",
    "            torch.save(model.state_dict(), \"paper_self_supervised_rc_{}.pt\".format(i))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Started epoch 0\n",
      "\n",
      "Batch: 0/296, Loss: 17646.0859375\n",
      "Batch: 50/296, Loss: 10.218378067016602\n",
      "Batch: 100/296, Loss: 8.624032020568848\n",
      "Batch: 150/296, Loss: 3.7366786003112793\n",
      "Batch: 200/296, Loss: 4.514866828918457\n",
      "Batch: 250/296, Loss: 0.9217681884765625\n",
      "Average Epoch 0 Loss: 66.88577281633341\n",
      "\n",
      "Validation Loss: 0.9694324612496344\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 0.9694324612496344\n",
      "Started epoch 1\n",
      "\n",
      "Batch: 0/296, Loss: 1.1164277791976929\n",
      "Batch: 50/296, Loss: 0.5281332731246948\n",
      "Batch: 100/296, Loss: 2.533245801925659\n",
      "Batch: 150/296, Loss: 0.3566368520259857\n",
      "Batch: 200/296, Loss: 0.11818763613700867\n",
      "Batch: 250/296, Loss: 0.01738981530070305\n",
      "Average Epoch 1 Loss: 0.756940320571507\n",
      "\n",
      "Validation Loss: 0.3884073811757371\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 0.3884073811757371\n",
      "Started epoch 2\n",
      "\n",
      "Batch: 0/296, Loss: 0.4992157220840454\n",
      "Batch: 50/296, Loss: 0.019082389771938324\n",
      "Batch: 100/296, Loss: 3.908580780029297\n",
      "Batch: 150/296, Loss: 0.15137018263339996\n",
      "Batch: 200/296, Loss: 0.15695790946483612\n",
      "Batch: 250/296, Loss: 1.5379798412322998\n",
      "Average Epoch 2 Loss: 0.5051468088267201\n",
      "\n",
      "Validation Loss: 0.5839193339527744\n",
      "\n",
      "Started epoch 3\n",
      "\n",
      "Batch: 0/296, Loss: 0.12329763174057007\n",
      "Batch: 50/296, Loss: 0.12200851738452911\n",
      "Batch: 100/296, Loss: 0.13764232397079468\n",
      "Batch: 150/296, Loss: 0.4905913472175598\n",
      "Batch: 200/296, Loss: 0.2191409319639206\n",
      "Batch: 250/296, Loss: 0.03760606423020363\n",
      "Average Epoch 3 Loss: 0.23432386658454118\n",
      "\n",
      "Validation Loss: 0.14838542678673577\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 0.14838542678673577\n",
      "Started epoch 4\n",
      "\n",
      "Batch: 0/296, Loss: 0.07498453557491302\n",
      "Batch: 50/296, Loss: 0.3333340883255005\n",
      "Batch: 100/296, Loss: 0.007743660360574722\n",
      "Batch: 150/296, Loss: 0.02527868188917637\n",
      "Batch: 200/296, Loss: 0.07857319712638855\n",
      "Batch: 250/296, Loss: 0.03553557023406029\n",
      "Average Epoch 4 Loss: 0.1861086419533397\n",
      "\n",
      "Validation Loss: 0.059873721415268796\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 0.059873721415268796\n",
      "Started epoch 5\n",
      "\n",
      "Batch: 0/296, Loss: 0.017951104789972305\n",
      "Batch: 50/296, Loss: 0.06690598279237747\n",
      "Batch: 100/296, Loss: 0.34146982431411743\n",
      "Batch: 150/296, Loss: 0.0017227728385478258\n",
      "Batch: 200/296, Loss: 0.048624008893966675\n",
      "Batch: 250/296, Loss: 0.8482358455657959\n",
      "Average Epoch 5 Loss: 0.16304196069779356\n",
      "\n",
      "Validation Loss: 0.10480736005558904\n",
      "\n",
      "Started epoch 6\n",
      "\n",
      "Batch: 0/296, Loss: 0.09140616655349731\n",
      "Batch: 50/296, Loss: 0.04383620619773865\n",
      "Batch: 100/296, Loss: 0.1057036966085434\n",
      "Batch: 150/296, Loss: 0.48807090520858765\n",
      "Batch: 200/296, Loss: 0.15581893920898438\n",
      "Batch: 250/296, Loss: 0.004799187183380127\n",
      "Average Epoch 6 Loss: 0.11266401329044744\n",
      "\n",
      "Validation Loss: 0.055225398582090905\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 0.055225398582090905\n",
      "Started epoch 7\n",
      "\n",
      "Batch: 0/296, Loss: 0.0025756743270903826\n",
      "Batch: 50/296, Loss: 0.027759574353694916\n",
      "Batch: 100/296, Loss: 0.03839275613427162\n",
      "Batch: 150/296, Loss: 0.01865721307694912\n",
      "Batch: 200/296, Loss: 0.01988828182220459\n",
      "Batch: 250/296, Loss: 0.0029323261696845293\n",
      "Average Epoch 7 Loss: 0.08819501252214282\n",
      "\n",
      "Validation Loss: 0.013347885700708932\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 0.013347885700708932\n",
      "Started epoch 8\n",
      "\n",
      "Batch: 0/296, Loss: 0.05306749790906906\n",
      "Batch: 50/296, Loss: 0.015945784747600555\n",
      "Batch: 100/296, Loss: 0.6336451172828674\n",
      "Batch: 150/296, Loss: 0.4072040915489197\n",
      "Batch: 200/296, Loss: 0.00046085036592558026\n",
      "Batch: 250/296, Loss: 0.008754346519708633\n",
      "Average Epoch 8 Loss: 0.07663850148237597\n",
      "\n",
      "Validation Loss: 0.04442666195952584\n",
      "\n",
      "Started epoch 9\n",
      "\n",
      "Batch: 0/296, Loss: 0.14042243361473083\n",
      "Batch: 50/296, Loss: 0.2025282382965088\n",
      "Batch: 100/296, Loss: 0.001760150888003409\n",
      "Batch: 150/296, Loss: 0.00011501304106786847\n",
      "Batch: 200/296, Loss: 0.034605421125888824\n",
      "Batch: 250/296, Loss: 0.23905804753303528\n",
      "Average Epoch 9 Loss: 0.12413735256010924\n",
      "\n",
      "Validation Loss: 0.1466418960040844\n",
      "\n",
      "Started epoch 10\n",
      "\n",
      "Batch: 0/296, Loss: 0.6584792137145996\n",
      "Batch: 50/296, Loss: 0.061915211379528046\n",
      "Batch: 100/296, Loss: 0.06418313086032867\n",
      "Batch: 150/296, Loss: 0.014577260240912437\n",
      "Batch: 200/296, Loss: 0.1875298023223877\n",
      "Batch: 250/296, Loss: 0.00031168083660304546\n",
      "Average Epoch 10 Loss: 0.06419800512506067\n",
      "\n",
      "Validation Loss: 0.022376670306460223\n",
      "\n",
      "Started epoch 11\n",
      "\n",
      "Batch: 0/296, Loss: 2.2924194126971997e-05\n",
      "Batch: 50/296, Loss: 0.0001254994422197342\n",
      "Batch: 100/296, Loss: 0.00024722208036109805\n",
      "Batch: 150/296, Loss: 0.021784892305731773\n",
      "Batch: 200/296, Loss: 0.32050567865371704\n",
      "Batch: 250/296, Loss: 0.03393193334341049\n",
      "Average Epoch 11 Loss: 0.1167266586010467\n",
      "\n",
      "Validation Loss: 0.025602638023447366\n",
      "\n",
      "Started epoch 12\n",
      "\n",
      "Batch: 0/296, Loss: 0.01663227751851082\n",
      "Batch: 50/296, Loss: 0.05458366125822067\n",
      "Batch: 100/296, Loss: 6.984733045101166e-05\n",
      "Batch: 150/296, Loss: 0.001528057036921382\n",
      "Batch: 200/296, Loss: 0.0018259475473314524\n",
      "Batch: 250/296, Loss: 0.03209401294589043\n",
      "Average Epoch 12 Loss: 0.0316535759261984\n",
      "\n",
      "Validation Loss: 0.0037827321404252444\n",
      "\n",
      "\n",
      "Saved model with best validation loss: 0.0037827321404252444\n",
      "Started epoch 13\n",
      "\n",
      "Batch: 0/296, Loss: 0.08239749073982239\n",
      "Batch: 50/296, Loss: 0.001423357636667788\n",
      "Batch: 100/296, Loss: 0.0069651007652282715\n",
      "Batch: 150/296, Loss: 0.011185853742063046\n",
      "Batch: 200/296, Loss: 0.0007764872279949486\n",
      "Batch: 250/296, Loss: 0.14357580244541168\n",
      "Average Epoch 13 Loss: 0.019573951351643536\n",
      "\n",
      "Validation Loss: 0.011021650735445549\n",
      "\n",
      "Started epoch 14\n",
      "\n",
      "Batch: 0/296, Loss: 0.0017976221861317754\n",
      "Batch: 50/296, Loss: 0.000869223615154624\n",
      "Batch: 100/296, Loss: 0.08191292732954025\n",
      "Batch: 150/296, Loss: 0.0025430426467210054\n",
      "Batch: 200/296, Loss: 0.1648552119731903\n",
      "Batch: 250/296, Loss: 0.01086483709514141\n",
      "Average Epoch 14 Loss: 0.16141066319192707\n",
      "\n",
      "Validation Loss: 0.2231641127084329\n",
      "\n",
      "Started epoch 15\n",
      "\n",
      "Batch: 0/296, Loss: 0.30616021156311035\n",
      "Batch: 50/296, Loss: 0.5978398323059082\n",
      "Batch: 100/296, Loss: 0.26209723949432373\n",
      "Batch: 150/296, Loss: 0.27816227078437805\n",
      "Batch: 200/296, Loss: 0.05703050643205643\n",
      "Batch: 250/296, Loss: 6.88446089043282e-05\n",
      "Average Epoch 15 Loss: 0.10092274204765003\n",
      "\n",
      "Validation Loss: 0.03515595928903259\n",
      "\n",
      "Started epoch 16\n",
      "\n",
      "Batch: 0/296, Loss: 0.17847386002540588\n",
      "Batch: 50/296, Loss: 0.00010050274431705475\n",
      "Batch: 100/296, Loss: 0.0010552096646279097\n",
      "Batch: 150/296, Loss: 0.0007645683363080025\n",
      "Batch: 200/296, Loss: 0.005085529759526253\n",
      "Batch: 250/296, Loss: 7.786104833940044e-05\n",
      "Average Epoch 16 Loss: 0.027841026949527215\n",
      "\n",
      "Validation Loss: 0.012632440515558447\n",
      "\n",
      "Started epoch 17\n",
      "\n",
      "Batch: 0/296, Loss: 1.7948448657989502e-05\n",
      "Batch: 50/296, Loss: 0.00027434396906755865\n",
      "Batch: 100/296, Loss: 0.8097937107086182\n",
      "Batch: 150/296, Loss: 0.16011017560958862\n",
      "Batch: 200/296, Loss: 0.005898535251617432\n",
      "Batch: 250/296, Loss: 0.037122391164302826\n",
      "Average Epoch 17 Loss: 0.3332710281342632\n",
      "\n",
      "Validation Loss: 0.03694158884178709\n",
      "\n",
      "Started epoch 18\n",
      "\n",
      "Batch: 0/296, Loss: 0.00476096011698246\n",
      "Batch: 50/296, Loss: 0.004507025703787804\n",
      "Batch: 100/296, Loss: 0.0021417003590613604\n",
      "Batch: 150/296, Loss: 0.060430221259593964\n",
      "Batch: 200/296, Loss: 0.0022274090442806482\n",
      "Batch: 250/296, Loss: 0.07593294978141785\n",
      "Average Epoch 18 Loss: 0.05724238638921437\n",
      "\n",
      "Validation Loss: 0.011551562216732201\n",
      "\n",
      "Started epoch 19\n",
      "\n",
      "Batch: 0/296, Loss: 0.03365150839090347\n",
      "Batch: 50/296, Loss: 0.0005606493796221912\n",
      "Batch: 100/296, Loss: 0.005942715331912041\n",
      "Batch: 150/296, Loss: 0.003072201507166028\n",
      "Batch: 200/296, Loss: 0.02809998393058777\n",
      "Batch: 250/296, Loss: 0.09350363910198212\n",
      "Average Epoch 19 Loss: 0.08070832835570481\n",
      "\n",
      "Validation Loss: 0.02854448022418826\n",
      "\n",
      "Started epoch 20\n",
      "\n",
      "Batch: 0/296, Loss: 0.00513963820412755\n",
      "Batch: 50/296, Loss: 0.0003519455494824797\n",
      "Batch: 100/296, Loss: 0.014006447978317738\n",
      "Batch: 150/296, Loss: 0.008444506675004959\n",
      "Batch: 200/296, Loss: 0.0002458728849887848\n",
      "Batch: 250/296, Loss: 0.0009883120656013489\n",
      "Average Epoch 20 Loss: 0.019293541153743526\n",
      "\n",
      "Validation Loss: 0.0198639055508103\n",
      "\n",
      "Started epoch 21\n",
      "\n",
      "Batch: 0/296, Loss: 0.00044797969167120755\n",
      "Batch: 50/296, Loss: 0.13822703063488007\n",
      "Batch: 100/296, Loss: 7.761021265650925e-07\n",
      "Batch: 150/296, Loss: 0.09456124901771545\n",
      "Batch: 200/296, Loss: 0.37874770164489746\n",
      "Batch: 250/296, Loss: 0.0036434284411370754\n",
      "Average Epoch 21 Loss: 0.11577186684202027\n",
      "\n",
      "Validation Loss: 0.03558509825646962\n",
      "\n",
      "Started epoch 22\n",
      "\n",
      "Batch: 0/296, Loss: 0.00013737876724917442\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch: 50/296, Loss: 2.471792140568141e-05\n",
      "Batch: 100/296, Loss: 8.232084655901417e-05\n",
      "Batch: 150/296, Loss: 0.023032132536172867\n",
      "Batch: 200/296, Loss: 0.08631636947393417\n",
      "Batch: 250/296, Loss: 0.159549281001091\n",
      "Average Epoch 22 Loss: 0.1330854403047686\n",
      "\n",
      "Validation Loss: 0.11809169286265742\n",
      "\n",
      "Started epoch 23\n",
      "\n",
      "Batch: 0/296, Loss: 0.29722481966018677\n",
      "Batch: 50/296, Loss: 1.4901161193847656e-08\n",
      "Batch: 100/296, Loss: 0.00805171113461256\n",
      "Batch: 150/296, Loss: 0.0018004218582063913\n",
      "Batch: 200/296, Loss: 0.0004298919520806521\n",
      "Batch: 250/296, Loss: 0.0004460994969122112\n",
      "Average Epoch 23 Loss: 0.012502065027002896\n",
      "\n",
      "Validation Loss: 0.00909582914934446\n",
      "\n",
      "Started epoch 24\n",
      "\n",
      "Batch: 0/296, Loss: 0.00017516003572382033\n",
      "Batch: 50/296, Loss: 5.002009493182413e-05\n",
      "Batch: 100/296, Loss: 7.654617547814269e-06\n",
      "Batch: 150/296, Loss: 6.163492798805237e-06\n",
      "Batch: 200/296, Loss: 1.9868215517249155e-08\n",
      "Batch: 250/296, Loss: 3.223461317247711e-05\n",
      "Average Epoch 24 Loss: 0.007457826524026214\n",
      "\n",
      "Validation Loss: 0.009100704124988609\n",
      "\n",
      "Started epoch 25\n",
      "\n",
      "Batch: 0/296, Loss: 0.0009400808485224843\n",
      "Batch: 50/296, Loss: 1.30385160446167e-08\n",
      "Batch: 100/296, Loss: 0.00036886270390823483\n",
      "Batch: 150/296, Loss: 0.0008042799890972674\n",
      "Batch: 200/296, Loss: 5.276916272123344e-05\n",
      "Batch: 250/296, Loss: 1.383821199851809e-05\n",
      "Average Epoch 25 Loss: 0.00866719063333017\n",
      "\n",
      "Validation Loss: 0.0042036142774100295\n",
      "\n",
      "Started epoch 26\n",
      "\n",
      "Batch: 0/296, Loss: 4.218891263008118e-06\n",
      "Batch: 50/296, Loss: 0.13242213428020477\n",
      "Batch: 100/296, Loss: 10.507217407226562\n",
      "Batch: 150/296, Loss: 9.451377868652344\n",
      "Batch: 200/296, Loss: 5.901889801025391\n",
      "Batch: 250/296, Loss: 0.588741660118103\n",
      "Average Epoch 26 Loss: 6.470211850290552\n",
      "\n",
      "Validation Loss: 0.9801685876114582\n",
      "\n",
      "Started epoch 27\n",
      "\n",
      "Batch: 0/296, Loss: 1.9497613906860352\n",
      "Batch: 50/296, Loss: 1.8979793787002563\n",
      "Batch: 100/296, Loss: 0.5412649512290955\n",
      "Batch: 150/296, Loss: 0.042447496205568314\n",
      "Batch: 200/296, Loss: 0.21826417744159698\n",
      "Batch: 250/296, Loss: 0.06347991526126862\n",
      "Average Epoch 27 Loss: 0.45820364620327825\n",
      "\n",
      "Validation Loss: 0.19083139130739663\n",
      "\n",
      "Started epoch 28\n",
      "\n",
      "Batch: 0/296, Loss: 0.042293429374694824\n",
      "Batch: 50/296, Loss: 0.0023907185532152653\n",
      "Batch: 100/296, Loss: 0.0003164097142871469\n",
      "Batch: 150/296, Loss: 0.0011395122855901718\n",
      "Batch: 200/296, Loss: 0.13582755625247955\n",
      "Batch: 250/296, Loss: 0.004265257157385349\n",
      "Average Epoch 28 Loss: 0.07491468584410348\n",
      "\n",
      "Validation Loss: 0.057666542034679164\n",
      "\n",
      "Started epoch 29\n",
      "\n",
      "Batch: 0/296, Loss: 0.002503328025341034\n",
      "Batch: 50/296, Loss: 0.022073809057474136\n",
      "Batch: 100/296, Loss: 0.03503791615366936\n",
      "Batch: 150/296, Loss: 0.000997112481854856\n",
      "Batch: 200/296, Loss: 0.5877335071563721\n",
      "Batch: 250/296, Loss: 0.00019029279064852744\n",
      "Average Epoch 29 Loss: 0.04031318364740311\n",
      "\n",
      "Validation Loss: 0.027172817332665716\n",
      "\n"
     ]
    }
   ],
   "source": [
    "run_epochs(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([6, 6, 6, 6, 6])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor(6)\n",
    "a.repeat(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 240, 240])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[50][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from torchvision ResNet, converted to v2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_channels, num_filters,\n",
    "                 first_layer_kernel_size, first_layer_conv_stride,\n",
    "                 blocks_per_layer_list, block_strides_list, block_fn,\n",
    "                 first_layer_padding=0,\n",
    "                 first_pool_size=None, first_pool_stride=None, first_pool_padding=0,\n",
    "                 growth_factor=2, norm_class=\"batch\", num_groups=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.first_conv = nn.Conv2d(\n",
    "            in_channels=input_channels, out_channels=num_filters,\n",
    "            kernel_size=first_layer_kernel_size,\n",
    "            stride=first_layer_conv_stride,\n",
    "            padding=first_layer_padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        # Diff: padding=SAME vs. padding=0\n",
    "        self.first_pool = nn.MaxPool2d(\n",
    "            kernel_size=first_pool_size,\n",
    "            stride=first_pool_stride,\n",
    "            padding=first_pool_padding,\n",
    "        )\n",
    "        self.norm_class = norm_class\n",
    "        self.num_groups = num_groups\n",
    "\n",
    "        block = self._resolve_block(block_fn)\n",
    "        self.layer_list = nn.ModuleList()\n",
    "        current_num_filters = num_filters\n",
    "        self.inplanes = num_filters\n",
    "        for i, (num_blocks, stride) in enumerate(zip(\n",
    "                blocks_per_layer_list, block_strides_list)):\n",
    "            self.layer_list.append(self._make_layer(\n",
    "                block=block,\n",
    "                planes=current_num_filters,\n",
    "                blocks=num_blocks,\n",
    "                stride=stride,\n",
    "            ))\n",
    "            current_num_filters *= growth_factor\n",
    "\n",
    "        self.final_bn = layers.resolve_norm_layer(\n",
    "            # current_num_filters // growth_factor\n",
    "            current_num_filters // growth_factor * block.expansion,\n",
    "            norm_class,\n",
    "            num_groups\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize()\n",
    "\n",
    "        # Expose attributes for downstream dimension computation\n",
    "        self.num_filters = num_filters\n",
    "        self.growth_factor = growth_factor\n",
    "        self.block = block\n",
    "        self.num_filter_last_seq = current_num_filters // growth_factor * block.expansion\n",
    "\n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        intermediate = []\n",
    "        h = self.first_conv(x)\n",
    "        h = self.first_pool(h)\n",
    "\n",
    "        if return_intermediate:\n",
    "            intermediate.append(h)\n",
    "        for i, layer in enumerate(self.layer_list):\n",
    "            h = layer(h)\n",
    "            if return_intermediate:\n",
    "                intermediate.append(h)\n",
    "\n",
    "        h = self.final_bn(h)\n",
    "        h = self.relu(h)\n",
    "\n",
    "        if return_intermediate:\n",
    "            return h, intermediate\n",
    "        else:\n",
    "            return h\n",
    "\n",
    "    @classmethod\n",
    "    def _resolve_block(cls, block_fn):\n",
    "        if block_fn == \"normal\":\n",
    "            return layers.BasicBlockV2_dbt\n",
    "        elif block_fn == \"bottleneck\":\n",
    "            return layers.BottleneckV2_dbt\n",
    "        else:\n",
    "            raise KeyError(block_fn)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        # downsample = None\n",
    "        # if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                      kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(planes * block.expansion),\n",
    "        )\n",
    "\n",
    "        layers_ = [\n",
    "            block(self.inplanes, planes, stride, downsample, self.norm_class, self.num_groups)\n",
    "        ]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers_.append(block(self.inplanes, planes, norm_class=self.norm_class, num_groups=self.num_groups))\n",
    "\n",
    "        return nn.Sequential(*layers_)\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            self._layer_init(m)\n",
    "\n",
    "    @classmethod\n",
    "    def _layer_init(cls, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # From original\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #             nn.init.xavier_normal_(m.weight)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.GroupNorm):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @classmethod\n",
    "    def from_parameters(cls, parameters):\n",
    "        return cls(\n",
    "            input_channels=parameters[\"input_channels\"],\n",
    "            num_filters=parameters[\"num_filters\"],\n",
    "            first_layer_kernel_size=parameters[\"first_layer_kernel_size\"],\n",
    "            first_layer_conv_stride=parameters[\"first_layer_conv_stride\"],\n",
    "            first_layer_padding=parameters.get(\"first_layer_padding\", 0),\n",
    "            blocks_per_layer_list=parameters[\"blocks_per_layer_list\"],\n",
    "            block_strides_list=parameters[\"block_strides_list\"],\n",
    "            block_fn=parameters[\"block_fn\"],\n",
    "            first_pool_size=parameters[\"first_pool_size\"],\n",
    "            first_pool_stride=parameters[\"first_pool_stride\"],\n",
    "            first_pool_padding=parameters.get(\"first_pool_padding\", 0),\n",
    "            growth_factor=parameters.get(\"growth_factor\", 2),\n",
    "            norm_class=parameters.get(\"norm_class\", \"batch\"),\n",
    "            num_groups=parameters.get(\"num_groups\", 1)\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_22(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            attention=False,\n",
    "            dropout=0.0,\n",
    "            hidden_size=256,\n",
    "\n",
    "            # resnet hyperparameters\n",
    "            #         input_channels=1,\n",
    "            first_layer_kernel_size=7,\n",
    "            first_layer_conv_stride=2,\n",
    "            first_pool_size=3,\n",
    "            first_pool_stride=2,\n",
    "            first_layer_padding=0,\n",
    "            first_pool_padding=0,\n",
    "            growth_factor=2,\n",
    "\n",
    "            # resnet22 settings\n",
    "            num_filters=16,\n",
    "            blocks_per_layer_list=[2, 2, 2, 2, 2],\n",
    "            block_strides_list=[1, 2, 2, 2, 2],\n",
    "            block_fn=\"normal\",\n",
    "            norm_class=\"group\",\n",
    "            num_groups=8,\n",
    "\n",
    "            num_image_slices_per_net=1,\n",
    "    ):\n",
    "        super(ResNet_22, self).__init__()\n",
    "\n",
    "        self.num_image_slices_per_net = num_image_slices_per_net\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.resnet = ResNet(\n",
    "            input_channels=3,\n",
    "            first_layer_kernel_size=first_layer_kernel_size,\n",
    "            first_layer_conv_stride=first_layer_conv_stride,\n",
    "            first_pool_size=first_pool_size,\n",
    "            first_pool_stride=first_pool_stride,\n",
    "            num_filters=num_filters,\n",
    "            blocks_per_layer_list=blocks_per_layer_list,\n",
    "            block_strides_list=block_strides_list,\n",
    "            block_fn=block_fn,\n",
    "            first_layer_padding=first_layer_padding,\n",
    "            first_pool_padding=first_pool_padding,\n",
    "            growth_factor=growth_factor,\n",
    "            norm_class=norm_class,\n",
    "            num_groups=num_groups,\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # use avgpool rather than torch.mean\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        h = self.resnet(x)\n",
    "        # Shape of pooled_h is [4, 256, 1, 1]\n",
    "        pooled_h = self.avgpool(h)\n",
    "        return pooled_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ResNet_22' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-e02a0ee98ace>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mResNet_22\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'ResNet_22' is not defined"
     ]
    }
   ],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = ResNet_22()\n",
    "model.eval()\n",
    "\n",
    "boom = model(trainset[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = resnet34()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc = Identity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 1, 1])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boom.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
