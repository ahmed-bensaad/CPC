{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import collections as col\n",
    "import numpy as np\n",
    "import random\n",
    "import cv2\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pdb\n",
    "from sklearn import metrics\n",
    "import time\n",
    "import os\n",
    "import PIL\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils, datasets\n",
    "from torchvision.models import resnet18, resnet34\n",
    "import torch.nn.functional as F\n",
    "from pytorch_datasets import DiagnosticInpainted\n",
    "import models\n",
    "import layers\n",
    "import utilities.reading_images as reading_images\n",
    "from utilities.loading import get_single_image\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(PixelCNN, self).__init__()\n",
    "        \n",
    "        # Conv2d: (input_channels, output_channels, kernel_size, padding)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, (1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.ConstantPad2d((1, 1, 0, 0), 0),\n",
    "            nn.Conv2d(256, 256, (1, 3)),\n",
    "            nn.ConstantPad2d((0, 0, 0, 1), 0),\n",
    "            nn.Conv2d(256, 256, (2, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, (1, 1))\n",
    "        )\n",
    "\n",
    "    def forward(self, latents):\n",
    "        \n",
    "        # latents: [B, C, H, W]\n",
    "        cres = latents\n",
    "        \n",
    "        for _ in range(5):\n",
    "            c = self.model(cres)\n",
    "            cres = cres + c\n",
    "        cres = self.relu(cres)\n",
    "        return cres      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9],\n",
       "        [10, 11, 12, 13, 14, 15, 16, 17, 18, 19]])"
      ]
     },
     "execution_count": 350,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boom = torch.tensor(np.arange(20).reshape(2, 10))\n",
    "boom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0,  1,  2,  3,  4],\n",
       "         [ 5,  6,  7,  8,  9]],\n",
       "\n",
       "        [[10, 11, 12, 13, 14],\n",
       "         [15, 16, 17, 18, 19]]])"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boom.reshape(2, 2, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yeah their implementation is wack. Not clear. \n",
    "# I'm going to go with WF. \n",
    "\n",
    "# NCE Loss\n",
    "# Questions: Is the dimension of Z (B*patches) or (B). \n",
    "#            I think it's (B, 6, 6, 4096)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class CPCLossNCE(nn.Module):\n",
    "    \n",
    "    def nce_loss(self, z_hat, pos_scores, negative_samples, mask_mat):\n",
    "        \n",
    "        # (B, 1)\n",
    "        pos_scores = pos_scores.float()\n",
    "        batch_size, emb_dim = z_hat.size()\n",
    "        nb_feat_vectors = negative_samples.size(1) // batch_size # 36 of them, if 6 by 6 wireframes. \n",
    "        \n",
    "        # (b, b) -> (b, b, nb_feat_vectors)\n",
    "        # all zeros with ones in diagonal tensor... (ie: b1 b1 are all 1s, b1 b2 are all zeros)\n",
    "        mask_pos = mask_mat.unsqueeze(dim=2).expand(-1, -1, nb_feat_vectors).float()\n",
    "        \n",
    "        # negative mask\n",
    "        mask_neg = 1. - mask_pos\n",
    "        \n",
    "        # ----------------------\n",
    "        # ALL SCORES COMPUTATION\n",
    "        # (b, dim) x (dim, nb_feats*b) -> (b, b, nb_feats)\n",
    "        raw_scores = torch.mm(z_hat, negative_samples)\n",
    "        raw_scores = raw_scores.reshape(batch_size, batch_size, nb_feat_vectors).float()\n",
    "        \n",
    "        # ----------------------\n",
    "        # EXTRACT NEGATIVE SCORES\n",
    "        # (batch_size, batch_size, nb_feat_vectors)\n",
    "        # HE'S TAKING THE NEGATIVE SAMPLES FROM THE OTHER MINIBATCHES\n",
    "        neg_scores = (mask_neg * raw_scores)\n",
    "        \n",
    "        # (batch_size, batch_size * nb_feat_vectors) -> (batch_size, batch_size, nb_feat_vectors)\n",
    "        neg_scores = neg_scores.reshape(batch_size, -1)\n",
    "        mask_neg = mask_neg.reshape(batch_size, -1)\n",
    "        \n",
    "        # STABLE SOFTMAX\n",
    "        # (n_batch_gpu, 1)\n",
    "        neg_maxes = torch.max(neg_scores, dim=1, keepdim=True)[0]\n",
    "        \n",
    "        # DENOMINATOR\n",
    "        # sum over only negative samples (none from the diagonal)\n",
    "        neg_sumexp = (mask_neg * torch.exp(neg_scores - neg_maxes)).sum(dim=1, keepdim=True)\n",
    "        all_logsumexp = torch.log(torch.exp(pos_scores - neg_maxes) + neg_sumexp)\n",
    "        \n",
    "        # NUMERATOR\n",
    "        # compute numerators for the NCE log-softmaxes\n",
    "        pos_shiftexp = pos_scores - neg_maxes\n",
    "        \n",
    "        # FULL NCE\n",
    "        nce_scores = pos_shiftexp - all_logsumexp\n",
    "        nce_scores = -nce_scores.mean()\n",
    "        \n",
    "        return nce_scores\n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, Z, C, W_list):\n",
    "        '''\n",
    "        param Z: latent vecs (B, C, H, W)\n",
    "        param C: context vecs (B, C, H, W)\n",
    "        param W_list: list of k-1 W projections\n",
    "        '''\n",
    "        \n",
    "        # (b, dim, w, h)\n",
    "        batch_size, emb_dim, h, w = Z.size()\n",
    "        \n",
    "        diag_mat = torch.eye(batch_size)\n",
    "        diag_mat = diag_mat.float()\n",
    "        \n",
    "        losses = []\n",
    "        # calculate loss for each k\n",
    "        \n",
    "        # All z's\n",
    "        # Below operations preserve raster order (for B, C, H, W) = (1, 5, 2, 2) check. \n",
    "        Z_neg = Z.permute(1, 0, 2, 3).reshape(emb_dim, -1)\n",
    "        \n",
    "        \n",
    "        for i in range(0, h-1):\n",
    "            for j in range(0, w):\n",
    "                cij = C[:, :, i, j]\n",
    "                \n",
    "                for k in range(i+1, h):\n",
    "                    Wk = W_list[str(k)]\n",
    "                    \n",
    "                    z_hat_ikj = Wk(cij)\n",
    "                    zikj = Z[:, :, k, j]\n",
    "                    \n",
    "                    # BATCH DOT PRODUCT\n",
    "                    # (b, d) x (b, d) -> (b, 1)\n",
    "                    pos_scores = torch.bmm(z_hat_ik_j.unsqueeze(1), zikj.unsqueeze(2))\n",
    "                    pos_scores = pos_scores.squeeze(-1).squeeze(-1)\n",
    "                    \n",
    "                    loss = self.nce_loss(z_hat_ik_j, pos_scores, Z_neg, diag_mat)\n",
    "                    losses.append(loss)\n",
    "                    \n",
    "                    \n",
    "        losses = torch.stack(losses)\n",
    "        loss = losses.mean()\n",
    "        return loss           \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raster_patchify(img, size = 80, overlap = 32):\n",
    "    '''\n",
    "    Left-to-right, top to bottom.\n",
    "    Assumes img is (3, 240, 240).\n",
    "    '''\n",
    "    patches = []\n",
    "     \n",
    "    h = -32\n",
    "    w = -32\n",
    "    for i in range(6):\n",
    "        h = h + 32\n",
    "        for j in range(6):\n",
    "            w = w + 32\n",
    "            patches.append(img[:, h:h+size, w:w+size])\n",
    "        w = -32\n",
    "            \n",
    "    return patches\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "36"
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "patches = raster_patchify(trainset[0][0])\n",
    "len(patches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(patches[35].permute(1, 2, 0))\n",
    "\n",
    "# plt.imshow(trainset[0][0].permute(1, 2, 0))\n",
    "# plt.scatter(80,80,color='r')\n",
    "# plt.scatter(80+32,80,color='r')\n",
    "# plt.scatter(80+64,80,color='r')\n",
    "# plt.scatter(80+96,80,color='r')\n",
    "# plt.scatter(80+128,80,color='r')\n",
    "# plt.scatter(80+160,80,color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(img_list):\n",
    "    patches = []\n",
    "    for (img, label) in img_list:\n",
    "        img_patches = raster_patchify(img)\n",
    "        patches.append(torch.stack(img_patches))\n",
    "        \n",
    "    return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(240),\n",
    "    transforms.ToTensor(),\n",
    "#     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "])\n",
    "\n",
    "trainset = datasets.ImageFolder(\n",
    "    root = '/gpfs/data/geraslab/Vish/imagewoof320/imagewoof2-320/train/',\n",
    "    transform = data_transform\n",
    ")\n",
    "\n",
    "train_dl = DataLoader(trainset, batch_size=10, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "valset = datasets.ImageFolder(\n",
    "    root = '/gpfs/data/geraslab/Vish/imagewoof320/imagewoof2-320/val/',\n",
    "    transform = data_transform\n",
    ")\n",
    "\n",
    "val_dl = DataLoader(valset, batch_size=10, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-369-e47e279da0c7>(19)<module>()\n",
      "-> Z = torch.stack(Z).squeeze(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-369-e47e279da0c7>(20)<module>()\n",
      "-> C = torch.stack(C).squeeze(1)\n",
      "(Pdb) n\n",
      "> <ipython-input-369-e47e279da0c7>(21)<module>()\n",
      "-> print('boom')\n",
      "(Pdb) Z.shape\n",
      "torch.Size([10, 256, 6, 6])\n",
      "(Pdb) C.shape\n",
      "torch.Size([10, 256, 6, 6])\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-369-e47e279da0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'boom'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-369-e47e279da0c7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mZ\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mZ\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'boom'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/capstone/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/capstone/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "torch.cuda.set_device(2)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = ResNet_22().to(device)\n",
    "pixel_cnn = PixelCNN(256).to(device)\n",
    "\n",
    "for i, x in enumerate(train_dl):\n",
    "    Z = []\n",
    "    C = []\n",
    "    for img_patches in x:\n",
    "        img_patches = img_patches.to(device)\n",
    "        z = model(img_patches).squeeze()\n",
    "        z = z.unsqueeze(0).permute(0, 2, 1).reshape(1, 256, 6, 6)\n",
    "        Z.append(z)\n",
    "        c = pixel_cnn(z)\n",
    "        C.append(c)\n",
    "    pdb.set_trace()\n",
    "        \n",
    "    Z = torch.stack(Z).squeeze(1)\n",
    "    C = torch.stack(C).squeeze(1)\n",
    "    print('boom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9025"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 240, 240])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[50][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Adapted from torchvision ResNet, converted to v2\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 input_channels, num_filters,\n",
    "                 first_layer_kernel_size, first_layer_conv_stride,\n",
    "                 blocks_per_layer_list, block_strides_list, block_fn,\n",
    "                 first_layer_padding=0,\n",
    "                 first_pool_size=None, first_pool_stride=None, first_pool_padding=0,\n",
    "                 growth_factor=2, norm_class=\"batch\", num_groups=1):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.first_conv = nn.Conv2d(\n",
    "            in_channels=input_channels, out_channels=num_filters,\n",
    "            kernel_size=first_layer_kernel_size,\n",
    "            stride=first_layer_conv_stride,\n",
    "            padding=first_layer_padding,\n",
    "            bias=False,\n",
    "        )\n",
    "        # Diff: padding=SAME vs. padding=0\n",
    "        self.first_pool = nn.MaxPool2d(\n",
    "            kernel_size=first_pool_size,\n",
    "            stride=first_pool_stride,\n",
    "            padding=first_pool_padding,\n",
    "        )\n",
    "        self.norm_class = norm_class\n",
    "        self.num_groups = num_groups\n",
    "\n",
    "        block = self._resolve_block(block_fn)\n",
    "        self.layer_list = nn.ModuleList()\n",
    "        current_num_filters = num_filters\n",
    "        self.inplanes = num_filters\n",
    "        for i, (num_blocks, stride) in enumerate(zip(\n",
    "                blocks_per_layer_list, block_strides_list)):\n",
    "            self.layer_list.append(self._make_layer(\n",
    "                block=block,\n",
    "                planes=current_num_filters,\n",
    "                blocks=num_blocks,\n",
    "                stride=stride,\n",
    "            ))\n",
    "            current_num_filters *= growth_factor\n",
    "\n",
    "        self.final_bn = layers.resolve_norm_layer(\n",
    "            # current_num_filters // growth_factor\n",
    "            current_num_filters // growth_factor * block.expansion,\n",
    "            norm_class,\n",
    "            num_groups\n",
    "        )\n",
    "        self.relu = nn.ReLU()\n",
    "        self.initialize()\n",
    "\n",
    "        # Expose attributes for downstream dimension computation\n",
    "        self.num_filters = num_filters\n",
    "        self.growth_factor = growth_factor\n",
    "        self.block = block\n",
    "        self.num_filter_last_seq = current_num_filters // growth_factor * block.expansion\n",
    "\n",
    "    def forward(self, x, return_intermediate=False):\n",
    "        intermediate = []\n",
    "        h = self.first_conv(x)\n",
    "        h = self.first_pool(h)\n",
    "\n",
    "        if return_intermediate:\n",
    "            intermediate.append(h)\n",
    "        for i, layer in enumerate(self.layer_list):\n",
    "            h = layer(h)\n",
    "            if return_intermediate:\n",
    "                intermediate.append(h)\n",
    "\n",
    "        h = self.final_bn(h)\n",
    "        h = self.relu(h)\n",
    "\n",
    "        if return_intermediate:\n",
    "            return h, intermediate\n",
    "        else:\n",
    "            return h\n",
    "\n",
    "    @classmethod\n",
    "    def _resolve_block(cls, block_fn):\n",
    "        if block_fn == \"normal\":\n",
    "            return layers.BasicBlockV2_dbt\n",
    "        elif block_fn == \"bottleneck\":\n",
    "            return layers.BottleneckV2_dbt\n",
    "        else:\n",
    "            raise KeyError(block_fn)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "        # downsample = None\n",
    "        # if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "        downsample = nn.Sequential(\n",
    "            nn.Conv2d(self.inplanes, planes * block.expansion,\n",
    "                      kernel_size=1, stride=stride, bias=False),\n",
    "            nn.BatchNorm2d(planes * block.expansion),\n",
    "        )\n",
    "\n",
    "        layers_ = [\n",
    "            block(self.inplanes, planes, stride, downsample, self.norm_class, self.num_groups)\n",
    "        ]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers_.append(block(self.inplanes, planes, norm_class=self.norm_class, num_groups=self.num_groups))\n",
    "\n",
    "        return nn.Sequential(*layers_)\n",
    "\n",
    "    def initialize(self):\n",
    "        for m in self.modules():\n",
    "            self._layer_init(m)\n",
    "\n",
    "    @classmethod\n",
    "    def _layer_init(cls, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            # From original\n",
    "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "        #             nn.init.xavier_normal_(m.weight)\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "        elif isinstance(m, nn.GroupNorm):\n",
    "            nn.init.constant_(m.weight, 1)\n",
    "            nn.init.constant_(m.bias, 0)\n",
    "\n",
    "    @classmethod\n",
    "    def from_parameters(cls, parameters):\n",
    "        return cls(\n",
    "            input_channels=parameters[\"input_channels\"],\n",
    "            num_filters=parameters[\"num_filters\"],\n",
    "            first_layer_kernel_size=parameters[\"first_layer_kernel_size\"],\n",
    "            first_layer_conv_stride=parameters[\"first_layer_conv_stride\"],\n",
    "            first_layer_padding=parameters.get(\"first_layer_padding\", 0),\n",
    "            blocks_per_layer_list=parameters[\"blocks_per_layer_list\"],\n",
    "            block_strides_list=parameters[\"block_strides_list\"],\n",
    "            block_fn=parameters[\"block_fn\"],\n",
    "            first_pool_size=parameters[\"first_pool_size\"],\n",
    "            first_pool_stride=parameters[\"first_pool_stride\"],\n",
    "            first_pool_padding=parameters.get(\"first_pool_padding\", 0),\n",
    "            growth_factor=parameters.get(\"growth_factor\", 2),\n",
    "            norm_class=parameters.get(\"norm_class\", \"batch\"),\n",
    "            num_groups=parameters.get(\"num_groups\", 1)\n",
    "        )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet_22(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            attention=False,\n",
    "            dropout=0.0,\n",
    "            hidden_size=256,\n",
    "\n",
    "            # resnet hyperparameters\n",
    "            #         input_channels=1,\n",
    "            first_layer_kernel_size=7,\n",
    "            first_layer_conv_stride=2,\n",
    "            first_pool_size=3,\n",
    "            first_pool_stride=2,\n",
    "            first_layer_padding=0,\n",
    "            first_pool_padding=0,\n",
    "            growth_factor=2,\n",
    "\n",
    "            # resnet22 settings\n",
    "            num_filters=16,\n",
    "            blocks_per_layer_list=[2, 2, 2, 2, 2],\n",
    "            block_strides_list=[1, 2, 2, 2, 2],\n",
    "            block_fn=\"normal\",\n",
    "            norm_class=\"group\",\n",
    "            num_groups=8,\n",
    "\n",
    "            num_image_slices_per_net=1,\n",
    "    ):\n",
    "        super(ResNet_22, self).__init__()\n",
    "\n",
    "        self.num_image_slices_per_net = num_image_slices_per_net\n",
    "\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.resnet = ResNet(\n",
    "            input_channels=3,\n",
    "            first_layer_kernel_size=first_layer_kernel_size,\n",
    "            first_layer_conv_stride=first_layer_conv_stride,\n",
    "            first_pool_size=first_pool_size,\n",
    "            first_pool_stride=first_pool_stride,\n",
    "            num_filters=num_filters,\n",
    "            blocks_per_layer_list=blocks_per_layer_list,\n",
    "            block_strides_list=block_strides_list,\n",
    "            block_fn=block_fn,\n",
    "            first_layer_padding=first_layer_padding,\n",
    "            first_pool_padding=first_pool_padding,\n",
    "            growth_factor=growth_factor,\n",
    "            norm_class=norm_class,\n",
    "            num_groups=num_groups,\n",
    "        )\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))  # use avgpool rather than torch.mean\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "\n",
    "        h = self.resnet(x)\n",
    "        # Shape of pooled_h is [4, 256, 1, 1]\n",
    "        pooled_h = self.avgpool(h)\n",
    "        return pooled_h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Identity(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return x\n",
    "    \n",
    "\n",
    "model = ResNet_22()\n",
    "model.eval()\n",
    "\n",
    "boom = model(trainset[0][0].unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 1, 1])"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boom.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
